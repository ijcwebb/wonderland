[["index.html", "R Programming for Business Module 1 Introduction to R 1.1 Intro to basics 1.2 Vectors 1.3 Matrices 1.4 Factors 1.5 Data frames 1.6 Lists 1.7 Challenge 1.8 Solutions", " R Programming for Business Iain Webb 2020-12-12 Module 1 Introduction to R In Introduction to R, you will master the basics of this widely used open source language, including factors, lists, and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. Oracle estimated over 2 million R users worldwide in 2012, cementing R as a leading programming language in statistics and data science. Every year, the number of R users grows by about 40%, and an increasing number of organizations are using it in their day-to-day activities. Begin your journey to learn R with us today! 1.1 Intro to basics Take your first steps with R. In this chapter, you will learn how to use the console as a calculator and how to assign variables. You will also get to know the basic data types in R. Lets get started. 1.1.1 How it works In the editor on the right you should type R code to solve the exercises. When you hit the Submit Answer button, every line of code is interpreted and executed by R and you get a message whether or not your code was correct. The output of your R code is shown in the console in the lower right corner. R makes use of the # sign to add comments, so that you and others can understand what the R code is about. Just like Twitter! Comments are not run as R code, so they will not influence your result. For example, Calculate 3 + 4 in the editor on the right is a comment. You can also execute R commands straight in the console. This is a good way to experiment with R code, as your submission is not checked for correctness. 1.1.1.1 Calculate 3 + 4 3 + 4 ## [1] 7 1.1.1.2 Calculate 6 + 12 6 + 12 ## [1] 18 See how the console shows the result of the R code you submitted? Now that youre familiar with the interface, lets get down to R business! 1.1.2 Arithmetic with R In its most basic form, R can be used as a simple calculator. Consider the following arithmetic operators: Addition: + Subtraction: - Multiplication: * Division: / Exponentiation: ^ Modulo: %% The last two might need some explaining: The ^ operator raises the number to its left to the power of the number to its right: for example 3^2 is 9. The modulo returns the remainder of the division of the number to the left by the number on its right, for example 5 modulo 3 or 5 %% 3 is 2. With this knowledge, follow the instructions below to complete the exercise. 1.1.2.1 An addition 5 + 5 ## [1] 10 1.1.2.2 A subtraction 5 - 5 ## [1] 0 1.1.2.3 A multiplication 3 * 5 ## [1] 15 1.1.2.4 A division (5 + 5) / 2 ## [1] 5 1.1.2.5 Exponentiation 2^5 ## [1] 32 1.1.2.6 Modulo 28 %% 6 ## [1] 4 1.1.3 Variable assignment A basic concept in (statistical) programming is called a variable. A variable allows you to store a value (e.g. 4) or an object (e.g. a function description) in R. You can then later use this variables name to easily access the value or the object that is stored within this variable. You can assign a value 4 to a variable my_var with the command my_var &lt;- 4 1.1.3.1 Assign the value 42 to x x &lt;- 42 1.1.3.2 Print out the value of the variable x x ## [1] 42 Have you noticed that R does not print the value of a variable to the console when you did the assignment? x &lt;- 42 did not generate any output, because R assumes that you will be needing this variable in the future. Otherwise you wouldnt have stored the value in a variable in the first place, right? Proceed to the next exercise! 1.1.4 Variable assignment (2) Suppose you have a fruit basket with five apples. As a data analyst in training, you want to store the number of apples in a variable with the name my_apples. 1.1.4.1 Assign the value 5 to the variable my_apples my_apples &lt;- 5 1.1.4.2 Print out the value of the variable my_apples my_apples ## [1] 5 1.1.5 Variable assignment (3) Every tasty fruit basket needs oranges, so you decide to add six oranges. As a data analyst, your reflex is to immediately create the variable my_oranges and assign the value 6 to it. Next, you want to calculate how many pieces of fruit you have in total. Since you have given meaningful names to these values, you can now code this in a clear way: my_apples + my_oranges 1.1.5.1 Assign a value to the variable my_oranges my_oranges &lt;- 6 1.1.5.2 Add these two variables together my_apples + my_oranges ## [1] 11 1.1.5.3 Create the variable my_fruit my_fruit &lt;- my_apples + my_oranges Nice one! The great advantage of doing calculations with variables is reusability. If you just change my_apples to equal 12 instead of 5 and rerun the script, my_fruit will automatically update as well. Continue to the next exercise. 1.1.6 Apples and oranges Common knowledge tells you not to add apples and oranges. But hey, that is what you just did, no :-)? The my_apples and my_oranges variables both contained a number in the previous exercise. The + operator works with numeric variables in R. If you really tried to add apples and oranges, and assigned a text value to the variable my_oranges (see the editor), you would be trying to assign the addition of a numeric and a character variable to the variable my_fruit. This is not possible. 1.1.7 Basic data types in R R works with numerous data types. Some of the most basic types to get started are: Decimal values like 4.5 are examples of the data type numeric. Whole numbers like 3, 4, 0 and -4 are called integers. Integers are also examples of the numeric data type. Boolean values (TRUE or FALSE) are called logical. Text (or string) values are examples of the data type character. Note how the quotation marks indicate that the text inside quotation marks is a character. 1.1.7.1 Change my_numeric to be 42 my_numeric &lt;- 42 1.1.7.2 Change my_character to be universe my_character &lt;- &quot;universe&quot; 1.1.7.3 Change my_logical to be FALSE my_logical &lt;- FALSE 1.1.8 Whats the data type? If you set the variable my_oranges to have the value 5 and the variable my_oranges to have the value six, attempting to add them will give an error. This is due to a mismatch in data types? You can add two (or more) numerics together, but you cant add a numeric and a character together. Avoid such embarrassing situations by checking the data type of a variable beforehand. You can do this with the class() function, as the code below shows. 1.1.8.1 Check class of my_numeric class(my_numeric) ## [1] &quot;numeric&quot; 1.1.8.2 Check class of my_character class(my_character) ## [1] &quot;character&quot; 1.1.8.3 Check class of my_logical class(my_logical) ## [1] &quot;logical&quot; This was the last exercise for this chapter. Head over to the next chapter to get immersed in the world of vectors! 1.2 Vectors We take you on a trip to Vegas, where you will learn how to analyze your gambling results using vectors in R. After completing this chapter, you will be able to create vectors in R, name them, select elements from them, and compare different vectors. 1.2.1 Create a vector Feeling lucky? You better, because this chapter takes you on a trip to the City of Sins, also known as Statisticians Paradise! Thanks to R and your new data-analytical skills, you will learn how to uplift your performance at the tables and fire off your career as a professional gambler. This chapter will show how you can easily keep track of your betting progress and how you can do some simple analyses on past actions. Next stop, Vegas Baby VEGAS!! 1.2.1.1 Define the variable vegas vegas &lt;- &quot;Go!&quot; 1.2.2 Create a vector (2) Let us focus first! On your way from rags to riches, you will make extensive use of vectors. Vectors are one-dimension arrays that can hold numeric data, character data, or logical data. In other words, a vector is a simple tool to store data. For example, you can store your daily gains and losses in the casinos. In R, you create a vector with the combine function c(). You place the vector elements separated by a comma between the parentheses. For example: numeric_vector &lt;- c(1, 2, 3) character_vector &lt;- c(\"a\", \"b\", \"c\") Once you have created these vectors in R, you can use them to do calculations. numeric_vector &lt;- c(1, 10, 49) character_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) 1.2.2.1 Complete the code for boolean_vector boolean_vector &lt;- c(TRUE, FALSE, TRUE) Notice that adding a space behind the commas in the c() function improves the readability of your code, but changes nothing else. Lets practice some more with vector creation in the next exercise. 1.2.3 Create a vector (3) 1.2.3.1 Poker winnings from Monday to Friday poker_vector &lt;- c(140, -50, 20, -120, 240) 1.2.3.2 Roulette winnings from Monday to Friday roulette_vector &lt;- c(-24, -50, 100, -350, 10) To check out the contents of your vectors, remember that you can always simply type the variable in the console and hit Enter. 1.2.4 Naming a vector As a data analyst, it is important to have a clear view on the data that you are using. Understanding what each element refers to is therefore essential. In the previous exercise, we created a vector with your winnings over the week. Each vector element refers to a day of the week but it is hard to tell which element belongs to which day. It would be nice if you could show that in the vector itself. You can give a name to the elements of a vector with the names() function. Have a look at this example: some_vector &lt;- c(\"John Doe\", \"poker player\") names(some_vector) &lt;- c(\"Name\", \"Profession\") This code first creates a vector some_vector and then gives the two elements a name. The first element is assigned the name Name, while the second element is labeled Profession. Printing the contents to the console yields following output: Name Profession &quot;John Doe&quot; &quot;poker player&quot; 1.2.4.1 Assign days as names of poker_vector names(poker_vector) &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) 1.2.4.2 Assign days as names of roulette_vector names(roulette_vector) &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) 1.2.5 Naming a vector (2) If you want to become a good statistician, you have to become lazy. (If you are already lazy, chances are high you are one of those exceptional, natural-born statistical talents.) In the previous exercises you probably experienced that it is boring and frustrating to type and retype information such as the days of the week. However, when you look at it from a higher perspective, there is a more efficient way to do this, namely, to assign the days of the week vector to a variable! Just like you did with your poker and roulette returns, you can also create a variable that contains the days of the week. This way you can use and re-use it. 1.2.5.1 The variable days_vector days_vector &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) 1.2.5.2 Assign the names of the day to roulette_vector and poker_vector names(roulette_vector) &lt;- days_vector names(poker_vector) &lt;- days_vector A word of advice: try to avoid code duplication at all times. Continue to the next exercise and learn how to do arithmetic with vectors! 1.2.6 Calculating total winnings Now that you have the poker and roulette winnings nicely as named vectors, you can start doing some data analytical magic. You want to find out the following type of information: How much has been your overall profit or loss per day of the week? Have you lost money over the week in total? Are you winning/losing money on poker or on roulette? To get the answers, you have to do arithmetic calculations on vectors. It is important to know that if you sum two vectors in R, it takes the element-wise sum. For example, the following three statements are completely equivalent: c(1, 2, 3) + c(4, 5, 6) c(1 + 4, 2 + 5, 3 + 6) c(5, 7, 9) You can also do the calculations with variables that represent vectors: a &lt;- c(1, 2, 3) b &lt;- c(4, 5, 6) c &lt;- a + b A_vector &lt;- c(1, 2, 3) B_vector &lt;- c(4, 5, 6) 1.2.6.1 Take the sum of A_vector and B_vector total_vector &lt;- A_vector + B_vector 1.2.6.2 Print out total_vector total_vector ## [1] 5 7 9 1.2.7 Calculating total winnings (2) Now you understand how R does arithmetic with vectors, it is time to get those Ferraris in your garage! First, you need to understand what the overall profit or loss per day of the week was. The total daily profit is the sum of the profit/loss you made on poker per day, and the profit/loss you made on roulette per day. In R, this is just the sum of roulette_vector and poker_vector. 1.2.7.1 Assign to total_daily how much you won/lost on each day total_daily &lt;- poker_vector + roulette_vector 1.2.7.2 Display the daily total total_daily ## Monday Tuesday Wednesday Thursday Friday ## 116 -100 120 -470 250 1.2.8 Calculating total winnings (3) Based on the previous analysis, it looks like you had a mix of good and bad days. This is not what your ego expected, and you wonder if there may be a very tiny chance you have lost money over the week in total? A function that helps you to answer this question is sum(). It calculates the sum of all elements of a vector. For example, to calculate the total amount of money you have lost/won with poker you do: total_poker &lt;- sum(poker_vector) 1.2.8.1 Total winnings with poker total_poker &lt;- sum(poker_vector) total_poker ## [1] 230 1.2.8.2 Total winnings with roulette total_roulette &lt;- sum(roulette_vector) total_roulette ## [1] -314 1.2.8.3 Total winnings overall total_week &lt;- total_poker + total_roulette 1.2.8.4 Print out total_week total_week ## [1] -84 1.2.9 Comparing total winnings Oops, it seems like you are losing money. Time to rethink and adapt your strategy! This will require some deeper analysis After a short brainstorm in your hotels jacuzzi, you realize that a possible explanation might be that your skills in roulette are not as well developed as your skills in poker. So maybe your total gains in poker are higher (or &gt; ) than in roulette. 1.2.9.1 Check if you realized higher total gains in poker than in roulette total_poker &gt; total_roulette ## [1] TRUE 1.2.10 Vector selection: the good times Your hunch seemed to be right. It appears that the poker game is more your cup of tea than roulette. Another possible route for investigation is your performance at the beginning of the working week compared to the end of it. You did have a couple of Margarita cocktails at the end of the week To answer that question, you only want to focus on a selection of the total_vector. In other words, our goal is to select specific elements of the vector. To select elements of a vector (and later matrices, data frames, ), you can use square brackets. Between the square brackets, you indicate what elements to select. For example, to select the first element of the poker vector, you type poker_vector[1]. To select the second element of the vector, you type poker_vector[2], etc. Notice that the first element in a vector in R has index 1, not 0 as in many other programming languages. 1.2.10.1 Define a new variable based on a selection poker_wednesday &lt;- poker_vector[3] R also makes it possible to select multiple elements from a vector at once. Learn how in the next exercise! 1.2.11 Vector selection: the good times (2) How about analyzing your midweek results? To select multiple elements from a vector, you can add square brackets at the end of it. You can indicate between the brackets what elements should be selected. For example: suppose you want to select the first and the fifth day of the week: use the vector c(1, 5) between the square brackets. For example, the code below selects the first and fifth element of poker_vector: poker_vector[c(1, 5)] 1.2.11.1 Define a new variable based on a selection poker_midweek &lt;- poker_vector[c(2,3,4)] Continue to the next exercise to specialize in vector selection some more! 1.2.12 Vector selection: the good times (3) Selecting multiple elements of poker_vector with c(2, 3, 4) is not very convenient. Many statisticians are lazy people by nature, so they created an easier way to do this: c(2, 3, 4) can be abbreviated to 2:4, which generates a vector with all natural numbers from 2 up to and including 4. So, another way to find the mid-week results is poker_vector[2:4]. 1.2.12.1 Define a new variable based on a selection roulette_selection_vector &lt;- roulette_vector[2:5] The colon operator is extremely useful and very often used in R programming, so remember it well. 1.2.13 Vector selection: the good times (4) Another way to tackle the previous exercise is by using the names of the vector elements (Monday, Tuesday, ) instead of their numeric positions. For example, poker_vector[\"Monday\"] will select the first element of poker_vector because Monday is the name of that first element. Just like you did in the previous exercise with numerics, you can also use the element names to select multiple elements, for example: poker_vector[c(\"Monday\",\"Tuesday\")] 1.2.13.1 Select poker results for Monday, Tuesday and Wednesday poker_start &lt;- poker_vector[c(&quot;Monday&quot;, &quot;Tuesday&quot;,&quot;Wednesday&quot;)] 1.2.13.2 Calculate the average of the elements in poker_start mean(poker_start) ## [1] 36.66667 Good job! Apart from subsetting vectors by index or by name, you can also subset vectors by comparison. The next exercises will show you how! 1.2.14 Selection by comparison - Step 1 By making use of comparison operators, we can approach the previous question in a more proactive way. The (logical) comparison operators known to R are: &lt; for less than &gt; for greater than &lt;= for less than or equal to &gt;= for greater than or equal to == for equal to each other != not equal to each other As seen in the previous chapter, stating 6 &gt; 5 returns TRUE. The nice thing about R is that you can use these comparison operators also on vectors. For example: c(4, 5, 6) &gt; 5 [1] FALSE FALSE TRUE This command tests for every element of the vector if the condition stated by the comparison operator is TRUE or FALSE. 1.2.14.1 Which days did you make money on poker? poker_vector &gt; 0 ## Monday Tuesday Wednesday Thursday Friday ## TRUE FALSE TRUE FALSE TRUE selection_vector &lt;- poker_vector &gt; 0 1.2.14.2 Print out selection_vector selection_vector ## Monday Tuesday Wednesday Thursday Friday ## TRUE FALSE TRUE FALSE TRUE 1.2.15 Selection by comparison - Step 2 Working with comparisons will make your data analytical life easier. Instead of selecting a subset of days to investigate yourself (like before), you can simply ask R to return only those days where you realized a positive return for poker. In the previous exercises you used selection_vector &lt;- poker_vector &gt; 0 to find the days on which you had a positive poker return. Now, you would like to know not only the days on which you won, but also how much you won on those days. You can select the desired elements, by putting selection_vector between the square brackets that follow poker_vector: poker_vector[selection_vector] R knows what to do when you pass a logical vector in square brackets: it will only select the elements that correspond to TRUE in selection_vector. 1.2.15.1 Select from poker_vector these days poker_winning_days &lt;- poker_vector[selection_vector] poker_winning_days ## Monday Wednesday Friday ## 140 20 240 I printed out selection_vector and poker_winning_days along the way, just to be sure I was doing it right. 1.2.16 Advanced selection Just like you did for poker, you also want to know those days where you realized a positive return for roulette. 1.2.16.1 Which days did you make money on roulette? selection_vector &lt;- roulette_vector &gt; 0 Notice how were reusing selection_vector for the second time. 1.2.16.2 Select from roulette_vector these days roulette_winning_days &lt;- roulette_vector[selection_vector] This exercise concludes the chapter on vectors. The next chapter will introduce you to the two-dimensional version of vectors: matrices. 1.3 Matrices In this chapter, you will learn how to work with matrices in R. By the end of the chapter, you will be able to create matrices and understand how to do basic computations with them. You will analyze the box office numbers of the Star Wars movies and learn how to use matrices in R. May the force be with you! 1.3.1 Whats a matrix? In R, a matrix is a collection of elements arranged into a fixed number of rows and columns. Since you are only working with rows and columns, a matrix is called two-dimensional. You can construct a matrix in R with the matrix() function. Consider the following example: matrix(1:9, byrow = TRUE, nrow = 3) In the matrix() function: the first argument is the collection of elements that R will arrange into the rows and columns of the matrix. Here, we use 1:9 which, as weve seen before, is a shortcut for c(1, 2, 3, 4, 5, 6, 7, 8, 9). the next argument, byrow, indicates that the matrix is filled by the rows. If we want the matrix to be filled by the columns, we just place byrow = FALSE. The third argument, nrow indicates that the matrix should have three rows. 1.3.1.1 Construct a matrix with 3 rows that contains the numbers 1 up to 9 matrix(1:9, byrow=TRUE, nrow = 3) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 What happens, do you think, if you change TRUE in the above code to FALSE? Try it and see. 1.3.2 Analyze matrices, you shall It is now time to get your hands dirty. In the following exercises you will analyze the box office numbers of the Star Wars franchise. May the force be with you! In the editor, three vectors are defined. Each one represents the box office numbers from the first three Star Wars movies. The first element of each vector indicates the US box office revenue, the second element refers to the Non-US box office (source: Wikipedia). In this exercise, youll combine all these figures into a single vector. Next, youll build a matrix from this vector. 1.3.2.1 Box office Star Wars (in millions!) new_hope &lt;- c(460.998, 314.4) empire_strikes &lt;- c(290.475, 247.900) return_jedi &lt;- c(309.306, 165.8) In other words, three vectors, all of length 3, containing numerics. 1.3.2.2 Create box_office box_office &lt;- c(new_hope, empire_strikes, return_jedi) In other words, one vector, called box_office, of length 6. 1.3.2.3 Print box_office box_office ## [1] 460.998 314.400 290.475 247.900 309.306 165.800 1.3.2.4 Construct star_wars_matrix star_wars_matrix &lt;- matrix(box_office, byrow = TRUE, nrow = 3) In other words, a matrix, called star_wars_matrix, with 3 rows, which has been filled, by row, with the 6 elements of the vector box-office. This forces there to be two elements per row. What happens if you try to fill a matrix with 3 rows using a vector containing 5 elements, or 7 elements, or 8 elements. Try it and see. Youll see that to fill a matrix containing n rows, you need a multiple of n elements. Draw a picture of a matrix to help you understand why. The force is actually with you! 1.3.3 Naming a matrix To help you remember what is stored in star_wars_matrix, you would like to add the names of the movies for the rows. Not only does this help you to read the data, but it is also useful to select certain elements from the matrix. Previously we used the function names() to name the elements of a vector. As were dealing with two-dimensional arrays now, we need to be a little more specific. Similar to vectors, you can add names for the rows and the columns of a matrix rownames(my_matrix) &lt;- row_names_vector colnames(my_matrix) &lt;- col_names_vector Below well create two new vectors for use in this chapter: region, and titles. You will use these vectors to name the columns and rows of star_wars_matrix, respectively. 1.3.3.1 Create vectors region and titles, used for naming region &lt;- c(&quot;US&quot;, &quot;non-US&quot;) titles &lt;- c(&quot;A New Hope&quot;, &quot;The Empire Strikes Back&quot;, &quot;Return of the Jedi&quot;) 1.3.3.2 Name the columns with region colnames(star_wars_matrix) &lt;- region 1.3.3.3 Name the rows with titles rownames(star_wars_matrix) &lt;- titles 1.3.3.4 Print out star_wars_matrix star_wars_matrix ## US non-US ## A New Hope 460.998 314.4 ## The Empire Strikes Back 290.475 247.9 ## Return of the Jedi 309.306 165.8 How large is this matrix? Is it still 3 rows and 3 columns? Yes, despite it looking larger now weve added names to it, this matrix is still a 3 x 2 matrix. Check this using the dimension function dim(). 1.3.3.5 Check dimension of star_wars_matrix dim(star_wars_matrix) ## [1] 3 2 Notice it gives you the number of rows first, then the number of columns. How, using the vector box_office, would you create a matrix with 2 columns and 3 rows? We might look at this in one of our challenges. 1.3.4 Calculating the worldwide box office The single most important thing for a movie in order to become an instant legend in Tinseltown is its worldwide box office figures. To calculate the total box office revenue for the three Star Wars movies, you have to take the sum of the US revenue column and the non-US revenue column. In R, when dealing with marices, the function rowSums() conveniently calculates the totals for each row of a matrix. This function creates a new vector: rowSums(my_matrix) which we can assign to a variable. 1.3.4.1 Calculate worldwide box office figures worldwide_vector &lt;- rowSums(star_wars_matrix) worldwide_vector ## A New Hope The Empire Strikes Back ## 775.398 538.375 ## Return of the Jedi ## 475.106 What does this vector tell you exactly? What about the vector colSums(star_wars_matrix)? What will this vector tell us? Have a thought then give this vector a name. us_and_abroad_vector &lt;- colSums(star_wars_matrix) 1.3.5 Using dimnames() Hopefully youre starting to understand (if you didnt already) that when we refer to matrices, we always refer to their rows first, then columns. For example, a 3 x 2 matrix is one with 3 rows and 2 columns. This is true also when we build a matrix from scratch. Below is code which builds the same matrix weve been building over the past few exercises: 1.3.5.1 Construct star_wars_matrix box_office &lt;- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8) star_wars_matrix &lt;- matrix(box_office, nrow = 3, byrow = TRUE, dimnames = list(c(&quot;A New Hope&quot;, &quot;The Empire Strikes Back&quot;, &quot;Return of the Jedi&quot;), c(&quot;US&quot;, &quot;non-US&quot;))) Notice how rather than use the functions rownames() and colnames(), we used the function dimnames() to names the rows and columns. We wrote dimnames = list(c(\"row\", \"names\"), c(\"column\", \"names\")). We also made use of the list function, which well learn to use fully in a later chapter in this module. 1.3.6 Adding a column for the Worldwide box office In a previous exercise you calculated the vector that contained the worldwide box office receipt for each of the three Star Wars movies. However, this vector is not yet part of star_wars_matrix. You can add a column or multiple columns to a matrix with the cbind() function, which merges matrices and/or vectors together by column. For example: big_matrix &lt;- cbind(matrix1, matrix2, vector1 ...) Lets add the worldwide totals to the matrix. 1.3.6.1 Bind the new variable worldwide_vector as a column to star_wars_matrix all_wars_matrix &lt;- cbind(star_wars_matrix, worldwide_vector) Obviously this step will only work if the two matrices have the same number of rows. After adding this column to our matrix, the logical next step is to add rows. Learn how in the next exercise 1.3.7 Adding a row For this exercise, well use our existing matrix star_wars_matrix, which covered the original trilogy of movies, and a second matrix star_wars_matrix2, also 3 x 2, which will contain the same data for the prequels trilogy. By the way, if you ever want to check out the contents of the workspace youre working in, you can type ls() in the console. 1.3.7.1 Creating the prequels matrix star_wars_matrix2 box_office &lt;- c(474.5, 552.5, 310.7, 338.7, 380.3, 468.5) star_wars_matrix2 &lt;- matrix(box_office, nrow = 3, byrow = TRUE, dimnames = list(c(&quot;The Phantom Menace&quot;, &quot;Attack of the Clones&quot;, &quot;Revenge of the Sith&quot;), c(&quot;US&quot;, &quot;non-US&quot;))) 1.3.7.2 Combine both Star Wars trilogies in one matrix all_wars_matrix &lt;- rbind(star_wars_matrix, star_wars_matrix2) Again, we had to be sure, which we were, that the two matrices contained the same number of columns into to be able to successfully merge them. 1.3.8 The total box office revenue for the entire saga Would you use colSums()or rowSums() on the matrix all_wars_matrix to calculate the total box office revenue for the entire saga in the US versus abroad? Thats right! Its colSums()! 1.3.8.1 Total revenue for US and non-US total_revenue_vector &lt;- colSums(all_wars_matrix) 1.3.8.2 Print out total_revenue_vector Head over to the next exercise to learn about matrix subsetting. 1.3.9 Selection of matrix elements Similar to vectors, you can use the square brackets [ ] to select one or multiple elements from a matrix. Whereas vectors have one dimension, matrices have two dimensions. You should therefore use a comma to separate the rows you want to select from the columns. Notice again that rows go first, then columns. For example: my_matrix[1,2] selects the element at the first row and second column. my_matrix[1:3,2:4] results in a matrix with the data on the rows 1, 2, 3 and columns 2, 3, 4. If you want to select all elements of a row or a column, no number is needed before or after the comma, respectively: my_matrix[,1] selects all elements of the first column. my_matrix[1,] selects all elements of the first row. What will my_matrix[1] select? Does it even selest anything?? Back to Star Wars with this newly acquired knowledge! 1.3.9.1 Select the non-US revenue for all movies non_us_all &lt;- all_wars_matrix[,2] 1.3.9.2 Average non-US revenue mean(non_us_all) ## [1] 347.9667 1.3.9.3 Select the non-US revenue for first two movies non_us_some &lt;- all_wars_matrix[1:2,2] 1.3.9.4 Average non-US revenue for first two movies mean(non_us_some) ## [1] 281.15 1.3.10 A little arithmetic with matrices Similar to what you have learned with vectors, the standard operators like +, -, /, *, etc. work in an element-wise way on matrices in R. For example, 2 * my_matrix multiplies each element of my_matrix by two. As a newly-hired data analyst for Lucasfilm, it is your job to find out how many visitors went to each movie for each geographical area. You already have the total revenue figures (in the matrix all_wars_matrix). Assume that the price of a ticket was 5 dollars. Simply dividing the box office numbers by this ticket price gives you the number of visitors. 1.3.10.1 Estimate the visitors visitors &lt;- all_wars_matrix / 5 1.3.10.2 Print the estimate to the console visitors ## US non-US ## A New Hope 92.1996 62.88 ## The Empire Strikes Back 58.0950 49.58 ## Return of the Jedi 61.8612 33.16 ## The Phantom Menace 94.9000 110.50 ## Attack of the Clones 62.1400 67.74 ## Revenge of the Sith 76.0600 93.70 What do these results tell you? A staggering 92 million people went to see A New Hope in US theaters! &gt; 1.3.11 A little arithmetic with matrices (2) After looking at the result of the previous exercise, big boss Lucas points out that the ticket prices went up over time. He asks to redo the analysis based on the prices you can find in ticket_prices_matrix (source: imagination). _Those who are familiar with matrices should note that this is not the standard matrix multiplication for which you should use %*% in R._ 1.3.11.1 Creating ticket_prices_matrix ticket_prices &lt;- c(5.0, 5.0, 6.0, 6.0, 7.0, 7.0, 4.0, 4.0, 4.5, 4.5, 4.9, 4.9) ticket_prices_matrix &lt;- matrix(ticket_prices, nrow = 6, byrow = TRUE, dimnames = list(c(&quot;A New Hope&quot;, &quot;The Empire Strikes Back&quot;, &quot;Return of the Jedi&quot;,&quot;The Phantom Menace&quot;, &quot;Attack of the Clones&quot;, &quot;Revenge of the Sith&quot;), c(&quot;US&quot;, &quot;non-US&quot;))) 1.3.11.2 Estimated number of visitors visitors &lt;- all_wars_matrix / ticket_prices_matrix 1.3.11.3 US visitors us_visitors &lt;- visitors[,1] 1.3.11.4 Average number of US visitors mean(us_visitors) ## [1] 75.01339 This exercise concludes the chapter on matrices. Next stop on your journey through the R language: factors. 1.4 Factors Data often falls into a limited number of categories. For example, human hair color can be categorized as black, brown, blond, red, grey, or whiteand perhaps a few more options for people who color their hair. In R, categorical data is stored in factors. Factors are very important in data analysis, so start learning how to create, subset, and compare them now. 1.4.1 Whats a factor and why would you use it? In this chapter you dive into the wonderful world of factors. The term factor refers to a statistical data type used to store categorical variables. The difference between a categorical variable and a continuous variable is that a categorical variable can belong to a limited number of categories. A continuous variable, on the other hand, can correspond to an infinite number of values. It is important that R knows whether it is dealing with a continuous or a categorical variable, as the statistical models you will develop in the future treat both types differently. (You will see later why this is the case.) A good example of a categorical variable is sex. In many circumstances you can limit the sex categories to Male or Female. (Sometimes you may need different categories. For example, you may need to consider chromosomal variation, hermaphroditic animals, or different cultural norms, but you will always have a finite number of categories.) 1.4.1.1 Assign to the variable theory what this chapter is about! theory &lt;- &quot;factors used for categorical variables&quot; 1.4.2 Whats a factor and why would you use it? (2) To create factors in R, you make use of the function factor(). First thing that you have to do is create a vector that contains all the observations that belong to a limited number of categories. For example, sex_vector contains the sex of 5 different individuals: sex_vector &lt;- c(\"Male\",\"Female\",\"Female\",\"Male\",\"Male\") It is clear that there are two categories, or in R-terms factor levels, at work here: Male and Female. The function factor() will encode the vector as a factor: factor_sex_vector &lt;- factor(sex_vector) 1.4.2.1 Sex vector sex_vector &lt;- c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;) class(sex_vector[2]) ## [1] &quot;character&quot; class(sex_vector) ## [1] &quot;character&quot; So currently R does not know that sex_vector are (or should be treated as) categories. 1.4.2.2 Convert sex_vector to a factor factor_sex_vector &lt;- factor(sex_vector) 1.4.2.3 Print out factor_sex_vector factor_sex_vector ## [1] Male Female Female Male Male ## Levels: Female Male If you want to find out more about the factor() function, do not hesitate to type ?factor in the console. This will open up a help page. Continue to the next exercise. 1.4.3 Whats a factor and why would you use it? (3) There are two types of categorical variables: a nominal categorical variable and an ordinal categorical variable. A nominal variable is a categorical variable without an implied order. This means that it is impossible to say that one is worth more than the other. For example, think of the categorical variable animals_vector with the categories Elephant, Giraffe, Donkey and Horse. Here, it is impossible to say that one stands above or below the other. (Note that some of you might disagree ;-) ). In contrast, ordinal variables do have a natural ordering. Consider for example the categorical variable temperature_vector with the categories: Low, Medium and High. Here it is obvious that Medium stands above Low, and High stands above Medium. 1.4.3.1 Animals animals_vector &lt;- c(&quot;Elephant&quot;, &quot;Giraffe&quot;, &quot;Donkey&quot;, &quot;Horse&quot;) factor_animals_vector &lt;- factor(animals_vector) factor_animals_vector ## [1] Elephant Giraffe Donkey Horse ## Levels: Donkey Elephant Giraffe Horse 1.4.3.2 Temperature temperature_vector &lt;- c(&quot;High&quot;, &quot;Low&quot;, &quot;High&quot;,&quot;Low&quot;, &quot;Medium&quot;) factor_temperature_vector &lt;- factor(temperature_vector, order = TRUE, levels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;)) factor_temperature_vector ## [1] High Low High Low Medium ## Levels: Low &lt; Medium &lt; High Can you already tell whats happening in this exercise? Awesome! Continue to the next exercise and get into the details of factor levels. 1.4.4 Factor levels When you first get a data set, you will often notice that it contains factors with specific factor levels. However, sometimes you will want to change the names of these levels for clarity or other reasons. R allows you to do this with the function levels(): levels(factor_vector) &lt;- c(\"name1\", \"name2\",...) A good illustration is the raw data that is provided to you by a survey. A common question for every questionnaire is the sex of the respondent. Here, for simplicity, just two categories were recorded, M and F. (You usually need more categories for survey data; either way, you use a factor to store the categorical data.) survey_vector &lt;- c(\"M\", \"F\", \"F\", \"M\", \"M\") Recording the sex with the abbreviations M and F can be convenient if you are collecting data with pen and paper, but it can introduce confusion when analyzing the data. At that point, you will often want to change the factor levels to Male and Female instead of M and F for clarity. Watch out: the order with which you assign the levels is important. If you type levels(factor_survey_vector), youll see that it outputs [1] F M. If you dont specify the levels of the factor when creating the vector, R will automatically assign them alphabetically. To correctly map F to Female and M to Male, the levels should be set to c(\"Female\", \"Male\"), in this order. 1.4.4.1 Code to build factor_survey_vector survey_vector &lt;- c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;) factor_survey_vector &lt;- factor(survey_vector) levels(factor_survey_vector) ## [1] &quot;F&quot; &quot;M&quot; 1.4.4.2 Specify the levels of factor_survey_vector levels(factor_survey_vector) &lt;- c(&quot;Female&quot;, &quot;Male&quot;) 1.4.5 Summarizing a factor After finishing this course, one of your favorite functions in R will be summary(). This will give you a quick overview of the contents of a variable: summary(my_var) Going back to our survey, you would like to know how many Male responses you have in your study, and how many Female responses. The summary() function gives you the answer to this question. 1.4.5.1 Generate summary for survey_vector summary(survey_vector) ## Length Class Mode ## 5 character character 1.4.5.2 Generate summary for factor_survey_vector summary(factor_survey_vector) ## Female Male ## 2 3 Have a look at the output. The fact that you identified Male and Female as factor levels in factor_survey_vector enables R to show the number of elements for each category. 1.4.6 Battle of the sexes You might wonder what happens when you try to compare elements of a factor. In factor_survey_vector you have a factor with two levels: Male and Female. But how does R value these relative to each other? 1.4.6.1 Male male &lt;- factor_survey_vector[1] 1.4.6.2 Female female &lt;- factor_survey_vector[2] 1.4.6.3 Battle of the sexes: Male larger than female? male &gt; female ## Warning in Ops.factor(male, female): &#39;&gt;&#39; not meaningful for factors ## [1] NA By default, R returns NA when you try to compare values in a factor, since the idea doesnt make sense. Next youll learn about ordered factors, where more meaningful comparisons are possible. 1.4.7 Ordered factors Since Male and Female are unordered (or nominal) factor levels, R returns a warning message, telling you that the greater than operator is not meaningful. As seen before, R attaches an equal value to the levels for such factors. But this is not always the case! Sometimes you will also deal with factors that do have a natural ordering between its categories. If this is the case, we have to make sure that we pass this information to R Let us say that you are leading a research team of five data analysts and that you want to evaluate their performance. To do this, you track their speed, evaluate each analyst as slow, medium or fast, and save the results in speed_vector. 1.4.7.1 Create speed_vector speed_vector &lt;- c(&quot;medium&quot;, &quot;slow&quot;, &quot;slow&quot;,&quot;medium&quot;, &quot;fast&quot;) 1.4.8 Ordered factors (2) speed_vector should be converted to an ordinal factor since its categories have a natural ordering. By default, the function factor() transforms speed_vector into an unordered factor. To create an ordered factor, you have to add two additional arguments: ordered and levels. factor(some_vector, ordered = TRUE, levels = c(lev1, lev2 )) By setting the argument ordered to TRUE in the function factor(), you indicate that the factor is ordered. With the argument levels you give the values of the factor in the correct order. 1.4.8.1 Convert speed_vector to ordered factor vector factor_speed_vector &lt;- factor(speed_vector, ordered = TRUE, levels = c(&quot;slow&quot;, &quot;medium&quot;, &quot;fast&quot;)) 1.4.8.2 Print factor_speed_vector factor_speed_vector ## [1] medium slow slow medium fast ## Levels: slow &lt; medium &lt; fast Have a look at the console. It is now indicated that the Levels indeed have an order associated, with the &lt; sign. Continue to the next exercise. 1.4.9 Comparing ordered factors Having a bad day at work, data analyst number two enters your office and starts complaining that data analyst number five is slowing down the entire project. Since you know that data analyst number two has the reputation of being a smarty-pants, you first decide to check if his statement is true. The fact that factor_speed_vector is now ordered enables us to compare different elements (the data analysts in this case). You can simply do this by using the well-known operators. 1.4.9.1 Factor value for second data analyst da2 &lt;- factor_speed_vector[2] 1.4.9.2 Factor value for fifth data analyst da5 &lt;- factor_speed_vector[5] 1.4.9.3 Is data analyst 2 faster than data analyst 5? da2 &gt; da5 ## [1] FALSE What do the results tell you? Data analyst two is complaining about the data analyst five while in fact they are the one slowing everything down! This concludes the chapter on factors. With a solid basis in vectors, matrices and factors, youre ready to dive into the wonderful world of data frames, a very important data structure in R! 1.5 Data frames Most datasets you will be working with will be stored as data frames. By the end of this chapter, you will be able to create a data frame, select interesting parts of a data frame, and order a data frame according to certain variables. 1.5.1 Whats a data frame? You may remember from the chapter about matrices that all the elements that you put in a matrix should be of the same type. Back then, your data set on Star Wars only contained numeric elements. When doing a market research survey, however, you often have questions such as: Are you married? or yes/no questions (logical) How old are you? (numeric) What is your opinion on this product? or other open-ended questions (character)  The output, namely the respondents answers to the questions formulated above, is a data set of different data types. You will often find yourself working with data sets that contain different data types instead of only one. A data frame has the variables of a data set as columns and the observations as rows. This will be a familiar concept for those coming from different statistical software packages such as SAS or SPSS. 1.5.1.1 Print out built-in R data frame mtcars ## mpg cyl disp hp drat wt qsec vs am gear ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 ## carb ## Mazda RX4 4 ## Mazda RX4 Wag 4 ## Datsun 710 1 ## Hornet 4 Drive 1 ## Hornet Sportabout 2 ## Valiant 1 ## Duster 360 4 ## Merc 240D 2 ## Merc 230 2 ## Merc 280 4 ## Merc 280C 4 ## Merc 450SE 3 ## Merc 450SL 3 ## Merc 450SLC 3 ## Cadillac Fleetwood 4 ## Lincoln Continental 4 ## Chrysler Imperial 4 ## Fiat 128 1 ## Honda Civic 2 ## Toyota Corolla 1 ## Toyota Corona 1 ## Dodge Challenger 2 ## AMC Javelin 2 ## Camaro Z28 4 ## Pontiac Firebird 2 ## Fiat X1-9 1 ## Porsche 914-2 2 ## Lotus Europa 2 ## Ford Pantera L 4 ## Ferrari Dino 6 ## Maserati Bora 8 ## Volvo 142E 2 1.5.2 Quick, have a look at your data set Wow, that is a lot of cars! Working with large data sets is not uncommon in data analysis. When you work with (extremely) large data sets and data frames, your first task as a data analyst is to develop a clear understanding of its structure and main elements. Therefore, it is often useful to show only a small part of the entire data set. So how to do this in R? Well, the function head() enables you to show the first observations of a data frame. Similarly, the function tail() prints out the last observations in your data set. Both head() and tail() print a top line called the header, which contains the names of the different variables in your data set. 1.5.2.1 Call head() on mtcars head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 tail(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.5 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.5 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.6 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.6 1 1 4 2 So, what do we have in this data set? For example, hp represents the cars horsepower; the Datsun has the lowest horse power of the 6 cars that are displayed. For a full overview of the variables meaning, type ?mtcars in the console and read the help page. Having used ?mtcars to view an explanation of the variables represented in this dataframe, what does the variable am refer to? And hp? 1.5.3 Have a look at the structure Another method that is often used to get a rapid overview of your data is the function str(). The function str() shows you the structure of your data set. For a data frame it tells you: The total number of observations (e.g. 32 types of car were tested) The total number of variables (e.g. 11 car features) A full list of the variables names (e.g. mpg, cyl  ) The data type of each variable (e.g. num) The first observations Applying the str() function will often be the first thing that you do when receiving a new data set or data frame. It is a great way to get more insight into your data set before diving into the real analysis. 1.5.3.1 Investigate the structure of mtcars mtcars ## mpg cyl disp hp drat wt qsec vs am gear ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 ## carb ## Mazda RX4 4 ## Mazda RX4 Wag 4 ## Datsun 710 1 ## Hornet 4 Drive 1 ## Hornet Sportabout 2 ## Valiant 1 ## Duster 360 4 ## Merc 240D 2 ## Merc 230 2 ## Merc 280 4 ## Merc 280C 4 ## Merc 450SE 3 ## Merc 450SL 3 ## Merc 450SLC 3 ## Cadillac Fleetwood 4 ## Lincoln Continental 4 ## Chrysler Imperial 4 ## Fiat 128 1 ## Honda Civic 2 ## Toyota Corolla 1 ## Toyota Corona 1 ## Dodge Challenger 2 ## AMC Javelin 2 ## Camaro Z28 4 ## Pontiac Firebird 2 ## Fiat X1-9 1 ## Porsche 914-2 2 ## Lotus Europa 2 ## Ford Pantera L 4 ## Ferrari Dino 6 ## Maserati Bora 8 ## Volvo 142E 2 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... 1.5.4 Creating a data frame Since using built-in data sets is not even half the fun of creating your own data sets, the rest of this chapter is based on your personally developed data set. Put your jet pack on because it is time for some space exploration! As a first goal, you want to construct a data frame that describes the main characteristics of eight planets in our solar system. According to your good friend Buzz, the main features of a planet are: The type of planet (Terrestrial or Gas Giant). The planets diameter relative to the diameter of the Earth. The planets rotation across the sun relative to that of the Earth. If the planet has rings or not (TRUE or FALSE). After doing some high-quality research on Wikipedia, you feel confident enough to create the necessary vectors: name, type, diameter, rotation and rings. 1.5.4.1 Definition of vectors name &lt;- c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;) type &lt;- c(&quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;) diameter &lt;- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883) rotation &lt;- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67) rings &lt;- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) The first element in each of these vectors correspond to the first observation. 1.5.4.2 Create a data frame from the vectors Next we construct a data frame with the data.frame() function. As arguments, you pass the vectors from before: they will become the different columns of your data frame. Because every column has the same length, the vectors you pass should also have the same length. But dont forget that it is possible (and likely) that they contain different types of data. planets_df &lt;- data.frame(name, type, diameter, rotation, rings) The logical next step, as you know by now, is inspecting the data frame you just created. 1.5.5 Creating a data frame (2) The planets_df data frame we just created should have 8 observations and 5 variables. Lets double check! 1.5.5.1 Check the structure of planets_df str(planets_df) ## &#39;data.frame&#39;: 8 obs. of 5 variables: ## $ name : Factor w/ 8 levels &quot;Earth&quot;,&quot;Jupiter&quot;,..: 4 8 1 3 2 6 7 5 ## $ type : Factor w/ 2 levels &quot;Gas giant&quot;,&quot;Terrestrial planet&quot;: 2 2 2 2 1 1 1 1 ## $ diameter: num 0.382 0.949 1 0.532 11.209 ... ## $ rotation: num 58.64 -243.02 1 1.03 0.41 ... ## $ rings : logi FALSE FALSE FALSE FALSE TRUE TRUE ... Now that you have a clear understanding of the planets_df data set, its time to see how you can select elements from it. 1.5.6 Selection of data frame elements Similar to vectors and matrices, you select elements from a data frame with the help of square brackets [ ]. By using a comma, you can indicate what to select from the rows and the columns respectively. For example: my_df[1,2] selects the value at the first row and second column in my_df. my_df[1:3,2:4] selects the values that appear in columns 2, 3, 4 of rows 1, 2, 3 in my_df. my_df[1, ] selects all elements of the first row. my_df[, 4] selects all elements of the fourth column. 1.5.6.1 Print out diameter of Mercury (row 1, column 3) planets_df[1,3] ## [1] 0.382 1.5.6.2 Print out data for Mars (entire fourth row) planets_df[4,] ## name type diameter rotation rings ## 4 Mars Terrestrial planet 0.532 1.03 FALSE Apart from selecting elements from your data frame by index, you can also use the column names. 1.5.7 Selection of data frame elements (2) Instead of using numerics to select elements of a data frame, you can also use the variable names to select columns of a data frame. Suppose you want to select the first three elements of the type column. One way to do this is planets_df[1:3,2] A possible disadvantage of this approach is that you have to know (or look up) the column number of type, which gets hard if you have a lot of variables. It is often easier to just make use of the variable name: planets_df[1:3,\"type\"] 1.5.7.1 Select the first 5 values of diameter column planets_df[1:5, &quot;diameter&quot;] ## [1] 0.382 0.949 1.000 0.532 11.209 planets_df[, &quot;diameter&quot;] ## [1] 0.382 0.949 1.000 0.532 11.209 9.449 4.007 3.883 1.5.8 Only planets with rings You will often want to select an entire column, namely one specific variable from a data frame. If you want to select all elements of the variable diameter, for example, both of these will do the trick: planets_df[,3] planets_df[,\"diameter\"] However, there is a short-cut. If your columns have names, you can use the $ sign: planets_df$diameter 1.5.8.1 View the diamter variable from planets_df planets_df$diameter ## [1] 0.382 0.949 1.000 0.532 11.209 9.449 4.007 3.883 1.5.8.2 Select the rings variable from planets_df rings_vector &lt;- planets_df$rings 1.5.8.3 Print out rings_vector rings_vector ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE Of course this vector is identical to the rings vector we built in the first place, when construct the planets_df dataframe. Continue to the next exercise and discover yet another way of subsetting! 1.5.9 Only planets with rings (2) You probably remember from high school that some planets in our solar system have rings and others do not. Unfortunately you can not recall their names. Could R help you out? If you type rings_vector in the console, you get: [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE This means that the first four observations (or planets) do not have a ring (FALSE), but the other four do (TRUE). However, you do not get a nice overview of the names of these planets, their diameter, etc. Lets try to use rings_vector to select the data for the four planets with rings. 1.5.9.1 Adapt the code to select all columns for planets with rings planets_df[rings_vector,] ## name type diameter rotation rings ## 5 Jupiter Gas giant 11.209 0.41 TRUE ## 6 Saturn Gas giant 9.449 0.43 TRUE ## 7 Uranus Gas giant 4.007 -0.72 TRUE ## 8 Neptune Gas giant 3.883 0.67 TRUE Notice that weve put this logical vector before the comma. This tells the R to display those observations associate with a TRUE value, and not those with a FALSE value. What would happen if you put rings_vector after the comma? And why? Try it and see! Using planets_df[rings_vector,] is a rather tedious solution. The next exercise will teach you how to do it in a more concise way. 1.5.10 Only planets with rings but shorter So what exactly did you learn in the previous exercises? You selected a subset from a data frame (planets_df) based on whether or not a certain condition was true (rings or no rings), and you managed to pull out all relevant data. Pretty awesome! By now, NASA is probably already flirting with your CV ;-). Now, let us move up one level and use the function subset(). You should see the subset() function as a short-cut to do exactly the same as what you did in the previous exercises. subset(my_df, subset = some_condition) The first argument of subset() specifies the data set for which you want a subset. By adding the second argument, you give R the necessary information and conditions to select the correct subset. The code below will give the exact same result as you got in the previous exercise, but this time, you didnt need to create the rings_vector! subset(planets_df, subset = rings) 1.5.10.1 Select planets with diameter &lt; 1 subset(planets_df, diameter &lt; 1) ## name type diameter rotation rings ## 1 Mercury Terrestrial planet 0.382 58.64 FALSE ## 2 Venus Terrestrial planet 0.949 -243.02 FALSE ## 4 Mars Terrestrial planet 0.532 1.03 FALSE subset(planets_df, rings == TRUE) ## name type diameter rotation rings ## 5 Jupiter Gas giant 11.209 0.41 TRUE ## 6 Saturn Gas giant 9.449 0.43 TRUE ## 7 Uranus Gas giant 4.007 -0.72 TRUE ## 8 Neptune Gas giant 3.883 0.67 TRUE Not only is the subset() function more concise, it is probably also more understandable for people who read your code. 1.5.11 Sorting Making and creating rankings is one of mankinds favorite affairs. These rankings can be useful (best universities in the world), entertaining (most influential movie stars) or pointless (best 007 look-a-like). In data analysis you can sort your data according to a certain variable in the data set. In R, this is done with the help of the function order(). order() is a function that gives you the ranked position of each element when it is applied on a variable, such as a vector for example: 1.5.11.1 Sort the vector a a &lt;- c(1000, 10, 100) order(a) ## [1] 2 3 1 a ## [1] 1000 10 100 10, which is the second element in a, is the smallest element, so 2 comes first in the output of order(a). 100, which is the third element in a is the second smallest element, so 3 comes second in the output of order(a). Note that order(a) has not altered a itself. We can use the output of order(a) to reshuffle a: 1.5.11.2 Reshuffle a in ascending order a[order(a)] ## [1] 10 100 1000 Once more, be aware that we havent altered a at all. We could, by assigning a[order(a)] to a (or to b if we wanted to keep a intact). 1.5.11.3 Play around with the order function in the console Now lets use the order() function to sort your data frame! 1.5.12 Sorting your data frame Alright, now that you understand the order() function, let us do something useful with it. You would like to rearrange your data frame such that it starts with the smallest planet and ends with the largest one. This will be a sort on the diameter column. 1.5.12.1 Use order() to create positions positions &lt;- order(planets_df$diameter) 1.5.12.2 Use positions to sort planets_df planets_df[positions, ] ## name type diameter rotation rings ## 1 Mercury Terrestrial planet 0.382 58.64 FALSE ## 4 Mars Terrestrial planet 0.532 1.03 FALSE ## 2 Venus Terrestrial planet 0.949 -243.02 FALSE ## 3 Earth Terrestrial planet 1.000 1.00 FALSE ## 8 Neptune Gas giant 3.883 0.67 TRUE ## 7 Uranus Gas giant 4.007 -0.72 TRUE ## 6 Saturn Gas giant 9.449 0.43 TRUE ## 5 Jupiter Gas giant 11.209 0.41 TRUE This exercise concludes the chapter on data frames. Remember that data frames are extremely important in R, you will need them all the time. Another very often used data structure is the list. This will be the subject of the next chapter! 1.6 Lists As opposed to vectors, lists can hold components of different types, just as your to-do lists can contain different categories of tasks. This chapter will teach you how to create, name, and subset these lists. 1.6.1 Lists, why would you need them? Congratulations! At this point in the course you are already familiar with: Vectors (one dimensional array): can hold numeric, character or logical values. The elements in a vector all have the same data type. Matrices (two dimensional array): can hold numeric, character or logical values. The elements in a matrix all have the same data type. Data frames (two-dimensional objects): can hold numeric, character or logical values. Within a column all elements have the same data type, but different columns can be of different data type. Pretty sweet for an R newbie, right? ;-) 1.6.2 Lists, why would you need them? (2) A list in R is similar to your to-do list at work or school: the different items on that list most likely differ in length, characteristic, and type of activity that has to be done. A list in R allows you to gather a variety of objects under one name (that is, the name of the list) in an ordered way. These objects can be matrices, vectors, data frames, even other lists, etc. It is not even required that these objects are related to each other in any way. You could say that a list is some kind super data type: you can store practically any piece of information in it! Cool. Lets get our hands dirty! 1.6.3 Creating a list Let us create our first list! To construct a list you use the function list(): my_list &lt;- list(comp1, comp2 ...) The arguments to the list function are the list components. Remember, these components can be matrices, vectors, other lists,  1.6.3.1 Vector with numerics from 1 up to 10 my_vector &lt;- 1:10 1.6.3.2 Matrix with numerics from 1 up to 9 my_matrix &lt;- matrix(1:9, ncol = 3) 1.6.3.3 First 10 elements of the built-in data frame mtcars my_df &lt;- mtcars[1:10,] 1.6.3.4 Construct list with these different elements: my_list &lt;- list(my_vector, my_matrix, my_df) 1.6.3.5 Print my_list my_list ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[3]] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 1.6.4 Creating a named list Well done, youre on a roll! Just like on your to-do list, you want to avoid not knowing or remembering what the components of your list stand for. That is why you should give names to them: my_list &lt;- list(name1 = your_comp1, name2 = your_comp2) This creates a list with components that are named name1, name2, and so on. If you want to name your lists after youve created them, you can use the names() function as you did with vectors. The following commands are fully equivalent to the assignment above: my_list &lt;- list(your_comp1, your_comp2) names(my_list) &lt;- c(\"name1\", \"name2\") 1.6.4.1 Adapt list() call to give the components names my_list &lt;- list(vec = my_vector, mat = my_matrix, df = my_df) 1.6.4.2 Print out my_list my_list ## $vec ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $mat ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## $df ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Great! Not only do you know how to construct lists now, you can also name them; a skill that will prove most useful in practice. Continue to the next exercise. 1.6.5 Creating a named list (2) Being a huge movie fan (remember your job at LucasFilms), you decide to start storing information on good movies with the help of lists. Start by creating a list for the movie The Shining. 1.6.5.1 Creating mov and act mov &lt;- &quot;The Shining&quot; act &lt;- c(&quot;Jack Nicholson&quot;, &quot;Shelley Duvall&quot;, &quot;Danny Lloyd&quot;,&quot;Scatman Crothers&quot;, &quot;Barry Nelson&quot;) 1.6.5.2 Creating rev scores &lt;- c(4.5, 4, 5) sources &lt;- c(&quot;IMDb1&quot;, &quot;IMDb2&quot;, &quot;IMDb3&quot;) comments &lt;- c(&quot;Best Horror Film I Have Ever Seen&quot;, &quot;A truly brilliant and scary film from Stanley Kubrick&quot;, &quot;A masterpiece of psychological horror&quot;) rev &lt;- data.frame(scores, sources, comments) 1.6.5.3 Finish the code to build shining_list shining_list &lt;- list(moviename = mov, actors = act, reviews = rev) Wonderful! You now know how to construct and name lists. As in the previous chapters, lets look at how to select elements for lists. Head over to the next exercise 1.6.6 Selecting elements from a list Your list will often be built out of numerous elements and components. Therefore, getting a single element, multiple elements, or a component out of it is not always straightforward. One way to select a component is using the numbered position of that component. For example, to grab the first component of shining_list you type shining_list[[1]] A quick way to check this out is typing it in the console. Important to remember: to select elements from vectors, you use single square brackets: [ ]. Dont mix them up! You can also refer to the names of the components, with [[ ]] or with the $ sign. Two ways of selecting the data frame representing the reviews: shining_list[[\"reviews\"]] shining_list$reviews Besides selecting components, you often need to select specific elements out of these components. For example, with shining_list[[2]][1] you select from the second component, actors (shining_list[[2]]), the first element ([1]). When you type this in the console, you will see the answer is Jack Nicholson. 1.6.6.1 Print out the vector representing the actors shining_list$actors ## [1] &quot;Jack Nicholson&quot; &quot;Shelley Duvall&quot; &quot;Danny Lloyd&quot; ## [4] &quot;Scatman Crothers&quot; &quot;Barry Nelson&quot; 1.6.6.2 Print the second element of the vector representing the actors shining_list$actors[2] ## [1] &quot;Shelley Duvall&quot; Great! Selecting elements from lists is rather easy isnt it? Continue to the next exercise. 1.6.7 Creating a new list for another movie You found reviews of another, more recent, Jack Nicholson movie: The Departed! Scores Comments 4.6 I would watch it again 5 Amazing! 4.8 I liked it 5 One of the best movies 4.2 Fascinating plot It would be useful to collect together all the pieces of information about the movie, like the title, actors, and reviews into a single variable. Since these pieces of data are different shapes, it is natural to combine them in a list variable. 1.6.7.1 Create movie_title and movie_actors movie_title &lt;- &quot;The Departed&quot; movie_actors &lt;- c(&quot;Leonardo DiCaprio&quot;, &quot;Matt Damon&quot;, &quot;Jack Nicholson&quot;, &quot;Mark Wahlberg&quot;, &quot;Vera Farmiga&quot;, &quot;Martin Sheen&quot;) 1.6.7.2 Use the table from the exercise to define the comments and scores vectors scores &lt;- c(4.6, 5, 4.8, 5, 4.2) comments &lt;- c(&quot;I would watch it again&quot;, &quot;Amazing!&quot;, &quot;I liked it&quot;, &quot;One of the best movies&quot;, &quot;Fascinating plot&quot;) 1.6.7.3 Save the average of the scores vector as avg_review avg_review &lt;- mean(scores) 1.6.7.4 Combine scores and comments into the reviews_df data frame reviews_df &lt;- data.frame(scores, comments) 1.6.7.5 Create and print out a list, called departed_list departed_list &lt;- list(movie_title, movie_actors, reviews_df, avg_review) departed_list ## [[1]] ## [1] &quot;The Departed&quot; ## ## [[2]] ## [1] &quot;Leonardo DiCaprio&quot; &quot;Matt Damon&quot; &quot;Jack Nicholson&quot; ## [4] &quot;Mark Wahlberg&quot; &quot;Vera Farmiga&quot; &quot;Martin Sheen&quot; ## ## [[3]] ## scores comments ## 1 4.6 I would watch it again ## 2 5.0 Amazing! ## 3 4.8 I liked it ## 4 5.0 One of the best movies ## 5 4.2 Fascinating plot ## ## [[4]] ## [1] 4.72 Good work! You successfully created another list of movie information, and combined different components into a single list. Congratulations on finishing the course! 1.7 Challenge [Withdrawn] 1.8 Solutions "],["intermediate-r.html", "Module 2 Intermediate R 2.1 Conditionals and Control Flow 2.2 Loops 2.3 Functions 2.4 The apply family 2.5 Utilities", " Module 2 Intermediate R Intermediate R is the next stop on your journey in mastering the R programming language. In this R training, you will learn about conditional statements, loops, and functions to power your own R scripts. Next, make your R code more efficient and readable using the apply functions. Finally, the utilities chapter gets you up to speed with regular expressions in R, data structure manipulations, and times and dates. This course will allow you to take the next step in advancing your overall knowledge and capabilities while programming in R. 2.1 Conditionals and Control Flow In this chapter, youll learn about relational operators for comparing R objects, and logical operators like and and or for combining TRUE and FALSE values. Then, youll use this knowledge to build conditional statements. 2.1.1 Video: Relational Operators 2.1.2 Equality The most basic form of comparison is equality. Lets briefly recap its syntax. The following statements all evaluate to TRUE (feel free to try them out in the console). 2.1.2.1 Playing around with equalities and inequalities 3 == (2 + 1) ## [1] TRUE &quot;intermediate&quot; != &quot;r&quot; ## [1] TRUE TRUE != FALSE ## [1] TRUE &quot;Rchitect&quot; != &quot;rchitect&quot; ## [1] TRUE Notice from the last expression that R is case sensitive: R is not equal to r. Keep this in mind when solving the exercises in this chapter! 2.1.2.2 Comparison of logicals TRUE == FALSE ## [1] FALSE 2.1.2.3 Comparison of numerics -6 * 14 != 17 - 101 ## [1] FALSE 2.1.2.4 Comparison of character strings &quot;useR&quot; == &quot;user&quot; ## [1] FALSE 2.1.2.5 Compare a logical with a numeric TRUE == 1 ## [1] TRUE Awesome! Since TRUE coerces to 1 under the hood, TRUE == 1 evaluates to TRUE. Make sure not to mix up == (comparison) and = (using for settings in functions). == is what needed to check the equality of R objects. 2.1.3 Greater and less than Apart from equality operators, Filip also introduced the less than and greater than operators: &lt; and &gt;. You can also add an equal sign to express less than or equal to or greater than or equal to, respectively. Have a look at the following R expressions, that all evaluate to FALSE: (1 + 2) &gt; 4 \"dog\" &lt; \"Cats\" TRUE &lt;= FALSE Remember that for string comparison, R determines the greater than relationship based on alphabetical order. Also, keep in mind that TRUE is treated as 1 for arithmetic, and FALSE is treated as 0. Therefore, FALSE &lt; TRUE is TRUE. 2.1.3.1 Comparison of numerics -6 * 5 + 2 &gt;= -10 + 1 ## [1] FALSE 2.1.3.2 Comparison of character strings &quot;raining&quot; &lt;= &quot;raining dogs&quot; ## [1] TRUE 2.1.3.3 Comparison of logicals TRUE &gt; FALSE ## [1] TRUE Make sure to have a look at the console output to see if R returns the results you expected. 2.1.4 Compare vectors You are already aware that R is very good with vectors. Without having to change anything about the syntax, Rs relational operators also work on vectors. But be careful: the comparison is element-by-element, so the tow vectors must have the same number of elements (i.e. they must be the same length). Lets go back to the example that was started in the video. You want to figure out whether your activity on social media platforms have paid off and decide to look at your results for LinkedIn and Facebook. The sample code in the editor initializes the vectors linkedin and facebook. Each of the vectors contains the number of profile views your LinkedIn and Facebook profiles had over the last seven days. 2.1.4.1 Create the vectors linked and facebook (used in the video) linkedin &lt;- c(16, 9, 13, 5, 2, 17, 14) facebook &lt;- c(17, 7, 5, 16, 8, 13, 14) 2.1.4.2 Popular days linkedin &gt; 15 ## [1] TRUE FALSE FALSE FALSE FALSE TRUE FALSE 2.1.4.3 Quiet days linkedin &lt;= 5 ## [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE 2.1.4.4 LinkedIn more popular than Facebook linkedin &gt; facebook ## [1] FALSE TRUE TRUE FALSE FALSE TRUE FALSE Have a look at the console output. Your LinkedIn profile was pretty popular on the sixth day, but less so on the fourth and fifth day. 2.1.5 Compare matrices Rs ability to deal with different data structures for comparisons does not stop at vectors. Matrices and relational operators also work together seamlessly! First well store the LinkedIn and Facebook data in a matrix (rather than in vectors). Well call this matrix views. The first row contains the LinkedIn information; the second row the Facebook information. The original vectors facebook and linkedin are still available as well. 2.1.5.1 Create social media data matrix views &lt;- matrix(c(linkedin, facebook), nrow = 2, byrow = TRUE) 2.1.5.2 When does views equal 13? views == 13 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE TRUE FALSE 2.1.5.3 When is views less than or equal to 14? views &lt;= 14 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE TRUE TRUE TRUE TRUE FALSE TRUE ## [2,] FALSE TRUE TRUE FALSE TRUE TRUE TRUE This exercise concludes the part on comparators. Now that you know how to query the relation between R objects, the next step will be to use the results to alter the behavior of your programs. Find out all about that in the next video! 2.1.6 Video: Logical Operators 2.1.7 &amp; and | Before you work your way through the next exercises, have a look at the following R expressions. All of them will evaluate to TRUE: TRUE &amp; TRUE FALSE | TRUE 5 &lt;= 5 &amp; 2 &lt; 3 3 &lt; 4 | 7 &lt; 6 Watch out: 3 &lt; x &lt; 7 to check if x is between 3 and 7 will not work; youll need 3 &lt; x &amp; x &lt; 7 for that. In this exercise, youll be working with the last variable. Well make this variable equal the last value of the linkedin vector that youve worked with previously. The linkedin vector represents the number of LinkedIn views your profile had in the last seven days, remember? 2.1.7.1 Defining the last variable last &lt;- tail(linkedin, 1) 2.1.7.2 Is last under 5 or above 10? last &lt; 5 | last &gt; 10 ## [1] TRUE 2.1.7.3 Is last between 15 (exclusive) and 20 (inclusive)? last &gt; 15 &amp; last &lt;= 20 ## [1] FALSE Have one last look at the console before proceeding; do the results of the different expressions make sense? 2.1.8 &amp; and | (2) Like relational operators, logical operators work perfectly fine with vectors and matrices. Ready for some advanced queries to gain more insights into your social outreach? 2.1.8.1 linkedin exceeds 10 but facebook below 10 linkedin &gt; 10 &amp; facebook &lt; 10 ## [1] FALSE FALSE TRUE FALSE FALSE FALSE FALSE 2.1.8.2 When were one or both visited at least 12 times? linkedin &gt;= 12 | facebook &gt;= 12 ## [1] TRUE FALSE TRUE TRUE FALSE TRUE TRUE 2.1.8.3 When is views between 11 (exclusive) and 14 (inclusive)? views &gt; 11 &amp; views &lt;= 14 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## [2,] FALSE FALSE FALSE FALSE FALSE TRUE TRUE Youll have noticed how easy it is to use logical operators to vectors and matrices. What do these results tell us? The third day of the recordings was the only day where your LinkedIn profile was visited more than 10 times, while your Facebook profile wasnt. Can you draw similar conclusions for the other results? 2.1.9 Question: Reverse the result: ! 2.1.10 Blend it all together With the things youve learned by now, youre able to solve pretty cool problems. Instead of recording the number of views for your own LinkedIn profile, suppose you conducted a survey inside the company youre working for. Youve asked every employee with a LinkedIn profile how many visits their profile has had over the past seven days. The data will be stored in the matrix employees_views, then a data fram li_df created from this matrix, with appropriate names for the rows. Finally, names will be added to the columns. 2.1.10.1 Creating data frame li_df employees_views &lt;- matrix(c(2, 3, 3, 6, 4, 2, 0, 19, 23, 18, 22, 23, 29, 25, 24, 18, 15, 19, 18, 22, 17, 22, 18, 27, 26, 19, 21, 25, 25, 25, 26, 31, 24, 36, 37, 22, 20, 29, 26, 23, 22, 29, 0, 4, 2, 2, 3, 4, 2, 12, 3, 15, 7, 1, 15, 11, 19, 22, 22, 19, 25, 24, 23, 23, 12, 19, 25, 18, 22, 22, 29, 27, 23, 25, 29, 30, 17, 13, 13, 20, 17, 12, 22, 20, 7, 17, 9, 5, 11, 9, 9, 26, 27, 28, 36, 29, 31, 30, 7, 6, 4, 11, 5, 5, 15, 32, 35, 31, 35, 24, 25, 36, 7, 17, 9, 12, 13, 6, 12, 9, 6, 3, 12, 3, 8, 6, 0, 1, 11, 6, 0, 4, 11, 9, 12, 6, 13, 12, 13, 11, 6, 15, 15, 10, 9, 7, 18, 17, 17, 12, 4, 14, 17, 7, 1, 12, 8, 2, 4, 4, 11, 5, 8, 0, 1, 6, 3, 1, 2, 7, 5, 3, 1, 5, 5, 29, 25, 32, 28, 28, 27, 27, 17, 15, 17, 23, 23, 17, 22, 26, 32, 33, 30, 33, 28, 26, 27, 29, 24, 29, 26, 31, 28, 4, 1, 1, 2, 1, 7, 4, 22, 22, 17, 20, 14, 19, 13, 9, 11, 7, 10, 8, 15, 5, 6, 5, 12, 5, 17, 17, 4, 18, 17, 12, 22, 22, 13, 12, 2, 12, 13, 7, 10, 6, 2, 32, 26, 20, 23, 24, 25, 21, 5, 13, 12, 11, 6, 5, 10, 6, 10, 11, 6, 6, 2, 5, 30, 37, 32, 35, 37, 41, 42, 34, 33, 32, 35, 33, 27, 35, 15, 19, 21, 18, 22, 26, 22, 28, 29, 30, 19, 21, 19, 26, 6, 8, 6, 7, 17, 11, 14, 17, 22, 27, 24, 18, 28, 24, 6, 10, 17, 18, 13, 10, 7, 18, 19, 22, 17, 21, 15, 23, 21, 27, 28, 28, 26, 17, 25, 10, 18, 20, 18, 12, 19, 17, 6, 15, 15, 15, 10, 14, 2, 30, 28, 29, 31, 24, 20, 25), nrow = 50, byrow = TRUE) li_df &lt;- data.frame(employees_views, row.names = c(&quot;employee_1&quot;, &quot;employee_2&quot;, &quot;employee_3&quot;, &quot;employee_4&quot;, &quot;employee_5&quot;, &quot;employee_6&quot;, &quot;employee_7&quot;, &quot;employee_8&quot;, &quot;employee_9&quot;, &quot;employee_10&quot;, &quot;employee_11&quot;, &quot;employee_12&quot;, &quot;employee_13&quot;, &quot;employee_14&quot;, &quot;employee_15&quot;, &quot;employee_16&quot;, &quot;employee_17&quot;, &quot;employee_18&quot;, &quot;employee_19&quot;, &quot;employee_20&quot;, &quot;employee_21&quot;, &quot;employee_22&quot;, &quot;employee_23&quot;, &quot;employee_24&quot;, &quot;employee_25&quot;, &quot;employee_26&quot;, &quot;employee_27&quot;, &quot;employee_28&quot;, &quot;employee_29&quot;, &quot;employee_30&quot;, &quot;employee_31&quot;, &quot;employee_32&quot;, &quot;employee_33&quot;, &quot;employee_34&quot;, &quot;employee_35&quot;, &quot;employee_36&quot;, &quot;employee_37&quot;, &quot;employee_38&quot;, &quot;employee_39&quot;, &quot;employee_40&quot;, &quot;employee_41&quot;, &quot;employee_42&quot;, &quot;employee_43&quot;, &quot;employee_44&quot;, &quot;employee_45&quot;, &quot;employee_46&quot;, &quot;employee_47&quot;, &quot;employee_48&quot;, &quot;employee_49&quot;, &quot;employee_50&quot;) ) names(li_df)[1] &lt;- &quot;day1&quot; names(li_df)[2] &lt;- &quot;day2&quot; names(li_df)[3] &lt;- &quot;day3&quot; names(li_df)[4] &lt;- &quot;day4&quot; names(li_df)[5] &lt;- &quot;day5&quot; names(li_df)[6] &lt;- &quot;day6&quot; names(li_df)[7] &lt;- &quot;day7&quot; 2.1.10.2 Select the second column, named day2, from li_df: second second &lt;- li_df[, 2] 2.1.10.3 Build a logical vector, TRUE if value in second is extreme: extremes extremes &lt;- second &lt; 5 | second &gt; 25 2.1.10.4 Count the number of TRUEs in extremes sum(extremes) ## [1] 16 Head over to the next video and learn how relational and logical operators can be used to alter the flow of your R scripts. 2.1.11 Video: Conditional Statements 2.1.12 The if statement Before diving into some exercises on the if statement, have another look at its syntax: if (condition) { expr } Remember your vectors with social profile views? Lets look at it from another angle. We create a variable called medium which gives information about the social website, and another called num_views which denotes the actual number of views that particular medium had on the last day of your recordings. Defining these variables related to your last day of recordings medium &lt;- &quot;LinkedIn&quot; num_views &lt;- 14 2.1.12.1 Examine the if statement for medium if (medium == &quot;LinkedIn&quot;) { print(&quot;Showing LinkedIn information&quot;) } ## [1] &quot;Showing LinkedIn information&quot; 2.1.12.2 Write the if statement for num_views if (num_views &gt; 15) { print(&quot;You are popular!&quot;) } Try to see what happens if you change the medium and num_views variables and run your code again. Lets further customize these if statements in the next exercise. 2.1.13 Add an else You can only use an else statement in combination with an if statement. The else statement does not require a condition; its corresponding code is simply run if all of the preceding conditions in the control structure are FALSE. Heres a recipe for its usage: if (condition) { expr1 } else { expr2 } Its important that the else keyword comes on the same line as the closing bracket of the if part! We will now extend the if statements that we coded in the previous exercises with the appropriate else statements! 2.1.13.1 Control structure for medium if (medium == LinkedIn) { print(Showing LinkedIn information) } else { print(Unknown medium) } 2.1.13.2 Control structure for num_views if (num_views &gt; 15) { print(Youre popular!) } else { print(Try to be more visible!) } You also had Facebook information available, remember? Time to add some more statements to our control structures using else if! 2.1.14 Customize further: else if The else if statement allows you to further customize your control structure. You can add as many else if statements as you like. Keep in mind that R ignores the remainder of the control structure once a condition has been found that is TRUE and the corresponding expressions have been executed. Heres an overview of the syntax to freshen your memory: if (condition1) { expr1 } else if (condition2) { expr2 } else if (condition3) { expr3 } else { expr4 } Again, its important that the else if keywords come on the same line as the closing bracket of the previous part of the control construct. 2.1.14.1 Control structure for medium if (medium == &quot;LinkedIn&quot;) { print(&quot;Showing LinkedIn information&quot;) } else if (medium == &quot;Facebook&quot;) { print(&quot;Showing Facebook information&quot;) } else { print(&quot;Unknown medium&quot;) } ## [1] &quot;Showing LinkedIn information&quot; 2.1.14.2 Control structure for num_views if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) } else if (num_views &lt;= 15 &amp; num_views &gt; 10) { print(&quot;Your number of views is average&quot;) } else { print(&quot;Try to be more visible!&quot;) } ## [1] &quot;Your number of views is average&quot; Have another look at the second control structure. Because R abandons the control flow as soon as it finds a condition that is met, you can simplify the condition for the else if part in the second construct to num_views &gt; 10. 2.1.15 Question: Else if 2.0 2.1.16 Take control! In this exercise, you will combine everything that youve learned so far: relational operators, logical operators and control constructs. Youll need it all! 2.1.16.1 Define li and fb li &lt;- 15 fb &lt;- 9 These two variables, li and fb denote the number of profile views your LinkedIn and Facebook profile had on the last day of recordings. Go through the instructions to create R code that generates a social media score, sms, based on the values of li and fb. 2.1.16.1.1 Code the control-flow construct if (li &gt;= 15 &amp; fb &gt;= 15) { sms &lt;- 2 * (li + fb) } else if (li &lt; 10 &amp; fb &lt; 10) { sms &lt;- 0.5 * (li + fb) } else { sms &lt;- li + fb } 2.1.16.2 Print the resulting sms to the console sms ## [1] 24 Feel free to play around some more with your solution by changing the values of li and fb. 2.2 Loops Loops can come in handy on numerous occasions. While loops are like repeated if statements, the for loop is designed to iterate over all elements in a sequence. Learn about them in this chapter. 2.2.1 Video: While loop 2.2.2 Write a while loop Lets get you started with building a while loop from the ground up. Have another look at its recipe: while (condition) { expr } Remember that the condition part of this recipe should becomeFALSEat some point during the execution. Otherwise, thewhile` loop will go on indefinitely. If your session expires when you run your code, check the body of your while loop carefully. Have a look at the code below; it initializes the speed variables and already provides a while loop template to get you started. 2.2.2.1 Initialize the speed variable speed &lt;- 64 2.2.2.2 Code the while loop while (speed &gt; 30) { print(&quot;Slow down!&quot;) speed &lt;- speed - 7 } ## [1] &quot;Slow down!&quot; ## [1] &quot;Slow down!&quot; ## [1] &quot;Slow down!&quot; ## [1] &quot;Slow down!&quot; ## [1] &quot;Slow down!&quot; 2.2.2.3 Print out the speed variable speed ## [1] 29 2.2.3 Throw in more conditionals In the previous exercise, you simulated the interaction between a driver and a drivers assistant: When the speed was too high, Slow down! got printed out to the console, resulting in a decrease of your speed by 7 units. There are several ways in which you could make your drivers assistant more advanced. For example, the assistant could give you different messages based on your speed or provide you with a current speed at a given moment. A while loop similar to the one youve coded in the previous exercise is already available in the editor. It prints out your current speed, but theres no code that decreases the speed variable yet, which is pretty dangerous. Can you make the appropriate changes? Note that well need to assign the value of 64 to the variable speed, as it currently has the value 29. 2.2.3.1 Initialize the speed variable speed &lt;- 64 2.2.3.2 Extend/adapt the while loop while (speed &gt; 30) { print(paste(&quot;Your speed is&quot;,speed)) if (speed &gt; 48) { print(&quot;Slow down big time!&quot;) speed &lt;- speed - 11 } else { print(&quot;Slow down!&quot;) speed &lt;- speed - 6 } } ## [1] &quot;Your speed is 64&quot; ## [1] &quot;Slow down big time!&quot; ## [1] &quot;Your speed is 53&quot; ## [1] &quot;Slow down big time!&quot; ## [1] &quot;Your speed is 42&quot; ## [1] &quot;Slow down!&quot; ## [1] &quot;Your speed is 36&quot; ## [1] &quot;Slow down!&quot; To further improve our driver assistant model, head over to the next exercise! 2.2.4 Stop the while loop: break There are some very rare situations in which severe speeding is necessary: what if a hurricane is approaching and you have to get away as quickly as possible? You dont want the drivers assistant sending you speeding notifications in that scenario, right? This seems like a great opportunity to include the break statement in the while loop youve been working on. Remember that the break statement is a control statement. When R encounters it, the while loop is abandoned completely. Once again, we begin by initialising the speed variable. 2.2.4.1 Initialize the speed variable speed &lt;- 88 2.2.4.2 Adding a break to our while loop while (speed &gt; 30) { print(paste(&quot;Your speed is&quot;, speed)) if (speed &gt; 80) { break } if (speed &gt; 48) { print(&quot;Slow down big time!&quot;) speed &lt;- speed - 11 } else { print(&quot;Slow down!&quot;) speed &lt;- speed - 6 } } ## [1] &quot;Your speed is 88&quot; Now that youve correctly solved this exercise, feel free to play around with different values of speed to see how the while loop handles the different cases. 2.2.5 Build a while loop from scratch The previous exercises guided you through developing a pretty advanced while loop, containing a break statement and different messages and updates as determined by control flow constructs. If you manage to solve this comprehensive exercise using a while loop, youre totally ready for the next topic: the for loop. 2.2.5.1 Initialize i as 1 i &lt;- 1 2.2.5.2 Code the while loop while (i &lt;= 10) { print(3 * i) if (3 * i %% 8 == 0) { break } i &lt;- i + 1 } ## [1] 3 ## [1] 6 ## [1] 9 ## [1] 12 ## [1] 15 ## [1] 18 ## [1] 21 ## [1] 24 Head over to the next video! 2.2.6 Video: For loop Loop over a vector In the previous video, Filip told you about two different strategies for using the for loop. To refresh your memory, consider the following loops that are equivalent in R: primes &lt;- c(2, 3, 5, 7, 11, 13) # loop version 1 for (p in primes) { print(p) } # loop version 2 for (i in 1:length(primes)) { print(primes[i]) } Remember our linkedin vector? Its a vector that contains the number of views your LinkedIn profile had in the last seven days. Lets remember ourselves of the vector. 2.2.6.1 Print the linkedin vector linkedin ## [1] 16 9 13 5 2 17 14 2.2.6.2 Loop version 1 for(elements in linkedin) { print (elements) } ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 2.2.6.3 Loop version 2 for(i in 1:length(linkedin)) { print (linkedin[i]) } ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 2.2.7 Loop over a list Looping over a list is just as easy and convenient as looping over a vector. There are again two different approaches here: primes_list &lt;- list(2, 3, 5, 7, 11, 13) #### loop version 1 for (p in primes_list) { print(p) } #### loop version 2 for (i in 1:length(primes_list)) { print(primes_list[[i]]) } Recall from earlier that to select elements from lists, rather than single square brackets, we need double square brackets [[ ]]. You will see them again in loop version 2 above. Suppose you have a list of all sorts of information on New York City: its population size, the names of the boroughs, and whether it is the capital of the United States. We first prepare a list nyc with all this information (source: Wikipedia). 2.2.7.1 Specify nyc list nyc &lt;- list(pop = 8405837, boroughs = c(&quot;Manhattan&quot;, &quot;Bronx&quot;, &quot;Brooklyn&quot;, &quot;Queens&quot;, &quot;Staten Island&quot;), capital = FALSE) 2.2.7.2 Loop version 1 for(elements in nyc) { print(elements) } ## [1] 8405837 ## [1] &quot;Manhattan&quot; &quot;Bronx&quot; &quot;Brooklyn&quot; &quot;Queens&quot; ## [5] &quot;Staten Island&quot; ## [1] FALSE 2.2.7.3 Loop version 2 for(i in 1:length(nyc)) { print(nyc[[i]]) } ## [1] 8405837 ## [1] &quot;Manhattan&quot; &quot;Bronx&quot; &quot;Brooklyn&quot; &quot;Queens&quot; ## [5] &quot;Staten Island&quot; ## [1] FALSE Filip mentioned that for loops can also be used for matrices. Lets put that to a test in the next exercise. 2.2.8 Loop over a matrix Well define a matrix ttt, that represents the status of a tic-tac-toe game. It contains the values X, O and NA. Well print out ttt in the console once its be defined to get a closer look. On row 1 and column 1, theres O, while on row 3 and column 2 theres NA. 2.2.8.1 Define ttt ttt &lt;- matrix(c(&quot;O&quot;, NA, &quot;X&quot;, NA, &quot;O&quot;, &quot;O&quot;, &quot;X&quot;, NA, &quot;X&quot;), byrow = TRUE, nrow = 3) ttt ## [,1] [,2] [,3] ## [1,] &quot;O&quot; NA &quot;X&quot; ## [2,] NA &quot;O&quot; &quot;O&quot; ## [3,] &quot;X&quot; NA &quot;X&quot; To solve this exercise, youll need a for loop inside a for loop, often called a nested loop. Doing this in R is a breeze! Simply use the following recipe: for (var1 in seq1) { for (var2 in seq2) { expr } } 2.2.8.2 define the double for loop for (i in 1:nrow(ttt)) { for (j in 1:ncol(ttt)) { print(paste(&quot;On row &quot;, i, &quot; and column &quot;, j, &quot; the board contains &quot;, ttt[i,j])) } } ## [1] &quot;On row 1 and column 1 the board contains O&quot; ## [1] &quot;On row 1 and column 2 the board contains NA&quot; ## [1] &quot;On row 1 and column 3 the board contains X&quot; ## [1] &quot;On row 2 and column 1 the board contains NA&quot; ## [1] &quot;On row 2 and column 2 the board contains O&quot; ## [1] &quot;On row 2 and column 3 the board contains O&quot; ## [1] &quot;On row 3 and column 1 the board contains X&quot; ## [1] &quot;On row 3 and column 2 the board contains NA&quot; ## [1] &quot;On row 3 and column 3 the board contains X&quot; Notice that this loop when through the whole of row 1 before moving onto row 2. This makes sense, as the rows are the outer loop and columns are the inner loop. Youre sufficiently comfortable with basic for looping, so its time to step it up a notch! 2.2.9 Mix it up with control flow Lets return to the LinkedIn profile views data, stored in a vector linkedin. In the first exercise on for loops you already did a simple printout of each element in this vector. A little more in-depth interpretation of this data wouldnt hurt, right? Time to throw in some conditionals! As with the while loop, you can use the if and else statements inside the for loop. 2.2.9.1 Code the for loop with conditionals for (li in linkedin) { if (li &gt; 10) { print (&quot;You&#39;re popular!&quot;) } else { print (&quot;Be more visible!&quot;) } print(li) } ## [1] &quot;You&#39;re popular!&quot; ## [1] 16 ## [1] &quot;Be more visible!&quot; ## [1] 9 ## [1] &quot;You&#39;re popular!&quot; ## [1] 13 ## [1] &quot;Be more visible!&quot; ## [1] 5 ## [1] &quot;Be more visible!&quot; ## [1] 2 ## [1] &quot;You&#39;re popular!&quot; ## [1] 17 ## [1] &quot;You&#39;re popular!&quot; ## [1] 14 In the next exercise, youll customize this for loop even further with break and next statements. 2.2.10 Next, you break it In the editor on the right youll find a possible solution to the previous exercise. The code loops over the linkedin vector and prints out different messages depending on the values of li. In this exercise, you will use the break and next statements: The break statement abandons the active loop: the remaining code in the loop is skipped and the loop is not iterated over anymore. The next statement skips the remainder of the code in the loop, but continues the iteration. 2.2.10.1 Adapt/extend the for loop for (li in linkedin) { if (li &gt; 16) { print (&quot;This is ridiculous, I&#39;m outta here!&quot;) break } if (li &lt; 5) { print (&quot;This is too embarrassing!&quot;) next } if (li &gt; 10) { print(&quot;You&#39;re popular!&quot;) } else { print(&quot;Be more visible!&quot;) } print(li) } ## [1] &quot;You&#39;re popular!&quot; ## [1] 16 ## [1] &quot;Be more visible!&quot; ## [1] 9 ## [1] &quot;You&#39;re popular!&quot; ## [1] 13 ## [1] &quot;Be more visible!&quot; ## [1] 5 ## [1] &quot;This is too embarrassing!&quot; ## [1] &quot;This is ridiculous, I&#39;m outta here!&quot; for, break, next? We name it, you can do it! 2.2.11 Build a for loop from scratch This exercise will not introduce any new concepts on for loops. We first define a variable rquote, then split this variable up into a vector that contains separate letters, and store them in a vector chars using the strsplit() function. Can you write code that counts the number of rs that come before the first u in rquote? 2.2.11.1 Pre-defined variables rquote &lt;- &quot;r&#39;s internals are irrefutably intriguing&quot; chars &lt;- strsplit(rquote, split = &quot;&quot;)[[1]] 2.2.11.2 Initialize rcount rcount &lt;- 0 2.2.11.3 Finish the for loop for (char in chars) { if (char == &quot;u&quot;) { break } if (char == &quot;r&quot;) { rcount &lt;- rcount + 1 } } 2.2.11.4 Print out rcount rcount ## [1] 5 For-midable! This exercise concludes the chapter on while and for loops. 2.3 Functions Functions are an extremely important concept in almost every programming language, and R is no different. Learn what functions are and how to use themthen take charge by writing your own functions. 2.3.1 Video: Introduction to functions 2.3.2 Function documentation Before even thinking of using an R function, you should clarify which arguments it expects. All the relevant details such as a description, usage, and arguments can be found in the documentation. To consult the documentation on the sample() function, for example, you can use one of following R commands: help(sample) ?sample If you execute these commands in the console of the DataCamp interface, youll be redirected to www.rdocumentation.org. If you execute these commands in the console of an IDE (integrated development environment) such as RStudio, the documentation will open in the Help panel. A quick hack to see the arguments of the sample() function is the args() function. Try it out in the console: args(sample) In the next exercises, youll be learning how to use the mean() function with increasing complexity. The first thing youll have to do is get acquainted with the mean() function. 2.3.2.1 Inspect the arguments of the mean() function args(mean) ## function (x, ...) ## NULL That wasnt too hard, was it? Take a look at the documentation and head over to the next exercise. 2.3.3 Use a function The documentation on the mean() function gives us quite some information: The mean() function computes the arithmetic mean. The most general method takes multiple arguments: x and .... The x argument should be a vector containing numeric, logical or time-related information. (Remember what we learnt about the numeric values of TRUE and FALSE to understand how you could take an average of logical values!) Remember that R can match arguments both by position and by name. Can you still remember the difference? Youll find out in this exercise! Once more, youll be working with the view counts of your social network profiles for the past 7 days. 2.3.3.1 Calculate average number of views avg_li &lt;- mean(linkedin) avg_fb &lt;- mean(facebook) 2.3.3.2 Inspect avg_li and avg_fb avg_li ## [1] 10.85714 avg_fb ## [1] 11.42857 Im sure youve already called more advanced R functions in your history as a programmer. Now you also know what actually happens under the hood ;-) 2.3.4 Use a function (2) Check the documentation on the mean() function again: ?mean The Usage section of the documentation includes two versions of the mean() function. The first usage, mean(x, ...) is the most general usage of the mean function. The Default S3 method, however, is: mean(x, trim = 0, na.rm = FALSE, ...) The ... is called the ellipsis. It is a way for R to pass arguments along without the function having to name them explicitly. The ellipsis will be treated in more detail in future courses. For the remainder of this exercise, just work with the second usage of the mean function. Notice that both trim and na.rm have default values. This makes them optional arguments. 2.3.4.1 Calculate the mean of the sum avg_sum &lt;- mean(linkedin + facebook) 2.3.4.2 Calculate the trimmed mean of the sum avg_sum_trimmed &lt;- mean(linkedin + facebook, trim = 0.2) 2.3.4.3 Inspect both new variables avg_sum ## [1] 22.28571 avg_sum_trimmed ## [1] 22.6 When the trim argument is not zero, it chops off a fraction (equal to trim) of the vector you pass as argument x. 2.3.5 Use a function (3) In the video, Filip guided you through the example of specifying arguments of the sd() function. The sd() function has an optional argument, na.rm that specified whether or not to remove missing values from the input vector before calculating the standard deviation. If youve had a good look at the documentation, youll know by now that the mean() function also has this argument, na.rm, and it does the exact same thing. By default, it is set to FALSE, as the Usage of the default S3 method shows: mean(x, trim = 0, na.rm = FALSE, ...) Lets see what happens if your vectors linkedin and facebook contain missing values (NA). 2.3.5.1 The linkedin and facebook vectors have been amended to include some NAs linkedin &lt;- c(16, 9, 13, 5, NA, 17, 14) facebook &lt;- c(17, NA, 5, 16, 8, 13, 14) 2.3.5.2 Basic average of linkedin mean(linkedin) ## [1] NA 2.3.5.3 Advanced average of linkedin mean(linkedin, na.rm = TRUE) ## [1] 12.33333 2.3.6 Functions inside functions You already know that R functions return objects that you can then use somewhere else. This makes it easy to use functions inside functions, as youve seen before: speed &lt;- 31 print(paste(\"Your speed is\", speed)) Notice that both the print() and paste() functions use the ellipsis -  - as an argument. Can you figure out how theyre used? 2.3.6.1 Calculate the mean absolute deviation mean(abs(linkedin - facebook), na.rm = TRUE) ## [1] 4.8 2.3.7 Question: Required, or optional? Using functions that are already available in R is pretty straightforward, but how about writing your own functions to supercharge your R programs? The next video will tell you how. 2.3.8 Video: Writing functions 2.3.9 Write your own function Wow, things are getting serious youre about to write your own function! Before you have a go at it, have a look at the following function template: my_fun &lt;- function(arg1, arg2) { body } Notice that this recipe uses the assignment operator (&lt;-) just as if you were assigning a vector to a variable for example. This is not a coincidence. Creating a function in R basically is the assignment of a function object to a variable! In the recipe above, youre creating a new R variable my_fun, that becomes available in the workspace as soon as you execute the definition. From then on, you can use the my_fun as a function. 2.3.9.1 Create a function pow_two() pow_two &lt;- function(x) { x ^ 2 } 2.3.9.2 Use the function pow_two(12) ## [1] 144 2.3.9.3 Create a function sum_abs() sum_abs &lt;- function(x, y) { abs(x) + abs(y) } 2.3.9.4 Use the function sum_abs(-2, 3) ## [1] 5 Step it up a notch in the next exercise! 2.3.10 Write your own function (2) There are situations in which your function does not require an input. Lets say you want to write a function that gives us the random outcome of throwing a fair die: throw_die &lt;- function() { number &lt;- sample(1:6, size = 1) number } throw_die() Up to you to code a function that doesnt take any arguments! 2.3.10.1 Define the function hello() hello &lt;- function() { print(&quot;Hi there!&quot;) return(TRUE) } 2.3.10.2 Call the function hello() hello() ## [1] &quot;Hi there!&quot; ## [1] TRUE 2.3.11 Write your own function (3) Do you still remember the difference between an argument with and without default values? Have another look at the sd() function by typing ?sd in the console. The usage section shows the following information: sd(x, na.rm = FALSE) This tells us that x has to be defined for the sd() function to be called correctly, however, na.rm already has a default value. Not specifying this argument wont cause an error. You can define default argument values in your own R functions as well. You can use the following recipe to do so: my_fun &lt;- function(arg1, arg2 = val2) { body } The editor on the right already includes an extended version of the pow_two() function from before. Can you finish it? 2.3.11.1 Finish the pow_two() function pow_two &lt;- function(x, print_info = TRUE) { y &lt;- x ^ 2 if (print_info == TRUE) { print(paste(x, &quot;to the power two equals&quot;, y)) } return(y) } 2.3.11.2 Playing around with pow_twos new argument pow_two(12) ## [1] &quot;12 to the power two equals 144&quot; ## [1] 144 pow_two(12, print_info = TRUE) ## [1] &quot;12 to the power two equals 144&quot; ## [1] 144 pow_two(12, print_info = FALSE) ## [1] 144 Have you tried calling this pow_two() function? Try pow_two(5), pow_two(5, TRUE) and pow_two(5, FALSE). Which ones give different results? 2.3.12 Question: Function scoping Normally I dont write the question text here. However, in the case of this question, I think its useful. The question goes like this An issue that Filip did not discuss in the video is function scoping. It implies that variables that are defined inside a function are not accessible outside that function. Try running the following code and see if you understand the results: pow_two &lt;- function(x) { y &lt;- x ^ 2 return(y) } pow_two(4) ## [1] 16 Did you trying calling y and x? Did you receive an error? y was defined inside the pow_two() function and therefore it is not accessible outside of that function. This is also true for the functions arguments of course - x in this case. If youre familiar with other programming languages, you might wonder whether R passes arguments by value or by reference. Find out in the next exercise! 2.3.13 Question: R passes arguments by value Once again, the text of this question is quite useful to us, so Ill reprint it. The title gives it away already: R passes arguments by value. What does this mean? Simply put, it means that an R function cannot change the variable that you input to that function. Lets look at a simple example (try it in the console): triple &lt;- function(x) { x &lt;- 3*x x } a &lt;- 5 triple(a) ## [1] 15 a ## [1] 5 Inside the triple() function, the argument x gets overwritten with its value times three. Afterwards this new x is returned. If you call this function with a variable a set equal to 5, you obtain 15. But did the value of a change? If R were to pass a to triple() by reference, the override of the x inside the function would ripple through to the variable a, outside the function. However, R passes by value, so the R objects you pass to a function can never change unless you do an explicit assignment. a remains equal to 5, even after calling triple(a). Given that R passes arguments by value and not by reference, the value of count is not changed after the first two calls of increment(). Only in the final expression, where count is re-assigned explicitly, does the value of count change. 2.3.14 R you functional? Now that youve acquired some skills in defining functions with different types of arguments and return values, you should try to create more advanced functions. As youve noticed in the previous exercises, its perfectly possible to add control-flow constructs, loops and even other functions to your function body. Remember our social media example, using the vectors linkedin and facebook? As a first step, you will be writing a function that can interpret a single value of this vector. In the next exercise, you will write another function that can handle an entire vector at once. Note that the linkedin and facebook vectors will be returned to their original forms (without NAs). 2.3.14.1 Define linkedin and facebook linkedin &lt;- c(16, 9, 13, 5, 2, 17, 14) facebook &lt;- c(17, 7, 5, 16, 8, 13, 14) 2.3.14.2 Define the interpret function interpret &lt;- function(num_views) { if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) return (num_views) } else { print(&quot;Try to be more visible!&quot;) return(0) } } 2.3.14.3 Call the interpret function twice interpret(linkedin[1]) ## [1] &quot;You&#39;re popular!&quot; ## [1] 16 interpret(facebook[2]) ## [1] &quot;Try to be more visible!&quot; ## [1] 0 The annoying thing here is that interpret() only takes one argument. Proceed to the next exercise to implement something more useful. 2.3.15 R you functional? (2) A possible implementation of the interpret() function is already available in the editor. In this exercise youll be writing another function that will use the interpret() function to interpret all the data from your daily profile views inside a vector. Furthermore, your function will return the sum of views on popular days, if asked for. A for loop is ideal for iterating over all the vector elements. The ability to return the sum of views on popular days is something you can code through a function argument with a default value. 2.3.15.1 The interpret() can be used inside interpret_all() interpret &lt;- function(num_views) { if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) return(num_views) } else { print(&quot;Try to be more visible!&quot;) return(0) } } 2.3.15.2 Define the interpret_all() function 2.3.15.3 views: vector with data to interpret 2.3.15.4 return_sum: return total number of views on popular days? interpret_all &lt;- function(views, return_sum = TRUE) { count &lt;- 0 for (v in views) { count &lt;- count + interpret(v) } if (return_sum == TRUE) { return(count) } else { return(NULL) } } 2.3.15.5 Call the interpret_all() function on both linkedin and facebook interpret_all(linkedin) ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] 33 interpret_all(facebook) ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] 33 Have a look at the results; it appears that the sum of views on popular days are the same for Facebook and LinkedIn, what a coincidence! Your different social profiles must be fairly balanced ;-) Head over to the next video! 2.3.16 Video: R packages 2.3.17 Load an R Package There are basically two extremely important functions when it comes down to R packages: install.packages(), which as you can expect, installs a given package. library() which loads packages, i.e. attaches them to the search list on your R workspace. To install packages, you need administrator privileges. This means that install.packages() will thus not work in the DataCamp interface. However, almost all CRAN packages are installed on our servers. You can load them with library(). In this exercise, youll be learning how to load the ggplot2 package, a powerful package for data visualization. Youll use it to create a plot of two variables of the mtcars data frame. The data has already been prepared for you in the workspace. Before starting, execute the following commands in the console: search(), to look at the currently attached packages and qplot(mtcars$wt, mtcars$hp), to build a plot of two variables of the mtcars data frame. An error should occur, because you havent loaded the ggplot2 package yet! 2.3.17.1 Load the ggplot2 package library(ggplot2) 2.3.17.2 Retry the qplot() function qplot(mtcars$wt, mtcars$hp) 2.3.17.3 Check out the currently attached packages again search() ## [1] &quot;.GlobalEnv&quot; &quot;package:tidyr&quot; &quot;package:babynames&quot; ## [4] &quot;package:dplyr&quot; &quot;package:ggplot2&quot; &quot;tools:rstudio&quot; ## [7] &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; ## [10] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; ## [13] &quot;Autoloads&quot; &quot;package:base&quot; Notice how search() and library() are closely interconnected functions. Head over to the next exercise. 2.3.17.4 Question: Different ways to load a package The library() and require() functions are not very picky when it comes down to argument types: both library(rjson) and library(\"rjson\") work perfectly fine for loading a package. Only chunk 1 and chunk 2 are correct. Can you figure out why the last two arent valid? The warning you receive with chunk 4 makes it quite clear whats wrong there. For chunk 3, it seems that the original author of the require() function wanted to allow people to be lazy, and not have to enclose the package name with quote marks \"\". To do this, they include a default setting within require(). View this using the args()function. Can you see why the changing on this default setting in chunk 4 combined with the lack of quotation marks throws an error? This exercise concludes the chapter on functions. Well done! 2.4 The apply family Whenever youre using a for loop, you may want to revise your code to see whether you can use the lapply function instead. Learn all about this intuitive way of applying a function over a list or a vector, and how to use its variants, sapply and vapply. 2.4.1 Use lapply with a built-in R function Before you go about solving the exercises below, have a look at the documentation of the lapply() function. The Usage section shows the following expression: lapply(X, FUN, ...) To put it generally, lapply takes a vector or list X, and applies the function FUN to each of its members. If FUN requires additional arguments, you pass them after youve specified X and FUN (in the ... part). The output of lapply() is a list, the same length as X, where each element is the result of applying FUN on the corresponding element of X. Now that you are truly brushing up on your data science skills, lets revisit some of the most relevant figures in data science history. Weve compiled a vector of famous mathematicians/statisticians and the year they were born. Up to you to extract some information! 2.4.1.1 The vector pioneers has already been created for you pioneers &lt;- c(&quot;GAUSS:1777&quot;, &quot;BAYES:1702&quot;, &quot;PASCAL:1623&quot;, &quot;PEARSON:1857&quot;) 2.4.1.2 Split names from birth year split_math &lt;- strsplit(pioneers, split = &quot;:&quot;) 2.4.1.3 Convert to lowercase strings: split_low split_low &lt;- lapply(split_math, tolower) 2.4.1.4 Take a look at the structure of split_low str(split_low) ## List of 4 ## $ : chr [1:2] &quot;gauss&quot; &quot;1777&quot; ## $ : chr [1:2] &quot;bayes&quot; &quot;1702&quot; ## $ : chr [1:2] &quot;pascal&quot; &quot;1623&quot; ## $ : chr [1:2] &quot;pearson&quot; &quot;1857&quot; 2.4.2 Use lapply with your own function As Filip explained in the instructional video, you can use lapply() on your own functions as well. You just need to code a new function and make sure it is available in the workspace. After that, you can use the function inside lapply() just as you did with base R functions. In the previous exercise you already used lapply() once to convert the information about your favorite pioneering statisticians to a list of vectors composed of two character strings. Lets write some code to select the names and the birth years separately. The sample code already includes code that defined select_first(), that takes a vector as input and returns the first element of this vector. 2.4.2.1 Write function select_first() select_first &lt;- function(x) { x[1] } 2.4.2.2 Apply select_first() over split_low: names names &lt;- lapply(split_low, select_first) 2.4.2.3 Write function select_second() select_second &lt;- function(x) { x[2] } 2.4.2.4 Apply select_second() over split_low: years years &lt;- lapply(split_low, select_second) Head over to the next exercise to learn about anonymous functions. 2.4.3 lapply and anonymous functions Writing your own functions and then using them inside lapply() is quite an accomplishment! But defining functions to use them only once is kind of overkill, isnt it? Thats why you can use so-called anonymous functions in R. Previously, you learned that functions in R are objects in their own right. This means that they arent automatically bound to a name. When you create a function, you can use the assignment operator to give the function a name. Its perfectly possible, however, to not give the function a name. This is called an anonymous function: # Named function triple &lt;- function(x) { 3 * x } # Anonymous function with same implementation function(x) { 3 * x } ## function(x) { 3 * x } # Use anonymous function inside lapply() lapply(list(1,2,3), function(x) { 3 * x }) ## [[1]] ## [1] 3 ## ## [[2]] ## [1] 6 ## ## [[3]] ## [1] 9 2.4.3.1 Transform: use anonymous function inside lapply names &lt;- lapply(split_low, function(x) { x[1] } ) 2.4.3.2 Transform: use anonymous function inside lapply years &lt;- lapply(split_low, function(x) { x[2] }) Now, theres another way to solve the issue of using the select_*() functions only once: you can make a more generic function that can be used in more places. Find out more about this in the next exercise. 2.4.4 Use lapply with additional arguments In the video, the triple() function was transformed to the multiply() function to allow for a more generic approach. lapply() provides a way to handle functions that require more than one argument, such as the multiply() function: multiply &lt;- function(x, factor) { x * factor } lapply(list(1,2,3), multiply, factor = 3) ## [[1]] ## [1] 3 ## ## [[2]] ## [1] 6 ## ## [[3]] ## [1] 9 On the right weve included a generic version of the select functions that youve coded earlier: select_el(). It takes a vector as its first argument, and an index as its second argument. It returns the vectors element at the specified index. 2.4.4.1 Generic select function select_el &lt;- function(x, index) { x[index] } 2.4.4.2 Use lapply() twice on split_low: names and years names &lt;- lapply(split_low, select_el, index = 1) years &lt;- lapply(split_low, select_el, index = 2) Your lapply skills are growing by the minute! 2.4.5 Apply functions that return NULL In all of the previous exercises, it was assumed that the functions that were applied over vectors and lists actually returned a meaningful result. For example, the tolower() function simply returns the strings with the characters in lowercase. This wont always be the case. Suppose you want to display the structure of every element of a list. You could use the str() function for this, which returns NULL: lapply(list(1, &quot;a&quot;, TRUE), str) ## num 1 ## chr &quot;a&quot; ## logi TRUE ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL This call actually returns a list, the same size as the input list, containing all NULL values. On the other hand calling str(TRUE) ## logi TRUE on its own prints only the structure of the logical to the console, not NULL. Thats because str() uses invisible() behind the scenes, which returns an invisible copy of the return value, NULL in this case. This prevents it from being printed when the result of str() is not assigned. Feel free to experiment some more with your code in the console. Did you notice that lapply() always returns a list, no matter the input? This can be kind of annoying. In the next video tutorial youll learn about sapply() to solve this. 2.4.6 Video: sapply() 2.4.7 How to use sapply You can use sapply() similar to how you used lapply(). The first argument of sapply() is the list or vector X over which you want to apply a function, FUN. Potential additional arguments to this function are specified afterwards (...): sapply(X, FUN, ...) In the next couple of exercises, youll be working with the variable temp, that contains temperature measurements for 7 days. temp is a list of length 7, where each element is a vector of length 5, representing 5 measurements on a given day. 2.4.7.1 Define temp temp &lt;- list(c(3, 7, 9, 6, -1), c(6, 9, 12, 13, 5), c(4, 8, 3, -1, -3), c(1, 4, 7, 2, -2), c(5, 7, 9, 4, 2), c(-3, 5, 8, 9, 4), c(3, 6, 9, 4, 1)) 2.4.7.2 View structure of temp str(temp) ## List of 7 ## $ : num [1:5] 3 7 9 6 -1 ## $ : num [1:5] 6 9 12 13 5 ## $ : num [1:5] 4 8 3 -1 -3 ## $ : num [1:5] 1 4 7 2 -2 ## $ : num [1:5] 5 7 9 4 2 ## $ : num [1:5] -3 5 8 9 4 ## $ : num [1:5] 3 6 9 4 1 2.4.7.3 Use lapply() to find each days minimum temperature lapply(temp, min) ## [[1]] ## [1] -1 ## ## [[2]] ## [1] 5 ## ## [[3]] ## [1] -3 ## ## [[4]] ## [1] -2 ## ## [[5]] ## [1] 2 ## ## [[6]] ## [1] -3 ## ## [[7]] ## [1] 1 2.4.7.4 Use sapply() to find each days minimum temperature sapply(temp, min) ## [1] -1 5 -3 -2 2 -3 1 2.4.7.5 Use lapply() to find each days maximum temperature lapply(temp, max) ## [[1]] ## [1] 9 ## ## [[2]] ## [1] 13 ## ## [[3]] ## [1] 8 ## ## [[4]] ## [1] 7 ## ## [[5]] ## [1] 9 ## ## [[6]] ## [1] 9 ## ## [[7]] ## [1] 9 2.4.7.6 Use sapply() to find each days maximum temperature sapply(temp, max) ## [1] 9 13 8 7 9 9 9 Can you tell the difference between the output of lapply() and sapply()? The former returns a list, while the latter returns a vector that is a simplified version of this list. Notice that this time, unlike in the cities example of the instructional video, the vector is not named. 2.4.8 sapply with your own function Like lapply(), sapply() allows you to use self-defined functions and apply them over a vector or a list: sapply(X, FUN, ...) Here, FUN can be one of Rs built-in functions, but it can also be a function you wrote. This self-written function can be defined before hand, or can be inserted directly as an anonymous function. 2.4.8.1 Finish function definition of extremes_avg extremes_avg &lt;- function(x) { ( min(x) + max(x) ) / 2 } 2.4.8.2 Apply extremes_avg() over temp using sapply() sapply(temp, extremes_avg) ## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0 2.4.8.3 Apply extremes_avg() over temp using lapply() lapply(temp, extremes_avg) ## [[1]] ## [1] 4 ## ## [[2]] ## [1] 9 ## ## [[3]] ## [1] 2.5 ## ## [[4]] ## [1] 2.5 ## ## [[5]] ## [1] 5.5 ## ## [[6]] ## [1] 3 ## ## [[7]] ## [1] 5 Of course, you could have solved this exercise using an anonymous function, but this would require you to use the code inside the definition of extremes_avg() twice. Duplicating code should be avoided as much as possible! 2.4.9 sapply with function returning vector In the previous exercises, youve seen how sapply() simplifies the list that lapply() would return by turning it into a vector. But what if the function youre applying over a list or a vector returns a vector of length greater than 1? If you dont remember from the video, dont waste more time in the valley of ignorance and head over to the instructions! 2.4.9.1 Create a function that returns min and max of a vector: extremes extremes &lt;- function(x) { c(min = min(x), max = max(x)) } 2.4.9.2 Apply extremes() over temp with sapply() sapply(temp, extremes) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## min -1 5 -3 -2 2 -3 1 ## max 9 13 8 7 9 9 9 2.4.9.3 Apply extremes() over temp with lapply() lapply(temp, extremes) ## [[1]] ## min max ## -1 9 ## ## [[2]] ## min max ## 5 13 ## ## [[3]] ## min max ## -3 8 ## ## [[4]] ## min max ## -2 7 ## ## [[5]] ## min max ## 2 9 ## ## [[6]] ## min max ## -3 9 ## ## [[7]] ## min max ## 1 9 Have a final look at the console and see how sapply() did a great job at simplifying the rather uninformative list of vectors that lapply() returns. It actually returned a nicely formatted matrix! 2.4.10 sapply cant simplify, now what? It seems like weve hit the jackpot with sapply(). On all of the examples so far, sapply() was able to nicely simplify the rather bulky output of lapply(). But, as with life, there are things you cant simplify. How does sapply() react? We already created a function, below_zero(), that takes a vector of numerical values and returns a vector that only contains the values that are strictly below zero. 2.4.10.1 Definition of below_zero() below_zero &lt;- function(x) { return(x[x &lt; 0]) } 2.4.10.2 Apply below_zero over temp using sapply(): freezing_s freezing_s &lt;- sapply(temp, below_zero) 2.4.10.3 Apply below_zero over temp using lapply(): freezing_l freezing_l &lt;- lapply(temp, below_zero) 2.4.10.4 Are freezing_s and freezing_l identical? identical(freezing_s, freezing_l) ## [1] TRUE Given that the length of the output of below_zero() changes for different input vectors, sapply() is not able to nicely convert the output of lapply() to a nicely formatted matrix. Instead, the output values of sapply() and lapply() are exactly the same, as shown by the TRUE output of identical(). 2.4.11 sapply with functions that return NULL You already have some apply tricks under your sleeve, but youre surely hungry for some more, arent you? In this exercise, youll see how sapply() reacts when it is used to apply a function that returns NULL over a vector or a list. A function print_info(), that takes a vector and prints the average of this vector, has already been created for you. It uses the cat() function. 2.4.11.1 Definition of print_info() print_info &lt;- function(x) { cat(&quot;The average temperature is&quot;, mean(x), &quot;\\n&quot;) } 2.4.11.2 Apply print_info() over temp using sapply() sapply(temp, print_info) ## The average temperature is 4.8 ## The average temperature is 9 ## The average temperature is 2.2 ## The average temperature is 2.4 ## The average temperature is 5.4 ## The average temperature is 4.6 ## The average temperature is 4.6 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL 2.4.11.3 Apply print_info() over temp using lapply() lapply(temp, print_info) ## The average temperature is 4.8 ## The average temperature is 9 ## The average temperature is 2.2 ## The average temperature is 2.4 ## The average temperature is 5.4 ## The average temperature is 4.6 ## The average temperature is 4.6 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL Notice here that, quite surprisingly, sapply() does not simplify the list of NULLs. Thats because the vector-version of a list of NULLs would simply be a NULL, which is no longer a vector with the same length as the input. Proceed to the next exercise. 2.4.12 Reverse engineering sapply This concludes the exercise set on sapply(). Head over to another video to learn all about vapply()! 2.4.13 Video: vapply 2.4.14 Use vapply Before you get your hands dirty with the third and last apply function that youll learn about in this intermediate R course, lets take a look at its syntax. The function is called vapply(), and it has the following syntax: vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE) Over the elements inside X, the function FUN is applied. The FUN.VALUE argument expects a template for the return argument of this function FUN. USE.NAMES is TRUE by default; in this case vapply() tries to generate a named array, if possible. For the next set of exercises, youll be working on the temp list again, that contains 7 numerical vectors of length 5. We also coded a function basics() that takes a vector, and returns a named vector of length 3, containing the minimum, mean and maximum value of the vector respectively. 2.4.14.1 Definition of basics() basics &lt;- function(x) { c(min = min(x), mean = mean(x), max = max(x)) } 2.4.14.2 Apply basics() over temp using vapply() vapply(temp, basics, numeric(3)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## min -1.0 5 -3.0 -2.0 2.0 -3.0 1.0 ## mean 4.8 9 2.2 2.4 5.4 4.6 4.6 ## max 9.0 13 8.0 7.0 9.0 9.0 9.0 Notice how, just as with sapply(), vapply() neatly transfers the names that you specify in the basics() function to the row names of the matrix that it returns. 2.4.15 Use vapply (2) So far youve seen that vapply() mimics the behavior of sapply() if everything goes according to plan. But what if it doesnt? In the video, Filip showed you that there are cases where the structure of the output of the function you want to apply, FUN, does not correspond to the template you specify in FUN.VALUE. In that case, vapply() will throw an error that informs you about the misalignment between expected and actual output. 2.4.15.1 Definition of the basics() function basics &lt;- function(x) { c(min = min(x), mean = mean(x), median = median(x), max = max(x)) } 2.4.15.2 Fix the error: vapply(temp, basics, numeric(4)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## min -1.0 5 -3.0 -2.0 2.0 -3.0 1.0 ## mean 4.8 9 2.2 2.4 5.4 4.6 4.6 ## median 6.0 9 3.0 2.0 5.0 5.0 4.0 ## max 9.0 13 8.0 7.0 9.0 9.0 9.0 2.4.16 From sapply to vapply As highlighted before, vapply() can be considered a more robust version of sapply(), because you explicitly restrict the output of the function you want to apply. Converting your sapply() expressions in your own R scripts to vapply() expressions is therefore a good practice (and also a breeze!). 2.4.16.1 Convert to vapply() expression vapply(temp, max, numeric(1)) ## [1] 9 13 8 7 9 9 9 2.4.16.2 Convert to vapply() expression vapply(temp, function(x, y) { mean(x) &gt; y }, y = 5, logical(1)) ## [1] FALSE TRUE FALSE FALSE TRUE FALSE FALSE Youve got no more excuses to use sapply() in the future! 2.5 Utilities Mastering R programming is not only about understanding its programming concepts. Having a solid understanding of a wide range of R functions is also important. This chapter introduces you to many useful functions for data structure manipulation, regular expressions, and working with times and dates. 2.5.1 Video: Useful functions 2.5.2 Mathematical utilities Have another look at some useful math functions that R features: abs(): Calculate the absolute value. sum(): Calculate the sum of all the values in a data structure. mean(): Calculate the arithmetic mean. round(): Round the values to 0 decimal places by default. Try out ?round in the console for variations of round() and ways to change the number of digits to round to. As a data scientist in training, youve estimated a regression model on the sales data for the past six months. After evaluating your model, you see that the training error of your model is quite regular, showing both positive and negative values. The error values are already defined in the workspace on the right (errors). 2.5.2.1 The errors vector has already been defined for you errors &lt;- c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3) 2.5.2.2 Sum of absolute rounded values of errors sum(round(abs(errors))) ## [1] 29 2.5.3 Find the error We went ahead and included some code on the right, but theres still an error. Can you trace it and fix it? In times of despair, help with functions such as sum() and rev() are a single command away; simply use ?sum and ?rev in the console. 2.5.3.1 Dont edit these two lines vec1 &lt;- c(1.5, 2.5, 8.4, 3.7, 6.3) vec2 &lt;- rev(vec1) 2.5.3.2 Fix the error mean(c(abs(vec1), abs(vec2))) ## [1] 4.48 If you check out the documentation of mean(), youll see that only the first argument, x, should be a vector. If you also specify a second argument, R will match the arguments by position and expect a specification of the trim argument. Therefore, merging the two vectors is a must! 2.5.4 Data Utilities R features a bunch of functions to juggle around with data structures:: seq(): Generate sequences, by specifying the from, to, and by arguments. rep(): Replicate elements of vectors and lists. sort(): Sort a vector in ascending order. Works on numerics, but also on character strings and logicals. rev(): Reverse the elements in a data structures for which reversal is defined. str(): Display the structure of any R object. append(): Merge vectors or lists. is.*(): Check for the class of an R object. as.*(): Convert an R object from one class to another. unlist(): Flatten (possibly embedded) lists to produce a vector. Remember the social media profile views data? Weve use them again now, although in list form. 2.5.4.1 The linkedin and facebook lists have already been created for you linkedin &lt;- list(16, 9, 13, 5, 2, 17, 14) facebook &lt;- list(17, 7, 5, 16, 8, 13, 14) 2.5.4.2 Convert linkedin and facebook to a vector: li_vec and fb_vec li_vec &lt;- unlist(linkedin) fb_vec &lt;- unlist(facebook) 2.5.4.3 Append fb_vec to li_vec: social_vec social_vec &lt;- append(li_vec, fb_vec) 2.5.4.4 Sort social_vec sort(social_vec, decreasing = TRUE) ## [1] 17 17 16 16 14 14 13 13 9 8 7 5 5 2 These instructions required you to solve this challenge in a step-by-step approach. If youre comfortable with the functions, you can combine some of these steps into powerful one-liners. 2.5.5 Find the error (2) Just as before, lets switch roles. Its up to you to see what unforgivable mistakes weve made. Go fix them! 2.5.5.1 Fix me rep(seq(1, 7, by = 2), times = 7) ## [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 Debugging code is also a big part of the daily routine of a data scientist, and you seem to be great at it! 2.5.6 Beat Gauss using R There is a popular story about young Gauss. As a pupil, he had a lazy teacher who wanted to keep the classroom busy by having them add up the numbers 1 to 100. Gauss came up with an answer almost instantaneously, 5050. On the spot, he had developed a formula for calculating the sum of an arithmetic series. There are more general formulas for calculating the sum of an arithmetic series with different starting values and increments. Instead of deriving such a formula, why not use R to calculate the sum of a sequence? 2.5.6.1 Create first sequence: seq1 seq1 &lt;- seq(1, 500, 3) 2.5.6.2 Create second sequence: seq2 seq2 &lt;- seq(1200, 900, -7) 2.5.6.3 Calculate total sum of the sequences sum(c(seq1, seq2)) ## [1] 87029 2.5.7 Video: Regular expressions 2.5.8 grepl &amp; grep In their most basic form, regular expressions can be used to see whether a pattern exists inside a character string or a vector of character strings. For this purpose, you can use: grepl(), which returns TRUE when a pattern is found in the corresponding character string. grep(), which returns a vector of indices of the character strings that contains the pattern. Both functions need a pattern and an x argument, where pattern is the regular expression you want to match for, and the x argument is the character vector from which matches should be sought. In this and the following exercises, youll be querying and manipulating a character vector of email addresses! The vector emails has already been defined in the editor on the right so you can begin with the instructions straight away! 2.5.8.1 The emails vector has already been defined for you emails &lt;- c(&quot;john.doe@ivyleague.edu&quot;, &quot;education@world.gov&quot;, &quot;dalai.lama@peace.org&quot;, &quot;invalid.edu&quot;, &quot;quant@bigdatacollege.edu&quot;, &quot;cookie.monster@sesame.tv&quot;) 2.5.8.2 Use grepl() to match for edu grepl(&quot;edu&quot;, emails) ## [1] TRUE TRUE FALSE TRUE TRUE FALSE 2.5.8.3 Use grep() to match for edu, save result to hits hits &lt;- grep(&quot;edu&quot;, emails) 2.5.8.4 Subset emails using hits emails[hits] ## [1] &quot;john.doe@ivyleague.edu&quot; &quot;education@world.gov&quot; ## [3] &quot;invalid.edu&quot; &quot;quant@bigdatacollege.edu&quot; You can probably guess what were trying to achieve here: select all the emails that end with .edu. However, the strings education@world.gov and invalid.edu were also matched. Lets see in the next exercise what you can do to improve our pattern and remove these false positives. 2.5.9 grepl &amp; grep (2) You can use the caret, ^, and the dollar sign, $ to match the content located in the start and end of a string, respectively. This could take us one step closer to a correct pattern for matching only the .edu email addresses from our list of emails. But theres more that can be added to make the pattern more robust: @, because a valid email must contain an at-sign. .*, which matches any character (the .) zero or more times (*). Both the dot and the asterisk are metacharacters. You can use them to match any character between the at-sign and the .edu portion of an email address. \\\\.edu$, to match the .edu part of the email at the end of the string. The \\\\ part escapes the dot: it tells R that you want to use the . as an actual character. 2.5.9.1 Use grepl() to match for .edu addresses more robustly grepl(&quot;@.*\\\\.edu$&quot;, emails) ## [1] TRUE FALSE FALSE FALSE TRUE FALSE 2.5.9.2 Use grep() to match for .edu addresses more robustly, save result to hits hits &lt;- grep(&quot;@.*\\\\.edu$&quot;, emails) 2.5.9.3 Subset emails using hits emails[hits] ## [1] &quot;john.doe@ivyleague.edu&quot; &quot;quant@bigdatacollege.edu&quot; A careful construction of our regular expression leads to more meaningful matches. However, even our robust email selector will often match some incorrect email addresses (for instance kiara@@fakemail.edu). Lets not worry about this too much and continue with sub() and gsub() to actually edit the email addresses! 2.5.10 sub &amp; gsub While grep() and grepl() were used to simply check whether a regular expression could be matched with a character vector, sub() and gsub() take it one step further: you can specify a replacement argument. If inside the character vector x, the regular expression pattern is found, the matching element(s) will be replaced with replacement. sub() only replaces the first match, whereas gsub() replaces all matches. Suppose that emails vector youve been working with is an excerpt of DataCamps email database. Why not offer the owners of the .edu email addresses a new email address on the datacamp.edu domain? This could be quite a powerful marketing stunt: Online education is taking over traditional learning institutions! Convert your email and be a part of the new generation! 2.5.10.1 Use sub() to convert the email domains to datacamp.edu sub(&quot;@.*\\\\.edu$&quot;, &quot;@datacamp.edu&quot;, emails) ## [1] &quot;john.doe@datacamp.edu&quot; &quot;education@world.gov&quot; ## [3] &quot;dalai.lama@peace.org&quot; &quot;invalid.edu&quot; ## [5] &quot;quant@datacamp.edu&quot; &quot;cookie.monster@sesame.tv&quot; Notice how only the valid .edu addresses are changed while the other emails remain unchanged. To get a taste of other things you can accomplish with regex, head over to the next exercise. 2.5.11 sub &amp; gsub (2) Regular expressions are a typical concept that youll learn by doing and by seeing other examples. Before you rack your brains over the regular expression in this exercise, have a look at the new things that will be used: .*: A usual suspect! It can be read as any character that is matched zero or more times. \\\\s: Match a space. The s is normally a character, escaping it (\\\\) makes it a metacharacter. [0-9]+: Match the numbers 0 to 9, at least once (+). ([0-9]+): The parentheses are used to make parts of the matching string available to define the replacement. Refer to () references using \\\\1, \\\\2, etc. in the replacement argument of sub(). The ([0-9]+) selects the entire number that comes before the word nomination in the string, and the entire match gets replaced by this number because of the \\\\1 that refers to the content inside the parentheses. The next video will get you up to speed with times and dates in R! 2.5.12 Video: Times &amp; Dates 2.5.13 Right here, right now In R, dates are represented by Date objects, while times are represented by POSIXct objects. Under the hood, however, these dates and times are simple numerical values. Date objects store the number of days since the 1st of January in 1970. POSIXct objects on the other hand, store the number of seconds since the 1st of January in 1970. The 1st of January in 1970 is the common origin for representing times and dates in a wide range of programming languages. There is no particular reason for this; it is a simple convention. Of course, its also possible to create dates and times before 1970; the corresponding numerical values are simply negative in this case. 2.5.13.1 Get the current date: today today &lt;- Sys.Date() 2.5.13.2 See what today looks like under the hood unclass(today) ## [1] 18608 2.5.13.3 Get the current time: now now &lt;- Sys.time() 2.5.13.4 See what now looks like under the hood unclass(now) ## [1] 1607810519 Using R to get the current date and time is nice, but you should also know how to create dates and times from character strings. Find out how in the next exercises! 2.5.14 Create and format dates To create a Date object from a simple character string in R, you can use the as.Date() function. The character string has to obey a format that can be defined using a set of symbols (the examples correspond to 13 January, 1982): %Y: 4-digit year (1982) %y: 2-digit year (82) %m: 2-digit month (01) %d: 2-digit day of the month (13) %A: weekday (Wednesday) %a: abbreviated weekday (Wed) %B: month (January) %b: abbreviated month (Jan) The following R commands will all create the same Date object for the 13th day in January of 1982: as.Date(&quot;1982-01-13&quot;) ## [1] &quot;1982-01-13&quot; as.Date(&quot;Jan-13-82&quot;, format = &quot;%b-%d-%y&quot;) ## [1] &quot;1982-01-13&quot; as.Date(&quot;13 January, 1982&quot;, format = &quot;%d %B, %Y&quot;) ## [1] &quot;1982-01-13&quot; Notice that the first line here did not need a format argument, because by default R matches your character string to the formats \"%Y-%m-%d\" or \"%Y/%m/%d\". In addition to creating dates, you can also convert dates to character strings that use a different date notation. For this, you use the format() function. Try the following lines of code: today &lt;- Sys.Date() format(Sys.Date(), format = &quot;%d %B, %Y&quot;) ## [1] &quot;12 December, 2020&quot; format(Sys.Date(), format = &quot;Today is a %A!&quot;) ## [1] &quot;Today is a Saturday!&quot; 2.5.14.1 Definition of character strings representing dates str1 &lt;- &quot;May 23, &#39;96&quot; str2 &lt;- &quot;2012-03-15&quot; str3 &lt;- &quot;30/January/2006&quot; 2.5.14.2 Convert the strings to dates: date1, date2, date3 date1 &lt;- as.Date(str1, format = &quot;%b %d, &#39;%y&quot;) date2 &lt;- as.Date(str2, format = &quot;%Y-%m-%d&quot;) date3 &lt;- as.Date(str3, format = &quot;%d/%B/%Y&quot;) 2.5.14.3 Convert dates to formatted strings format(date1, &quot;%A&quot;) ## [1] &quot;Thursday&quot; format(date2, &quot;%d&quot;) ## [1] &quot;15&quot; format(date3, &quot;%b %Y&quot;) ## [1] &quot;Jan 2006&quot; You can use POSIXct objects, i.e. Time objects in R, in a similar fashion. Give it a try in the next exercise. 2.5.15 Create and format times Similar to working with dates, you can use as.POSIXct() to convert from a character string to a POSIXct object, and format() to convert from a POSIXct object to a character string. Again, you have a wide variety of symbols: %H: hours as a decimal number (00-23) %I: hours as a decimal number (01-12) %M: minutes as a decimal number %S: seconds as a decimal number %T: shorthand notation for the typical format %H:%M:%S %p: AM/PM indicator For a full list of conversion symbols, consult the strptime documentation in the console. Again, as.POSIXct() uses a default format to match character strings. In this case, its %Y-%m-%d %H:%M:%S. In this exercise, abstraction is made of different time zones. 2.5.15.1 Definition of character strings representing times str1 &lt;- &quot;May 23, &#39;96 hours:23 minutes:01 seconds:45&quot; str2 &lt;- &quot;2012-3-12 14:23:08&quot; 2.5.15.2 Convert the strings to POSIXct objects: time1, time2 time1 &lt;- as.POSIXct(str1, format = &quot;%B %d, &#39;%y hours:%H minutes:%M seconds:%S&quot;) time2 &lt;- as.POSIXct(str2, format = &quot;%Y-%m-%d %T&quot;) 2.5.15.3 Convert times to formatted strings format(time1, format = &quot;%M&quot;) ## [1] &quot;01&quot; format(time2, format = &quot;%I:%M %p&quot;) ## [1] &quot;02:23 PM&quot; 2.5.16 Calculations with Dates Both Date and POSIXct R objects are represented by simple numerical values under the hood. This makes calculation with time and date objects very straightforward: R performs the calculations using the underlying numerical values, and then converts the result back to human-readable time information again. You can increment and decrement Date objects, or do actual calculations with them (try it out in the console!): today &lt;- Sys.Date() today + 1 ## [1] &quot;2020-12-13&quot; today - 1 ## [1] &quot;2020-12-11&quot; as.Date(&quot;2015-03-12&quot;) - as.Date(&quot;2015-02-27&quot;) ## Time difference of 13 days To control your eating habits, you decided to write down the dates of the last five days that you ate pizza. In the workspace, these dates are defined as five Date objects, day1 to day5. The code on the right also contains a vector pizza with these 5 Date objects. 2.5.16.1 day1, day2, day3, day4 and day5 are already available in the workspace day1 &lt;- as.Date(&quot;2020-09-13&quot;) day2 &lt;- as.Date(&quot;2020-09-15&quot;) day3 &lt;- as.Date(&quot;2020-09-20&quot;) day4 &lt;- as.Date(&quot;2020-09-26&quot;) day5 &lt;- as.Date(&quot;2020-10-01&quot;) 2.5.16.2 Difference between last and first pizza day day5 - day1 ## Time difference of 18 days 2.5.16.3 Create vector pizza pizza &lt;- c(day1, day2, day3, day4, day5) 2.5.16.4 Create differences between consecutive pizza days: day_diff day_diff &lt;- diff(pizza) 2.5.16.5 Average period between two consecutive pizza days mean(day_diff) ## Time difference of 4.5 days 2.5.17 Calculations with Times Calculations using POSIXct objects are completely analogous to those using Date objects. Try to experiment with this code to increase or decrease POSIXct objects: now &lt;- Sys.time() now + 3600 # add an hour ## [1] &quot;2020-12-12 23:02:00 GMT&quot; now - 3600 * 24 # subtract a day ## [1] &quot;2020-12-11 22:02:00 GMT&quot; Adding or subtracting time objects is also straightforward: birth &lt;- as.POSIXct(&quot;1879-03-14 14:37:23&quot;) death &lt;- as.POSIXct(&quot;1955-04-18 03:47:12&quot;) einstein &lt;- death - birth einstein ## Time difference of 27792.51 days Youre developing a website that requires users to log in and out. You want to know what is the total and average amount of time a particular user spends on your website. This user has logged in 5 times and logged out 5 times as well. These times are gathered in the vectors login and logout, which are already defined in the workspace. 2.5.17.1 login and logout are already defined in the workspace login &lt;- c(as.POSIXct(&quot;2020-09-17 10:18:04 UTC&quot;), as.POSIXct(&quot;2020-09-22 09:14:18 UTC&quot;), as.POSIXct(&quot;2020-09-22 12:21:51 UTC&quot;), as.POSIXct(&quot;2020-09-22 12:37:24 UTC&quot;), as.POSIXct(&quot;2020-09-24 21:37:55 UTC&quot;)) logout &lt;- c(as.POSIXct(&quot;2020-09-17 10:56:29 UTC&quot;), as.POSIXct(&quot;2020-09-22 09:14:52 UTC&quot;), as.POSIXct(&quot;2020-09-22 12:35:48 UTC&quot;), as.POSIXct(&quot;2020-09-22 13:17:22 UTC&quot;), as.POSIXct(&quot;2020-09-24 22:08:47 UTC&quot;)) 2.5.17.2 Calculate the difference between login and logout: time_online time_online &lt;- logout - login 2.5.17.3 Inspect the variable time_online time_online ## Time differences in secs ## [1] 2305 34 837 2398 1852 2.5.17.4 Calculate the total time online sum(time_online) ## Time difference of 7426 secs 2.5.17.5 Calculate the average time online mean(time_online) ## Time difference of 1485.2 secs 2.5.18 Time is of the essence The dates when a season begins and ends can vary depending on who you ask. People in Australia will tell you that spring starts on September 1st. The Irish people in the Northern hemisphere will swear that spring starts on February 1st, with the celebration of St. Brigids Day. Then theres also the difference between astronomical and meteorological seasons: while astronomers are used to equinoxes and solstices, meteorologists divide the year into 4 fixed seasons that are each three months long. (source: www.timeanddate.com) A vector astro, which contains character strings representing the dates on which the 4 astronomical seasons start, has been defined on your workspace. Similarly, a vector meteo has already been created for you, with the meteorological beginnings of a season. 2.5.18.1 Define astro and meteo astro &lt;- c(&quot;20-Mar-2015&quot;, &quot;25-Jun-2015&quot;, &quot;23-Sep-2015&quot;, &quot;22-Dec-2015&quot;) names(astro) &lt;- c(&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;, &quot;winter&quot;) meteo &lt;- c(&quot;March 1, 15&quot;, &quot;June 1, 15&quot;, &quot;September 1, 15&quot;, &quot;December 1, 15&quot;) names(meteo) &lt;- c(&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;, &quot;winter&quot;) 2.5.18.2 Convert astro to vector of Date objects: astro_dates astro_dates &lt;- as.Date(astro, format = &quot;%d-%b-%Y&quot;) 2.5.18.3 Convert meteo to vector of Date objects: meteo_dates meteo_dates &lt;- as.Date(meteo, format = &quot;%B %d, %y&quot;) 2.5.18.4 Calculate the maximum absolute difference between astro_dates and meteo_dates max(abs(astro_dates - meteo_dates)) ## Time difference of 24 days Impressive! Great job on finishing this course! "],["r-markdown.html", "Module 3 R Markdown 3.1 Getting started with R Markdown 3.2 Adding Analyses and Visualizations 3.3 Improving the Report 3.4 Customizing the Report", " Module 3 R Markdown R Markdown is an easy to use formatting language you can use to reveal insights from data and author your findings as a PDF, HTML file, or Shiny app. In this course, youll learn how to create and modify each element of a Markdown file, including the code, text, and metadata. Youll analyze data with dplyr, create visualizations with ggplot2, and author your analyses and plots as reports. Youll gain hands-on experience of building reports as you work with real-world data from the International Finance Corporation (IFC)learning how to efficiently organize reports using code chunk options, create lists and tables, and include a table of contents. By the end of the course, youll have the skills you need to add your brands fonts and colors using parameters and Cascading Style Sheets (CSS), to make your reports stand out. 3.1 Getting started with R Markdown In this chapter, youll learn about the three components of a Markdown file: the code, the text, and the metadata. Youll also learn to add and modify each of these elements to your own reports, as you create your first Markdown files. 3.1.1 Video: Introduction to R Markdown 3.1.2 Creating your first R Markdown file Throughout the course, youll be working on creating an investment report using two datasets from the World Bank IFC. The first dataset, investment_annual_summary, provides the summary of the dollars in millions provided to each region for each fiscal year, from 2012 to 2018. To get started on your report, you first want to print out the dataset. To create your report, youll need to edit the Markdown file shown on the right of the DataCamp console, as described in the instructions, then press the green Knit HTML button to knit the file and see the resulting HTML file. Well discuss other output types later in the course. Because this textbook is written using R Markdown, Ill be including the R Markdown output for this chapter here. In each exercise, the first code chunk in the Markdown file will load the readr package and the datasets youll be using in the exercise. Youll learn more about the details of this code chunk later in the course, but you wont need to modify it for any of the exercises in this chapter. As you can see from the output, adding the dataset name to the code chunk and knitting the document resulted in the dataset being printed in your report. You can also see from the report that the dataset has 42 rows, and that each row contains information about the `fiscal_year, region, and dollars_in_millions for the investments made that year. 3.1.3 Adding code chunks to your file When creating your own reports, one of the first things youll want to do is add some code! In the video, we discussed how you can add your own code by adding code chunks. Previously, we looked at the investment_annual_summary dataset well be using throughout the course. In this exercise, lets take a look at the annual summary dataset as well as the other dataset well be using, investment_services_projects. From the report, you can see that the investment_services_project dataset contains information about each individual project and includes a lot more variables than the investment_annual_summary dataset. Youll be exploring both of these datasets in much more depth in the coming exercises as you create your investment report! 3.1.4 Video: Adding and formatting text 3.1.5 Question: Formatting text 3.1.6 Adding sections to your report Previously, you added the names of the datasets youll be using to build your report to the Markdown file that will be used to create the report. Now, youll add some headers and text to the document to provide some additional detail about the datasets to the audience who will read the report. As you can see from the report, the more hashes you place in front of the text, the smaller the header will be when you knit the file. 3.1.7 Question: Including links and images 3.1.8 Video: The YAML header 3.1.9 Editing the YAML header The YAML header contains the metadata for your report and includes information like the title, author, and output format. In this exercise, youll update your report by adding some more detail about who created the report and when it was created. Remember, you can add the date to your report manually by entering the date as a string. However, adding the date with Sys.Date() is much more efficient and scalable, since it will ensure that the date is updated automatically each time you edit and knit your file. 3.1.10 Formatting the date Now that your report includes some more high-level detail, youd like to include the date using a different format. Be sure to refer to the tables of date formatting options from the video below. Source: DataCamp Keep in mind that there are many text and numeric options for formatting the date of your report. Now that you understand each of the elements of the Markdown file, and how to modify them, youre ready to add more detail to your Investment Report! 3.2 Adding Analyses and Visualizations In this chapter, youll use dplyr to begin to analyze the World Bank IFC datasets and include the analyses in your report. Youll then create visualizations of the data using ggplot2 and learn to modify how the plots display in your knit report. 3.2.1 Video: Analyzing the data 3.2.2 Filtering for a specific country Previously, you learned how to filter the data to find out more information about projects that occurred in Indonesia. Now, youll build a report that provides this information for another country thats included in the investment_services_project data. In this exercise, youll begin filtering the data to gather information about projects that occurred in Brazil. From the report, you can see that there were a total of 88 projects in Brazil during the 2012 to 2018 fiscal years. 3.2.3 Filtering for a specific year Now that youve filtered the data for the projects in a specific country, you can filter the results further to look at all projects that occurred in the 2018 fiscal year. Recall, the fiscal year starts on July 1st of the previous year and ends on June 30th of the year of interest. Now your report includes information about all projects that occurred in Brazil during the 2012 to 2018 fiscal years and the projects that occurred in the 2018 fiscal year. Next, youll learn how to create and add plots to these reports, so that your audience is able to visualize this information when they read the final report. 3.2.4 Referencing code results in the report In this exercise, youll use summarize() and brazil_investment_projects_2018 to find the total investment amount for all projects in Brazil in the 2018 fiscal year. Then, youll add text to the report to include the information and reference the code results in the text, so that the calculated amount is printed in the text of the report when you knit the file. Now, if you update the analysis and the brazil_investment_projects_2018_total amount changes, you wont need to modify the amount manually in the sentence that describes the information. Instead, since the object name is referenced in the text, the report will update automatically the next time the report is knit. 3.2.5 Video: Adding plots 3.2.6 Visualizing the Investment Annual Summary data Now that you have all of the data ready for the report youre creating, you can start making plots that will be included in the report to help your audience visualize the data when theyre reading the report. Youll start by creating a line plot of the investment_annual_summary data. Notice that for each region, the highest investments were made during the 2013 or 2014 fiscal years. This is an example of an insight you might want to include as text in the final report. 3.2.7 Visualizing all projects for one country Previously, you created a line plot using the investment_annual_summary. Now, youll use the data you filtered to create scatterplots that summarize the information about the projects that occurred in Brazil. Notice the warning message in the report that tells us that there were 7 rows removed that contain missing values. Youll learn how to handle warning messages later in the course, but this may be something youd want to include in the text of your report to specify which data points were excluded from the plot and why. 3.2.8 Visualizing all projects for one country and year Now, youll create a line plot using the data that was filtered for all projects that occurred in Brazil in the 2018 fiscal year. In the previous exercises, the labels were added for you. While creating this plot, youll gain some experience adding your own labels that will appear when you knit the report. Now your report has plots for the Investment Annual Summary data, as well as for all projects in Brazil during the 2012 to 2018 fiscal years and all projects in the 2018 fiscal year. Now that youve created these plots for your report, youll learn some of the options you have for formatting how the plots appear in the final report. 3.2.9 Video: Plot options 3.2.10 Setting chunk options globally Now that your plots are ready to include in your report, you can modify how they appear once the file is knit. Previously, you learned the difference between setting options globally and setting them locally. In this exercise, youll set options for the figures globally, which means the options will apply to all figures throughout the code chunks in the report. Recall, the options for fig.align are 'left', 'right', and 'center'. These options can be set globally or locally, depending on whether or not you want all figures to appear uniformly throughout the report, or if you want the options to vary by figure. 3.2.11 Setting chunk options locally When creating a report, you may want to set the chunk options locally so that the figure display in the final report varies. The investment_annual_summary data provides helpful background information, but the focus of the report is on projects in Brazil. In this exercise, youll modify the chunk options locally so that the plots that display information about projects in Brazil appear slightly larger in the final report than the plot that provides the overview of the Investment Annual Summary data. You can see in the final report that the plots that display information about projects in Brazil are slightly larger than the plot that provides the Investment Annual Summary overview. Notice that the fig.align = center option remained in the setup code chunk at the top, so this option has been set globally and determines the alignment for all figures in the report. Also note that you can override globally set options with locally set ones. For example, if you wanted all figures to display at 50% width except for two figures you wanted to be larger, you could include out.width = '50%' in the global options but out.width = '95%' in the local options of the two figures. 3.2.12 Adding figure captions Now that the figures have been modified, youll add some captions to label the figures and provide some information about what is displayed in each plot. Now you can see that each figure is labeled in the final report and includes a caption that describes what each plot displays. 3.3 Improving the Report Now that youve learned how to add, label, and modify code chunks, youll learn about code chunk options. You can use these to determine whether the code and results appear in the knit report. Youll also discover how to create lists and tables to include in your report. 3.3.1 Video: Organizing the report 3.3.2 Creating a bulleted list Previously, you learned how to add text to your Markdown file to include additional information for your audience. Now, youll create a bulleted list to specify which regions are included in the investment_annual_summary data. Refer to the image below from the video to recall the list of regions that should be included in your table. Source: DataCamp Remember, you can structure the list formatting by adding indentation before an item on the list. 3.3.3 Creating a numbered list When adding a list to your report, you can use either bulleted or numbered lists. In this exercise, youll modify the bulleted list of regions from the previous exercise to create a numbered list. Adding a numbered list to the report is a helpful way to quickly display the total number of regions that are included in the data. 3.3.4 Adding a table Previously, you printed the datasets used in your report to your report so that the audience was able to look through the data themselves. Now, youll create a table of the investment_region_summary to display this information more clearly to your audience. The investment_region_summary provides the total of all investments for each region from the 2012 to 2018 fiscal years. Now your report begins with a list of all regions in the investment_annual_summary data, as well as a table that summarizes the total investments that were made to each region across the 2012 to 2018 fiscal years. 3.3.5 Video: Code chunk options 3.3.6 Question: Comparing code chunk options include=FALSE: Code runs but does not appears in report, results do not appear. echo=FALSE: Code runs but does not appear in the report, results appear. eval=FALSE: Code appears in report but does not run, results do not appear. The other option you recently learned is collapse, which was the only option youve learned so far that has a default of FALSE. 3.3.7 Collapsing blocks in the knit report By default, the collapse option is set to FALSE, and the code and any output appear in the knit file in separate blocks. You encountered this earlier when creating plots of the data. In this exercise, youll modify the Markdown file so that the code and resulting warning messages appear in the same block. Notice that the warning messages for the plots now appear as comments in the same block as the code that creates the plot, instead of being separated into an individual block. Youll learn more about warning messages and how to specify whether or not they are included in the report soon! 3.3.8 Modifying the report using include and echo The exercises in the course have used include = FALSE to prevent the code and results of the setup and data chunks from appearing in the knit report. Although you wont modify those options, since the code and results from those chunks should be excluded from the report, its important to note how they impact the final report. In this exercise, youll use the echo option to modify whether or not the code appears in the report. Even though the code no longer displays in the knit report, the results of the code still appears. This is an option youll want to use if your report is being written for a non-technical audience, who is interested in the results but may not be interested in the code itself. Notice that the warning messages that mention that data was excluded from the plots still appear in the knit report. Next, youll learn how to modify whether or not these warning messages are included in the report! 3.3.9 Video: Warnings, messages, and errors 3.3.10 Excluding messages In the past, you havent encountered messages in the report because the include option has been set to FALSE in the data chunk to prevent the code or the results from the code from appearing in the report. In this exercise, youll use the message option to prevent messages from appearing in the report, while still including the code in the report. Notice that the code for the data chunk is now included without any of the messages that you would otherwise see when loading the data or packages used in the report. 3.3.11 Excluding warnings Previously, you used the collapse option so that the code and resulting warning messages appear in the same block in the knit report. In this exercise, youll use the warning option to prevent warnings from appearing in the final report. Notice that DataCamp added a sentence before each of the code chunks that were impacted to let the audience know that: Projects that do not have an associated investment amount are excluded from the plot. If you are excluding any warning messages from the report, its important to include information about any data that is not included and why. 3.4 Customizing the Report In this final chapter, youll learn how to customize your report by adding a table of contents and adding a CSS file to the YAML header, to personalize reports with your brands fonts and colors. Youll also learn how to efficiently create new reports from your data using parameters, which will save you time from manually updating existing reports to create new ones. 3.4.1 Video: Adding a table of contents 3.4.2 Adding the table of contents Adding a table of contents to your report is a useful way to help your audience navigate through the different sections of your report. It provides an overview of what your report contains, and can help your audience navigate through the report easily. In this exercise, youll add a table of contents to your report to provide an overview of the topics that the report includes. Now your audience can reference the table of contents at the beginning of your report to understand the information that will be covered in the report. 3.4.3 Specifying headers and number sectioning Now that youve added a table of contents, youll modify how it appears in the report and which information it includes. Youll use toc_depth to specify the depth of headers that will be included in the table of contents and number_sections to add section numbering for the headers in the report. The headers were modified to start with a single hash before adding section numbering because, if the largest headers in the report start with two hashes, the section numbering will start with zeros. Remember that, for toc_depth, the default depth is 3 for HTML documents and 2 for PDF documents. 3.4.4 Adding table of contents options When toc_float is included, the table of contents appears on the left side of the document and remains visible while the reader scrolls through the document. By default, it displays the largest header, will expand as someone is reading through the report or interacting with the table of contents to navigate to another section, and animates page scrolls when navigating the report. In this exercise, youll add toc_float and modify these settings using the collapsed and smooth_scroll fields so that the full table of contents remains visible and page scrolls are not animated. If you want to add toc_float to the report and keep the default collapsed and smooth_scroll options (both turned on, i.e. TRUE, by default), you can set the toc_float field to true in the YAML header. 3.4.5 Video: Creating a report with a parameter 3.4.6 Adding a parameter to the report In this exercise, youll add a parameter for country to the report and modify the existing code so that you can create new reports about the investment projects for any country included in the investment_services_projects data. Now that youve reviewed the code for your report, youll review the text and the YAML header of the document before creating a new report using the country parameter. 3.4.7 Creating a new report using a parameter Now that youve added a parameter to the document, youll create a new report for Bangladesh from the investment_services_projects data using the country parameter. Before knitting the report, youll review and modify the text of the document to ensure that the knit report will reflect the country that is specified in the parameter. Notice that by only modifying the parameter, you were able to create a new report for Bangladesh that includes all of the information that was previously provided for Brazil. When creating new documents using parameters, there may be some information you want to add that is specific to the new country and report, but parameters are an efficient and quick way to get started! 3.4.8 Video: Multiple parameters 3.4.9 Adding multiple parameters to the report Previously, you added a parameter for country to create new reports to summarize information about the investment projects for any country included in the investment_services_projects data. Now, youll add parameters for the fiscal year and modify the existing code so that you can create new reports about the investment projects for any country and fiscal year from the investment_services_projects data. Now, youll be able to create a report for any country and fiscal year by modifying only the parameters in the YAML header! 3.4.10 Creating a new report using multiple parameters Now that youve added parameters to account for the fiscal year, youll create a new report for another country and fiscal year from the investment_services_projects data. Now youre able to create new reports using multiple parameters by modifying only the information in the YAML header. As before, there may be some information you want to add that is specific to the country and fiscal year, but these parameters will help you get started on a new report! 3.4.11 Video: Customizing the report 3.4.12 Customizing the report style Now that youve learned how to customize the style of your report, youll begin to add specific fonts and colors to your existing report. Notice that the document background and code chunks in the knit file are modified to reflect the various fonts and colors you listed. 3.4.13 Customizing the header and table of contents In this exercise, youll continue to add styles by modifying the table of contents and header sections of the Markdown file. Notice the difference in appearance of the table of contents section. If you want to customize any of these sections further, you can add the properties youve used so far to any of the sections listed. For example, you can add opactity to the pre section to customize the code chunks, the same way that you did with the #header section. 3.4.14 Customizing the title, author, and date Previously, you modified the header of the document using the #header section. Now, youll practice customizing the title, author, and date sections individually. In this version of the report, you used the same settings for the h4.author and h4.date sections, but you can use any colors or fonts youd like with this syntax. 3.4.15 Referencing the CSS file Rather than adding styles to each Markdown file within the file, you can create and reference a Cascading Style Sheet (CSS) file each time you create a new file that contains particular styles and fonts. In this exercise, the styles youve specified have been added to a CSS file called styles.css. Youll reference this file in the YAML header instead of specifying the styles within the Markdown file. Notice that, although the styles are no longer listed within the Markdown file, the knit report still reflects all of the styles youve been adding over the past few exercises. 3.4.16 Video: Congratulations! "],["data-manipulation-with-dplyr.html", "Module 4 Data Manipulation with dplyr 4.1 Transforming Data with dplyr 4.2 Aggregating Data 4.3 Selecting and Transforming Data 4.4 Case Study: The babynames Dataset 4.5 Challenge 4.6 Solutions", " Module 4 Data Manipulation with dplyr Say youve found a great dataset and would like to learn more about it. How can you start to answer the questions you have about the data? You can use dplyr to answer those questionsit can also help with basic transformations of your data. Youll also learn to aggregate your data and add, remove, or change the variables. Along the way, youll explore a dataset containing information about counties in the United States. Youll finish the course by applying these tools to the babynames dataset to explore trends of baby names in the United States. 4.1 Transforming Data with dplyr Learn verbs you can use to transform your data, including select, filter, arrange, and mutate. Youll use these functions to modify the counties dataset to view particular observations and answer questions about the data. 4.1.1 Video: The counties dataset 4.1.2 Question: Understanding your data 4.1.2.1 Load the counties data set and the dplyr package counties &lt;- read.csv(&quot;data_acs2015_county_data.csv&quot;) library(&quot;dplyr&quot;) Take a look at the counties dataset using the glimpse() function. glimpse(counties) ## Observations: 3,141 ## Variables: 40 ## $ census_id &lt;int&gt; 1001, 1003, 1005, 1007, 1009, 1011, 10... ## $ state &lt;fct&gt; Alabama, Alabama, Alabama, Alabama, Al... ## $ county &lt;fct&gt; Autauga, Baldwin, Barbour, Bibb, Bloun... ## $ region &lt;fct&gt; South, South, South, South, South, Sou... ## $ metro &lt;fct&gt; , , , , , , , , , , , , , , , , , , , ... ## $ population &lt;int&gt; 55221, 195121, 26932, 22604, 57710, 10... ## $ men &lt;int&gt; 26745, 95314, 14497, 12073, 28512, 566... ## $ women &lt;int&gt; 28476, 99807, 12435, 10531, 29198, 501... ## $ hispanic &lt;dbl&gt; 2.6, 4.5, 4.6, 2.2, 8.6, 4.4, 1.2, 3.5... ## $ white &lt;dbl&gt; 75.8, 83.1, 46.2, 74.5, 87.9, 22.2, 53... ## $ black &lt;dbl&gt; 18.5, 9.5, 46.7, 21.4, 1.5, 70.7, 43.8... ## $ native &lt;dbl&gt; 0.4, 0.6, 0.2, 0.4, 0.3, 1.2, 0.1, 0.2... ## $ asian &lt;dbl&gt; 1.0, 0.7, 0.4, 0.1, 0.1, 0.2, 0.4, 0.9... ## $ pacific &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0... ## $ citizens &lt;int&gt; 40725, 147695, 20714, 17495, 42345, 80... ## $ income &lt;int&gt; 51281, 50254, 32964, 38678, 45813, 319... ## $ income_err &lt;int&gt; 2391, 1263, 2973, 3995, 3141, 5884, 17... ## $ income_per_cap &lt;int&gt; 24974, 27317, 16824, 18431, 20532, 175... ## $ income_per_cap_err &lt;int&gt; 1080, 711, 798, 1618, 708, 2055, 714, ... ## $ poverty &lt;dbl&gt; 12.9, 13.4, 26.7, 16.8, 16.7, 24.6, 25... ## $ child_poverty &lt;dbl&gt; 18.6, 19.2, 45.3, 27.9, 27.2, 38.4, 39... ## $ professional &lt;dbl&gt; 33.2, 33.1, 26.8, 21.5, 28.5, 18.8, 27... ## $ service &lt;dbl&gt; 17.0, 17.7, 16.1, 17.9, 14.1, 15.0, 16... ## $ office &lt;dbl&gt; 24.2, 27.1, 23.1, 17.8, 23.9, 19.7, 21... ## $ construction &lt;dbl&gt; 8.6, 10.8, 10.8, 19.0, 13.5, 20.1, 10.... ## $ production &lt;dbl&gt; 17.1, 11.2, 23.1, 23.7, 19.9, 26.4, 23... ## $ drive &lt;dbl&gt; 87.5, 84.7, 83.8, 83.2, 84.9, 74.9, 84... ## $ carpool &lt;dbl&gt; 8.8, 8.8, 10.9, 13.5, 11.2, 14.9, 12.4... ## $ transit &lt;dbl&gt; 0.1, 0.1, 0.4, 0.5, 0.4, 0.7, 0.0, 0.2... ## $ walk &lt;dbl&gt; 0.5, 1.0, 1.8, 0.6, 0.9, 5.0, 0.8, 1.2... ## $ other_transp &lt;dbl&gt; 1.3, 1.4, 1.5, 1.5, 0.4, 1.7, 0.6, 1.2... ## $ work_at_home &lt;dbl&gt; 1.8, 3.9, 1.6, 0.7, 2.3, 2.8, 1.7, 2.7... ## $ mean_commute &lt;dbl&gt; 26.5, 26.4, 24.1, 28.8, 34.9, 27.5, 24... ## $ employed &lt;int&gt; 23986, 85953, 8597, 8294, 22189, 3865,... ## $ private_work &lt;dbl&gt; 73.6, 81.5, 71.8, 76.8, 82.0, 79.5, 77... ## $ public_work &lt;dbl&gt; 20.9, 12.3, 20.8, 16.1, 13.5, 15.1, 16... ## $ self_employed &lt;dbl&gt; 5.5, 5.8, 7.3, 6.7, 4.2, 5.4, 6.2, 5.0... ## $ family_work &lt;dbl&gt; 0.0, 0.4, 0.1, 0.4, 0.4, 0.0, 0.2, 0.1... ## $ unemployment &lt;dbl&gt; 7.6, 7.5, 17.6, 8.3, 7.7, 18.0, 10.9, ... ## $ land_area &lt;dbl&gt; 594.44, 1589.78, 884.88, 622.58, 644.7... 4.1.3 Selecting columns Select the following four columns from the counties variable: state county population poverty You dont need to save the result to a variable. 4.1.3.1 Select the columns counties %&gt;% select(state, county, population, poverty) Recall that if you want to keep the data youve selected, you can use assignment to create a new table. 4.1.4 Video: The filter and arrange verbs 4.1.5 Arranging observations Here you see the counties_selected dataset with a few interesting variables selected. These variables: private_work, public_work, self_employed describe whether people work for the government, for private companies, or for themselves. In these exercises, youll sort these observations to find the most interesting cases. counties_selected &lt;- counties %&gt;% select(state, county, population, private_work, public_work, self_employed) 4.1.5.1 Add a verb to sort in descending order of public_work counties_selected %&gt;% arrange(desc(public_work)) We sorted the counties in descending order according to public_work. What if we were interested in looking at observations in counties that have a large population or within a specific state? Lets take a look at that next! 4.1.6 Filtering for conditions You use the filter() verb to get only observations that match a particular condition, or match multiple conditions. counties_selected &lt;- counties %&gt;% select(state, county, population) 4.1.6.1 Filter for counties with a population above 1000000 counties_selected %&gt;% filter(population &gt; 1000000) 4.1.6.2 Filter for counties in the state of California that have a population above 1000000 counties_selected %&gt;% filter(state == &quot;California&quot;, population &gt; 1000000) ## state county population ## 1 California Alameda 1584983 ## 2 California Contra Costa 1096068 ## 3 California Los Angeles 10038388 ## 4 California Orange 3116069 ## 5 California Riverside 2298032 ## 6 California Sacramento 1465832 ## 7 California San Bernardino 2094769 ## 8 California San Diego 3223096 ## 9 California Santa Clara 1868149 Now you know that there are 9 counties in the state of California with a population greater than one million. In the next exercise, youll practice filtering and then sorting a dataset to focus on specific observations! 4.1.7 Filtering and arranging Were often interested in both filtering and sorting a dataset, to focus on observations of particular interest to you. Here, youll find counties that are extreme examples of what fraction of the population works in the private sector. counties_selected &lt;- counties %&gt;% select(state, county, population, private_work, public_work, self_employed) 4.1.7.1 Filter for Texas and more than 10000 people; sort in descending order of private_work counties_selected %&gt;% filter(state == &quot;Texas&quot;, population &gt; 10000) %&gt;% arrange(desc(private_work)) ## state county population private_work public_work ## 1 Texas Gregg 123178 84.7 9.8 ## 2 Texas Collin 862215 84.1 10.0 ## 3 Texas Dallas 2485003 83.9 9.5 ## 4 Texas Harris 4356362 83.4 10.1 ## 5 Texas Andrews 16775 83.1 9.6 ## 6 Texas Tarrant 1914526 83.1 11.4 ## 7 Texas Titus 32553 82.5 10.0 ## 8 Texas Denton 731851 82.2 11.9 ## 9 Texas Ector 149557 82.0 11.2 ## 10 Texas Moore 22281 82.0 11.7 ## 11 Texas Jefferson 252872 81.9 13.4 ## 12 Texas Fort Bend 658331 81.7 12.5 ## 13 Texas Panola 23900 81.5 11.7 ## 14 Texas Midland 151290 81.2 11.4 ## 15 Texas Potter 122352 81.2 12.4 ## 16 Texas Frio 18168 81.1 15.4 ## 17 Texas Johnson 155450 80.9 13.3 ## 18 Texas Smith 217552 80.9 12.7 ## 19 Texas Orange 83217 80.8 13.7 ## 20 Texas Harrison 66417 80.6 13.9 ## 21 Texas Brazoria 331741 80.5 14.4 ## 22 Texas Calhoun 21666 80.5 12.8 ## 23 Texas Austin 28886 80.4 11.8 ## 24 Texas Montgomery 502586 80.4 11.7 ## 25 Texas Jim Wells 41461 80.3 12.6 ## 26 Texas Hutchinson 21858 80.2 13.9 ## 27 Texas Ochiltree 10642 80.1 10.4 ## 28 Texas Victoria 90099 80.1 12.5 ## 29 Texas Hardin 55375 80.0 15.7 ## 30 Texas Bexar 1825502 79.6 14.8 ## 31 Texas Gray 22983 79.6 13.3 ## 32 Texas McLennan 241505 79.5 14.7 ## 33 Texas Newton 14231 79.4 15.8 ## 34 Texas Ward 11225 79.3 15.0 ## 35 Texas Grimes 26961 79.2 15.0 ## 36 Texas Lamar 49566 79.2 13.9 ## 37 Texas Rusk 53457 79.2 12.8 ## 38 Texas Wharton 41264 78.9 12.3 ## 39 Texas Williamson 473592 78.9 14.9 ## 40 Texas Marion 10248 78.7 15.2 ## 41 Texas Hood 53171 78.6 11.0 ## 42 Texas Hunt 88052 78.6 14.4 ## 43 Texas Ellis 157058 78.5 14.2 ## 44 Texas Upshur 40096 78.5 13.0 ## 45 Texas Grayson 122780 78.4 13.8 ## 46 Texas Liberty 77486 78.4 13.6 ## 47 Texas Atascosa 47050 78.1 14.3 ## 48 Texas Waller 45847 78.1 16.3 ## 49 Texas Deaf Smith 19245 78.0 13.6 ## 50 Texas Chambers 37251 77.9 15.9 ## 51 Texas Jasper 35768 77.7 15.5 ## 52 Texas Scurry 17238 77.7 16.2 ## 53 Texas Parmer 10004 77.6 12.5 ## 54 Texas San Patricio 66070 77.6 17.9 ## 55 Texas Taylor 134435 77.5 16.5 ## 56 Texas Fayette 24849 77.4 13.4 ## 57 Texas Kaufman 109289 77.4 16.7 ## 58 Texas Matagorda 36598 77.4 15.6 ## 59 Texas Comal 119632 77.2 13.2 ## 60 Texas Gonzales 20172 77.2 13.2 ## 61 Texas Hill 34923 77.1 15.0 ## 62 Texas Hockley 23322 77.1 15.6 ## 63 Texas Guadalupe 143460 77.0 17.9 ## 64 Texas Cooke 38761 76.9 14.8 ## 65 Texas Rockwall 85536 76.9 16.2 ## 66 Texas Wise 61243 76.9 14.9 ## 67 Texas Gaines 18916 76.8 12.0 ## 68 Texas Tom Green 115056 76.8 16.5 ## 69 Texas Erath 40039 76.7 15.3 ## 70 Texas Hopkins 35645 76.7 13.2 ## 71 Texas Palo Pinto 27921 76.7 14.1 ## 72 Texas Nueces 352060 76.6 16.3 ## 73 Texas Parker 121418 76.6 15.2 ## 74 Texas Lubbock 290782 76.5 16.8 ## 75 Texas Travis 1121645 76.5 16.0 ## 76 Texas Eastland 18328 76.4 15.1 ## 77 Texas Montague 19478 76.4 13.3 ## 78 Texas Angelina 87748 76.2 17.8 ## 79 Texas Jackson 14486 76.1 13.6 ## 80 Texas Nolan 15061 76.1 17.1 ## 81 Texas Cass 30328 76.0 15.9 ## 82 Texas Duval 11577 76.0 17.2 ## 83 Texas Hale 35504 75.9 16.4 ## 84 Texas Galveston 308163 75.8 18.2 ## 85 Texas Bosque 17971 75.7 12.8 ## 86 Texas Henderson 79016 75.6 13.9 ## 87 Texas Bowie 93155 75.5 19.4 ## 88 Texas Aransas 24292 75.4 12.4 ## 89 Texas Randall 126782 75.4 17.5 ## 90 Texas Morris 12700 75.3 17.5 ## 91 Texas Rains 11037 75.3 15.6 ## 92 Texas Shelby 25725 75.3 16.8 ## 93 Texas Brown 37833 75.2 16.5 ## 94 Texas Sabine 10440 75.1 18.1 ## 95 Texas Wood 42712 75.0 15.6 ## 96 Texas Gillespie 25398 74.8 10.5 ## 97 Texas Navarro 48118 74.8 17.3 ## 98 Texas Caldwell 39347 74.7 18.6 ## 99 Texas Tyler 21462 74.6 19.8 ## 100 Texas Webb 263251 74.6 19.4 ## 101 Texas Wichita 131957 74.6 19.2 ## 102 Texas Lee 16664 74.5 16.7 ## 103 Texas Burnet 44144 74.1 14.2 ## 104 Texas El Paso 831095 73.9 20.0 ## 105 Texas Kendall 37361 73.9 15.7 ## 106 Texas Lamb 13742 73.8 16.1 ## 107 Texas Camp 12516 73.7 16.0 ## 108 Texas Freestone 19586 73.6 19.0 ## 109 Texas Medina 47392 73.6 17.3 ## 110 Texas Washington 34236 73.6 18.1 ## 111 Texas Hays 177562 73.5 19.4 ## 112 Texas Live Oak 11873 73.5 14.2 ## 113 Texas Lavaca 19549 73.4 14.4 ## 114 Texas Zapata 14308 73.3 18.1 ## 115 Texas Fannin 33748 73.1 19.8 ## 116 Texas Young 18329 73.1 15.7 ## 117 Texas Cherokee 51167 72.9 20.4 ## 118 Texas Kerr 50149 72.8 16.7 ## 119 Texas Cameron 417947 72.7 18.2 ## 120 Texas Colorado 20757 72.6 17.3 ## 121 Texas Clay 10479 72.4 17.0 ## 122 Texas DeWitt 20540 72.4 17.7 ## 123 Texas Franklin 10599 72.3 13.7 ## 124 Texas Comanche 13623 72.2 15.0 ## 125 Texas Howard 36105 72.0 23.0 ## 126 Texas Llano 19323 72.0 11.4 ## 127 Texas Nacogdoches 65531 72.0 20.0 ## 128 Texas Wilson 45509 71.8 20.6 ## 129 Texas Van Zandt 52736 71.6 16.8 ## 130 Texas Bastrop 76948 71.3 20.1 ## 131 Texas Callahan 13532 71.2 17.0 ## 132 Texas Falls 17410 70.8 20.5 ## 133 Texas Hidalgo 819217 70.7 17.2 ## 134 Texas Bee 32659 70.5 22.7 ## 135 Texas Dimmit 10682 70.5 21.9 ## 136 Texas Blanco 10723 70.4 17.5 ## 137 Texas Zavala 12060 70.4 23.8 ## 138 Texas Terry 12687 70.2 19.7 ## 139 Texas Anderson 57915 69.8 23.6 ## 140 Texas Robertson 16532 69.8 22.4 ## 141 Texas Karnes 14879 69.6 22.0 ## 142 Texas Kleberg 32029 69.5 25.9 ## 143 Texas Burleson 17293 69.4 21.7 ## 144 Texas Leon 16819 69.4 14.5 ## 145 Texas Bandera 20796 69.2 19.0 ## 146 Texas Bell 326041 69.2 25.7 ## 147 Texas San Jacinto 27023 69.1 23.0 ## 148 Texas Maverick 56548 68.8 25.0 ## 149 Texas Trinity 14405 68.8 17.8 ## 150 Texas Jones 19978 68.7 20.5 ## 151 Texas Polk 46113 68.7 22.2 ## 152 Texas Pecos 15807 68.6 24.5 ## 153 Texas Dawson 13542 68.5 21.9 ## 154 Texas Milam 24344 68.5 21.3 ## 155 Texas Red River 12567 68.5 17.4 ## 156 Texas Brazos 205271 68.3 25.6 ## 157 Texas Starr 62648 68.2 22.5 ## 158 Texas Houston 22949 67.8 22.1 ## 159 Texas Reeves 14179 67.8 28.3 ## 160 Texas Runnels 10445 67.7 19.9 ## 161 Texas Willacy 22002 66.9 23.9 ## 162 Texas Uvalde 26952 66.8 23.8 ## 163 Texas Wilbarger 13158 66.0 29.6 ## 164 Texas Val Verde 48980 65.9 28.9 ## 165 Texas Lampasas 20219 65.5 26.5 ## 166 Texas Madison 13838 65.1 22.4 ## self_employed ## 1 5.4 ## 2 5.8 ## 3 6.4 ## 4 6.3 ## 5 6.8 ## 6 5.4 ## 7 7.4 ## 8 5.7 ## 9 6.7 ## 10 5.9 ## 11 4.5 ## 12 5.7 ## 13 6.9 ## 14 7.1 ## 15 6.2 ## 16 3.4 ## 17 5.7 ## 18 6.3 ## 19 5.4 ## 20 5.4 ## 21 5.0 ## 22 6.7 ## 23 7.7 ## 24 7.8 ## 25 7.1 ## 26 5.8 ## 27 9.2 ## 28 7.1 ## 29 4.2 ## 30 5.6 ## 31 7.0 ## 32 5.7 ## 33 4.8 ## 34 5.3 ## 35 5.7 ## 36 6.7 ## 37 7.9 ## 38 8.6 ## 39 6.0 ## 40 5.9 ## 41 9.9 ## 42 7.0 ## 43 7.3 ## 44 8.3 ## 45 7.5 ## 46 7.8 ## 47 7.5 ## 48 5.6 ## 49 7.9 ## 50 6.1 ## 51 6.7 ## 52 6.0 ## 53 9.3 ## 54 4.4 ## 55 5.7 ## 56 9.0 ## 57 5.8 ## 58 7.0 ## 59 9.1 ## 60 9.1 ## 61 7.6 ## 62 7.1 ## 63 4.8 ## 64 8.1 ## 65 6.7 ## 66 8.1 ## 67 11.2 ## 68 6.6 ## 69 7.9 ## 70 9.8 ## 71 9.0 ## 72 6.9 ## 73 8.0 ## 74 6.5 ## 75 7.4 ## 76 7.8 ## 77 9.9 ## 78 5.9 ## 79 9.3 ## 80 6.5 ## 81 8.1 ## 82 6.4 ## 83 7.4 ## 84 5.8 ## 85 11.2 ## 86 10.4 ## 87 5.1 ## 88 12.2 ## 89 7.1 ## 90 7.1 ## 91 9.0 ## 92 7.8 ## 93 8.2 ## 94 6.8 ## 95 9.0 ## 96 14.3 ## 97 7.8 ## 98 6.7 ## 99 5.3 ## 100 5.8 ## 101 6.0 ## 102 8.4 ## 103 11.7 ## 104 6.0 ## 105 10.4 ## 106 9.9 ## 107 9.9 ## 108 7.3 ## 109 9.0 ## 110 8.3 ## 111 7.0 ## 112 12.1 ## 113 12.1 ## 114 8.7 ## 115 7.1 ## 116 10.8 ## 117 6.5 ## 118 10.4 ## 119 8.9 ## 120 9.0 ## 121 10.4 ## 122 9.6 ## 123 13.3 ## 124 12.3 ## 125 4.9 ## 126 16.4 ## 127 7.9 ## 128 7.6 ## 129 11.1 ## 130 8.3 ## 131 9.4 ## 132 8.4 ## 133 11.9 ## 134 6.7 ## 135 7.6 ## 136 11.8 ## 137 5.6 ## 138 9.8 ## 139 6.4 ## 140 7.6 ## 141 7.8 ## 142 4.4 ## 143 8.4 ## 144 15.7 ## 145 11.6 ## 146 5.0 ## 147 7.6 ## 148 6.1 ## 149 12.1 ## 150 10.3 ## 151 8.7 ## 152 6.0 ## 153 9.5 ## 154 9.8 ## 155 13.8 ## 156 5.8 ## 157 8.6 ## 158 9.8 ## 159 3.9 ## 160 12.1 ## 161 9.1 ## 162 9.4 ## 163 4.4 ## 164 5.1 ## 165 8.0 ## 166 11.8 ## [ reached getOption(&quot;max.print&quot;) -- omitted 3 rows ] Youve learned how to filter and sort a dataset to answer questions about the data. Notice that you only need to slightly modify your code if you are interested in sorting the observations by a different column. 4.1.8 Video: Mutate 4.1.9 Calculating the number of government employees In the video, you used the unemployment variable, which is a percentage, to calculate the number of unemployed people in each county. In this exercise, youll do the same with another percentage variable: public_work. The code provided already selects the state, county, population, and public_work columns. counties_selected &lt;- counties %&gt;% select(state, county, population, public_work) 4.1.9.1 Sort in descending order of the public_workers column counties_selected %&gt;% mutate(public_workers = public_work * population / 100) %&gt;% arrange(desc(public_workers)) It looks like Los Angeles is the county with the most government employees. 4.1.10 Calculating the percentage of women in a county The dataset includes columns for the total number (not percentage) of men and women in each county. You could use this, along with the population variable, to compute the fraction of men (or women) within each county. In this exercise, youll select the relevant columns yourself. 4.1.10.1 Select the columns state, county, population, men, and women counties_selected &lt;- counties %&gt;% select(state, county, population, men, women) 4.1.10.2 Calculate proportion_women as the fraction of the population made up of women counties_selected %&gt;% mutate(proportion_women = women / population) Notice that the proportion_women variable was added as a column to the counties_selected dataset, and the data now has 6 columns instead of 5. 4.1.11 Select, mutate, filter, and arrange In this exercise, youll put together everything youve learned in this chapter (select(), mutate(), filter() and arrange()), to find the counties with the highest proportion of men. counties %&gt;% # Select the five columns select(state, county, population, men, women) %&gt;% # Add the proportion_men variable mutate(proportion_men = men / population) %&gt;% # Filter for population of at least 10,000 filter(population &gt;= 10000) %&gt;% # Arrange proportion of men in descending order arrange(desc(proportion_men)) ## state county population men ## 1 Virginia Sussex 11864 8130 ## 2 California Lassen 32645 21818 ## 3 Georgia Chattahoochee 11914 7940 ## 4 Louisiana West Feliciana 15415 10228 ## 5 Florida Union 15191 9830 ## 6 Texas Jones 19978 12652 ## 7 Missouri DeKalb 12782 8080 ## 8 Texas Madison 13838 8648 ## 9 Virginia Greensville 11760 7303 ## 10 Texas Anderson 57915 35469 ## 11 Arkansas Lincoln 14062 8596 ## 12 Texas Bee 32659 19722 ## 13 Florida Hamilton 14395 8671 ## 14 Illinois Lawrence 16665 9997 ## 15 Mississippi Tallahatchie 14959 8907 ## 16 Mississippi Greene 14129 8409 ## 17 Texas Karnes 14879 8799 ## 18 Texas Frio 18168 10723 ## 19 Florida Gulf 15785 9235 ## 20 Florida Franklin 11628 6800 ## 21 Texas Walker 69330 40484 ## 22 Georgia Telfair 16416 9502 ## 23 Colorado Fremont 46809 27003 ## 24 Louisiana Allen 25653 14784 ## 25 Ohio Noble 14508 8359 ## 26 Georgia Tattnall 25302 14529 ## 27 Louisiana Claiborne 16639 9524 ## 28 Texas Pecos 15807 9003 ## 29 Missouri Pulaski 53443 30390 ## 30 Texas Howard 36105 20496 ## 31 Illinois Johnson 12829 7267 ## 32 Texas Dawson 13542 7670 ## 33 Florida DeSoto 34957 19756 ## 34 Texas Reeves 14179 8004 ## 35 Florida Taylor 22685 12781 ## 36 Tennessee Bledsoe 13686 7677 ## 37 Louisiana Grant 22362 12523 ## 38 Florida Wakulla 31128 17366 ## 39 Tennessee Morgan 21794 12150 ## 40 Kentucky Morgan 13428 7477 ## 41 Florida Bradford 27223 15150 ## 42 California Kings 150998 83958 ## 43 Kentucky Martin 12631 7020 ## 44 Virginia Buckingham 17068 9476 ## 45 California Del Norte 27788 15418 ## 46 Missouri Pike 18517 10273 ## 47 Florida Jackson 48900 27124 ## 48 Mississippi Yazoo 27911 15460 ## 49 Florida Glades 13272 7319 ## 50 New York Franklin 51280 28253 ## 51 Pennsylvania Union 44958 24755 ## 52 Tennessee Wayne 16897 9295 ## 53 Colorado Summit 28940 15912 ## 54 Texas Grimes 26961 14823 ## 55 Georgia Macon 14045 7720 ## 56 Ohio Madison 43456 23851 ## 57 Arkansas St. Francis 27345 14996 ## 58 Michigan Chippewa 38586 21156 ## 59 Kentucky McCreary 18001 9863 ## 60 Florida Washington 24629 13478 ## 61 Virginia Prince George 37380 20440 ## 62 Florida Calhoun 14615 7991 ## 63 North Carolina Avery 17695 9673 ## 64 Indiana Sullivan 21111 11540 ## 65 Oregon Malheur 30551 16697 ## 66 Wyoming Carbon 15739 8592 ## 67 Maryland Somerset 25980 14150 ## 68 Tennessee Hardeman 26253 14297 ## 69 Florida Dixie 16091 8746 ## 70 Florida Hardee 27468 14920 ## 71 Texas Willacy 22002 11947 ## 72 North Dakota Williams 29619 16066 ## 73 Colorado Logan 21928 11889 ## 74 Michigan Houghton 36660 19876 ## 75 Texas Tyler 21462 11629 ## 76 North Carolina Onslow 183753 99526 ## 77 New York Wyoming 41446 22442 ## 78 Illinois Randolph 33069 17902 ## 79 California Amador 36995 20012 ## 80 Texas Live Oak 11873 6421 ## 81 Michigan Gogebic 15824 8556 ## 82 Oklahoma Hughes 13785 7450 ## 83 Texas Scurry 17238 9312 ## 84 Georgia Dooly 14293 7717 ## 85 Wisconsin Adams 20451 11033 ## 86 Florida Okeechobee 39255 21176 ## 87 North Carolina Greene 21328 11499 ## 88 Missouri St. Francois 66010 35586 ## 89 Texas Terry 12687 6835 ## 90 Colorado Gunnison 15651 8427 ## 91 Missouri Mississippi 14208 7650 ## 92 Alabama Barbour 26932 14497 ## 93 Indiana Miami 36211 19482 ## 94 South Carolina Edgefield 26466 14234 ## 95 Oklahoma Okfuskee 12248 6585 ## 96 Virginia Powhatan 28207 15159 ## 97 Louisiana East Feliciana 19855 10666 ## 98 Oklahoma Beckham 23300 12515 ## 99 Texas Polk 46113 24757 ## 100 Wisconsin Jackson 20543 11022 ## 101 Alaska Fairbanks North Star Borough 99705 53477 ## 102 Indiana Perry 19414 10407 ## 103 Arizona Graham 37407 20049 ## 104 Kentucky Clay 21300 11415 ## 105 Tennessee Johnson 18017 9643 ## 106 Colorado Chaffee 18309 9798 ## 107 Louisiana Catahoula 10247 5483 ## 108 Georgia Charlton 13130 7024 ## 109 Florida Holmes 19635 10501 ## 110 Alaska Kodiak Island Borough 13973 7468 ## 111 Kansas Leavenworth 78227 41806 ## 112 Alabama Bibb 22604 12073 ## 113 Minnesota Pine 29218 15605 ## 114 Colorado Grand 14411 7689 ## 115 South Carolina Marlboro 27993 14930 ## 116 Kansas Riley 75022 40002 ## 117 Texas Gray 22983 12253 ## 118 Texas Houston 22949 12227 ## 119 Oklahoma Woodward 20986 11172 ## 120 Ohio Marion 65943 35022 ## 121 Georgia Butts 23445 12451 ## 122 Pennsylvania Wayne 51642 27420 ## 123 Illinois Perry 21810 11572 ## 124 Michigan Gratiot 41878 22213 ## 125 Colorado Eagle 52576 27887 ## 126 Indiana Putnam 37650 19967 ## 127 Missouri Cooper 17593 9328 ## 128 Colorado Pitkin 17420 9235 ## 129 Wyoming Goshen 13544 7180 ## 130 Alabama Bullock 10678 5660 ## 131 Florida Jefferson 14198 7522 ## 132 Florida Hendry 38363 20318 ## 133 Virginia Nottoway 15711 8320 ## 134 Georgia Dodge 21180 11215 ## 135 Massachusetts Nantucket 10556 5589 ## 136 Michigan Ionia 64064 33917 ## 137 Wyoming Sublette 10117 5353 ## 138 Wisconsin Juneau 26494 14018 ## 139 Florida Monroe 75901 40159 ## 140 Pennsylvania Huntingdon 45906 24284 ## 141 Illinois Lee 35027 18524 ## 142 Louisiana Winn 14855 7851 ## 143 Ohio Pickaway 56515 29866 ## 144 Kentucky Oldham 63037 33307 ## 145 Texas Fannin 33748 17810 ## 146 New York Seneca 35144 18527 ## 147 Texas Rusk 53457 28172 ## 148 Florida Madison 18729 9869 ## 149 Colorado Park 16189 8525 ## 150 Florida Gilchrist 16992 8946 ## 151 Florida Baker 27135 14277 ## 152 Alaska Bethel Census Area 17776 9351 ## 153 Virginia Manassas Park city 15625 8219 ## 154 Nevada Humboldt 17067 8971 ## 155 Wisconsin Waushara 24321 12783 ## 156 Texas DeWitt 20540 10795 ## 157 Oklahoma Atoka 13906 7307 ## 158 Idaho Idaho 16312 8571 ## 159 Ohio Ross 77334 40627 ## 160 Oklahoma Texas 21588 11340 ## 161 Utah Sanpete 28261 14845 ## 162 Illinois Fayette 22136 11622 ## 163 Tennessee Hickman 24283 12745 ## 164 Virginia Southampton 18410 9661 ## 165 Virginia Brunswick 16930 8882 ## 166 Virginia Lunenburg 12558 6588 ## women proportion_men ## 1 3734 0.6852664 ## 2 10827 0.6683412 ## 3 3974 0.6664428 ## 4 5187 0.6635096 ## 5 5361 0.6470937 ## 6 7326 0.6332966 ## 7 4702 0.6321389 ## 8 5190 0.6249458 ## 9 4457 0.6210034 ## 10 22446 0.6124320 ## 11 5466 0.6112928 ## 12 12937 0.6038764 ## 13 5724 0.6023619 ## 14 6668 0.5998800 ## 15 6052 0.5954275 ## 16 5720 0.5951589 ## 17 6080 0.5913704 ## 18 7445 0.5902136 ## 19 6550 0.5850491 ## 20 4828 0.5847953 ## 21 28846 0.5839319 ## 22 6914 0.5788255 ## 23 19806 0.5768762 ## 24 10869 0.5763069 ## 25 6149 0.5761649 ## 26 10773 0.5742234 ## 27 7115 0.5723902 ## 28 6804 0.5695578 ## 29 23053 0.5686432 ## 30 15609 0.5676776 ## 31 5562 0.5664510 ## 32 5872 0.5663861 ## 33 15201 0.5651515 ## 34 6175 0.5644968 ## 35 9904 0.5634119 ## 36 6009 0.5609382 ## 37 9839 0.5600125 ## 38 13762 0.5578900 ## 39 9644 0.5574929 ## 40 5951 0.5568216 ## 41 12073 0.5565147 ## 42 67040 0.5560206 ## 43 5611 0.5557755 ## 44 7592 0.5551910 ## 45 12370 0.5548438 ## 46 8244 0.5547875 ## 47 21776 0.5546830 ## 48 12451 0.5539035 ## 49 5953 0.5514617 ## 50 23027 0.5509555 ## 51 20203 0.5506250 ## 52 7602 0.5500977 ## 53 13028 0.5498272 ## 54 12138 0.5497941 ## 55 6325 0.5496618 ## 56 19605 0.5488540 ## 57 12349 0.5484001 ## 58 17430 0.5482818 ## 59 8138 0.5479140 ## 60 11151 0.5472411 ## 61 16940 0.5468165 ## 62 6624 0.5467670 ## 63 8022 0.5466516 ## 64 9571 0.5466345 ## 65 13854 0.5465288 ## 66 7147 0.5459051 ## 67 11830 0.5446497 ## 68 11956 0.5445854 ## 69 7345 0.5435337 ## 70 12548 0.5431775 ## 71 10055 0.5429961 ## 72 13553 0.5424221 ## 73 10039 0.5421835 ## 74 16784 0.5421713 ## 75 9833 0.5418414 ## 76 84227 0.5416293 ## 77 19004 0.5414757 ## 78 15167 0.5413529 ## 79 16983 0.5409380 ## 80 5452 0.5408069 ## 81 7268 0.5406977 ## 82 6335 0.5404425 ## 83 7926 0.5402019 ## 84 6576 0.5399146 ## 85 9418 0.5394846 ## 86 18079 0.5394472 ## 87 9829 0.5391504 ## 88 30424 0.5391001 ## 89 5852 0.5387404 ## 90 7224 0.5384320 ## 91 6558 0.5384291 ## 92 12435 0.5382816 ## 93 16729 0.5380133 ## 94 12232 0.5378221 ## 95 5663 0.5376388 ## 96 13048 0.5374198 ## 97 9189 0.5371947 ## 98 10785 0.5371245 ## 99 21356 0.5368768 ## 100 9521 0.5365331 ## 101 46228 0.5363522 ## 102 9007 0.5360565 ## 103 17358 0.5359692 ## 104 9885 0.5359155 ## 105 8374 0.5352167 ## 106 8511 0.5351466 ## 107 4764 0.5350834 ## 108 6106 0.5349581 ## 109 9134 0.5348103 ## 110 6505 0.5344593 ## 111 36421 0.5344191 ## 112 10531 0.5341090 ## 113 13613 0.5340886 ## 114 6722 0.5335508 ## 115 13063 0.5333476 ## 116 35020 0.5332036 ## 117 10730 0.5331332 ## 118 10722 0.5327901 ## 119 9814 0.5323549 ## 120 30921 0.5310950 ## 121 10994 0.5310727 ## 122 24222 0.5309632 ## 123 10238 0.5305823 ## 124 19665 0.5304217 ## 125 24689 0.5304131 ## 126 17683 0.5303320 ## 127 8265 0.5302109 ## 128 8185 0.5301378 ## 129 6364 0.5301240 ## 130 5018 0.5300618 ## 131 6676 0.5297929 ## 132 18045 0.5296249 ## 133 7391 0.5295653 ## 134 9965 0.5295090 ## 135 4967 0.5294619 ## 136 30147 0.5294237 ## 137 4764 0.5291094 ## 138 12476 0.5291009 ## 139 35742 0.5290971 ## 140 21622 0.5289940 ## 141 16503 0.5288492 ## 142 7004 0.5285089 ## 143 26649 0.5284615 ## 144 29730 0.5283722 ## 145 15938 0.5277350 ## 146 16617 0.5271739 ## 147 25285 0.5270030 ## 148 8860 0.5269368 ## 149 7664 0.5265921 ## 150 8046 0.5264831 ## 151 12858 0.5261470 ## 152 8425 0.5260464 ## 153 7406 0.5260160 ## 154 8096 0.5256343 ## 155 11538 0.5255952 ## 156 9745 0.5255599 ## 157 6599 0.5254566 ## 158 7741 0.5254414 ## 159 36707 0.5253446 ## 160 10248 0.5252918 ## 161 13416 0.5252822 ## 162 10514 0.5250271 ## 163 11538 0.5248528 ## 164 8749 0.5247691 ## 165 8048 0.5246308 ## 166 5970 0.5246058 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2272 rows ] Notice Sussex County in Virginia is more than two thirds male: this is because of two mens prisons in the county. 4.2 Aggregating Data Now that you know how to transform your data, youll want to know more about how to aggregate your data to make it more interpretable. Youll learn a number of functions you can use to take many observations in your data and summarize them, including count, group_by, summarize, ungroup, and top_n. 4.2.1 Video: The count verb 4.2.2 Counting by region The counties dataset contains columns for region, state, population, and the number of citizens, which we selected and saved as the counties_selected table. In this exercise, youll focus on the region column. counties_selected &lt;- counties %&gt;% select(region, state, population, citizens) 4.2.2.1 Use count to find the number of counties in each region counties_selected %&gt;% count(region, sort=TRUE) ## # A tibble: 4 x 2 ## region n ## &lt;fct&gt; &lt;int&gt; ## 1 South 1420 ## 2 North Central 1055 ## 3 West 448 ## 4 Northeast 218 Since the results have been arranged, you can see that the South has the greatest number of counties. 4.2.3 Counting citizens by state You can weigh your count by particular variables rather than finding the number of counties. In this case, youll find the number of citizens in each state. counties_selected &lt;- counties %&gt;% select(region, state, population, citizens) 4.2.3.1 Find number of counties per state, weighted by citizens counties_selected %&gt;% count(state, wt=citizens, sort=TRUE) ## # A tibble: 50 x 2 ## state n ## &lt;fct&gt; &lt;int&gt; ## 1 California 24280349 ## 2 Texas 16864962 ## 3 Florida 13933052 ## 4 New York 13531404 ## 5 Pennsylvania 9710416 ## 6 Illinois 8979999 ## 7 Ohio 8709050 ## 8 Michigan 7380136 ## 9 North Carolina 7107998 ## 10 Georgia 6978660 ## # ... with 40 more rows From our result, we can see that California is the state with the most citizens. 4.2.4 Mutating and counting You can combine multiple verbs together to answer increasingly complicated questions of your data. For example: What are the US states where the most people walk to work? Youll use the walk column, which offers a percentage of people in each county that walk to work, to add a new column and count based on it. counties_selected &lt;- counties %&gt;% select(state, population, walk) counties_selected %&gt;% # Add population_walk containing the total number of people who walk to work mutate(population_walk = population * walk / 100) %&gt;% # Count weighted by the new column count(state, wt = population_walk, sort=TRUE) ## # A tibble: 50 x 2 ## state n ## &lt;fct&gt; &lt;dbl&gt; ## 1 New York 1237938. ## 2 California 1017964. ## 3 Pennsylvania 505397. ## 4 Texas 430793. ## 5 Illinois 400346. ## 6 Massachusetts 316765. ## 7 Florida 284723. ## 8 New Jersey 273047. ## 9 Ohio 266911. ## 10 Washington 239764. ## # ... with 40 more rows We can see that while California had the largest total population, New York state has the largest number of people who walk to work. 4.2.5 Video: The group by, summarize and ungroup verbs 4.2.5.1 Summarizing The summarize() verb is very useful for collapsing a large dataset into a single observation. counties_selected &lt;- counties %&gt;% select(county, population, income, unemployment) 4.2.5.2 Summarize to find minimum population, maximum unemployment, and average income counties_selected %&gt;% summarise(min_population = min(population), max_unemployment = max(unemployment), average_income = mean(income)) ## min_population max_unemployment average_income ## 1 85 29.4 NA If we wanted to take this a step further, we could use filter() to determine the specific counties that returned the value for min_population and max_unemployment. counties_selected %&gt;% filter(population == min(population)) ## county population income unemployment ## 1 Kalawao 85 66250 0 counties_selected %&gt;% filter(unemployment == max(unemployment)) ## county population income unemployment ## 1 Corson 4149 31676 29.4 4.2.6 Summarizing by state Another interesting column is land_area, which shows the land area in square miles. Here, youll summarize both population and land area by state, with the purpose of finding the density (in people per square miles). counties_selected &lt;- counties %&gt;% select(state, county, population, land_area) 4.2.6.1 Add a density column, then sort in descending order counties_selected %&gt;% group_by(state) %&gt;% summarize(total_area = sum(land_area), total_population = sum(population)) %&gt;% mutate(density = total_population / total_area) %&gt;% arrange(desc(density)) ## # A tibble: 50 x 4 ## state total_area total_population density ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 New Jersey 7354. 8904413 1211. ## 2 Rhode Island 1034. 1053661 1019. ## 3 Massachusetts 7800. 6705586 860. ## 4 Connecticut 4842. 3593222 742. ## 5 Maryland 9707. 5930538 611. ## 6 Delaware 1949. 926454 475. ## 7 New York 47126. 19673174 417. ## 8 Florida 53625. 19645772 366. ## 9 Pennsylvania 44743. 12779559 286. ## 10 Ohio 40861. 11575977 283. ## # ... with 40 more rows Looks like New Jersey and Rhode Island are the most crowded of the US states, with more than a thousand people per square mile. 4.2.7 Summarizing by state and region You can group by multiple columns instead of grouping by one. Here, youll practice aggregating by state and region, and notice how useful it is for performing multiple aggregations in a row. counties_selected &lt;- counties %&gt;% select(region, state, county, population) 4.2.7.1 Calculate the average_pop and median_pop columns counties_selected %&gt;% group_by(region, state) %&gt;% summarize(total_pop = sum(population)) %&gt;% summarize(average_pop = mean(total_pop), median_pop = median(total_pop)) ## # A tibble: 4 x 3 ## region average_pop median_pop ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 North Central 5628866. 5580644 ## 2 Northeast 5600438. 2461161 ## 3 South 7369565. 4804098 ## 4 West 5723364. 2798636 It looks like the South has the highest average_pop of 7370486, while the North Central region has the highest median_pop of 5580644. 4.2.8 Video: The top_n verb 4.2.9 Selecting a county from each region Previously, you used the walk column, which offers a percentage of people in each county that walk to work, to add a new column and count to find the total number of people who walk to work in each county. Now, youre interested in finding the county within each region with the highest percentage of citizens who walk to work. counties_selected &lt;- counties %&gt;% select(region, state, county, metro, population, walk) 4.2.9.1 Group by region and find the greatest number of citizens who walk to work counties_selected %&gt;% group_by(region) %&gt;% top_n(1, walk) ## # A tibble: 4 x 6 ## # Groups: region [4] ## region state county metro population walk ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 West Alaska Aleutians East Bo~ Nonmet~ 3304 71.2 ## 2 Northeast New York New York Metro 1629507 20.7 ## 3 North Centr~ North Dako~ McIntosh Nonmet~ 2759 17.5 ## 4 South Virginia Lexington city Nonmet~ 7071 31.7 Notice that three of the places lots of people walk to work are low-population nonmetro counties, but that New York City also pops up! 4.2.10 Finding the highest-income state in each region Youve been learning to combine multiple dplyr verbs together. Here, youll combine group_by(), summarize(), and top_n() to find the state in each region with the highest income. When you group by multiple columns and then summarize, its important to remember that the summarize peels off one of the groups, but leaves the rest on. For example, if you group_by(X, Y) then summarize, the result will still be grouped by X. counties_selected &lt;- counties %&gt;% select(region, state, county, population, income) counties_selected %&gt;% group_by(region, state) %&gt;% # Calculate average income summarise(average_income = mean(income)) %&gt;% # Find the highest income state in each region top_n(1, average_income) ## # A tibble: 4 x 3 ## # Groups: region [4] ## region state average_income ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 North Central North Dakota 55575. ## 2 Northeast New Jersey 73014. ## 3 South Maryland 69200. ## 4 West Hawaii 64879 From our results, we can see that New Jersey in the Northeast is the state with the highest average_income of 73014. 4.2.11 Using summarize, top_n, and count together In this chapter, youve learned to use five dplyr verbs related to aggregation: count(), group_by(), summarize(), ungroup(), and top_n(). In this exercise, youll use all of them to answer a question: In how many states do more people live in metro areas than non-metro areas? Recall that the metro column has one of the two values Metro (for high-density city areas) or Nonmetro (for suburban and country areas). counties_selected &lt;- counties %&gt;% select(state, metro, population) 4.2.11.1 Count the states with more people in Metro or Nonmetro areas counties_selected %&gt;% group_by(state, metro) %&gt;% summarize(total_pop = sum(population)) %&gt;% top_n(1, total_pop) %&gt;% ungroup(total_pop) %&gt;% count(metro) ## # A tibble: 1 x 2 ## metro n ## &lt;fct&gt; &lt;int&gt; ## 1 &quot;&quot; 50 Notice that 44 states have more people living in Metro areas, and 6 states have more people living in Nonmetro areas. 4.3 Selecting and Transforming Data Learn advanced methods to select and transform columns. Also learn about select helpers, which are functions that specify criteria for columns you want to choose, as well as the rename and transmute verbs. 4.3.1 Video: Selecting 4.3.2 Selecting columns Using the select verb, we can answer interesting questions about our dataset by focusing in on related groups of verbs. The colon (:) is useful for getting many columns at a time. 4.3.2.1 Glimpse the counties table glimpse(counties) ## Observations: 3,141 ## Variables: 40 ## $ census_id &lt;int&gt; 1001, 1003, 1005, 1007, 1009, 1011, 10... ## $ state &lt;fct&gt; Alabama, Alabama, Alabama, Alabama, Al... ## $ county &lt;fct&gt; Autauga, Baldwin, Barbour, Bibb, Bloun... ## $ region &lt;fct&gt; South, South, South, South, South, Sou... ## $ metro &lt;fct&gt; , , , , , , , , , , , , , , , , , , , ... ## $ population &lt;int&gt; 55221, 195121, 26932, 22604, 57710, 10... ## $ men &lt;int&gt; 26745, 95314, 14497, 12073, 28512, 566... ## $ women &lt;int&gt; 28476, 99807, 12435, 10531, 29198, 501... ## $ hispanic &lt;dbl&gt; 2.6, 4.5, 4.6, 2.2, 8.6, 4.4, 1.2, 3.5... ## $ white &lt;dbl&gt; 75.8, 83.1, 46.2, 74.5, 87.9, 22.2, 53... ## $ black &lt;dbl&gt; 18.5, 9.5, 46.7, 21.4, 1.5, 70.7, 43.8... ## $ native &lt;dbl&gt; 0.4, 0.6, 0.2, 0.4, 0.3, 1.2, 0.1, 0.2... ## $ asian &lt;dbl&gt; 1.0, 0.7, 0.4, 0.1, 0.1, 0.2, 0.4, 0.9... ## $ pacific &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0... ## $ citizens &lt;int&gt; 40725, 147695, 20714, 17495, 42345, 80... ## $ income &lt;int&gt; 51281, 50254, 32964, 38678, 45813, 319... ## $ income_err &lt;int&gt; 2391, 1263, 2973, 3995, 3141, 5884, 17... ## $ income_per_cap &lt;int&gt; 24974, 27317, 16824, 18431, 20532, 175... ## $ income_per_cap_err &lt;int&gt; 1080, 711, 798, 1618, 708, 2055, 714, ... ## $ poverty &lt;dbl&gt; 12.9, 13.4, 26.7, 16.8, 16.7, 24.6, 25... ## $ child_poverty &lt;dbl&gt; 18.6, 19.2, 45.3, 27.9, 27.2, 38.4, 39... ## $ professional &lt;dbl&gt; 33.2, 33.1, 26.8, 21.5, 28.5, 18.8, 27... ## $ service &lt;dbl&gt; 17.0, 17.7, 16.1, 17.9, 14.1, 15.0, 16... ## $ office &lt;dbl&gt; 24.2, 27.1, 23.1, 17.8, 23.9, 19.7, 21... ## $ construction &lt;dbl&gt; 8.6, 10.8, 10.8, 19.0, 13.5, 20.1, 10.... ## $ production &lt;dbl&gt; 17.1, 11.2, 23.1, 23.7, 19.9, 26.4, 23... ## $ drive &lt;dbl&gt; 87.5, 84.7, 83.8, 83.2, 84.9, 74.9, 84... ## $ carpool &lt;dbl&gt; 8.8, 8.8, 10.9, 13.5, 11.2, 14.9, 12.4... ## $ transit &lt;dbl&gt; 0.1, 0.1, 0.4, 0.5, 0.4, 0.7, 0.0, 0.2... ## $ walk &lt;dbl&gt; 0.5, 1.0, 1.8, 0.6, 0.9, 5.0, 0.8, 1.2... ## $ other_transp &lt;dbl&gt; 1.3, 1.4, 1.5, 1.5, 0.4, 1.7, 0.6, 1.2... ## $ work_at_home &lt;dbl&gt; 1.8, 3.9, 1.6, 0.7, 2.3, 2.8, 1.7, 2.7... ## $ mean_commute &lt;dbl&gt; 26.5, 26.4, 24.1, 28.8, 34.9, 27.5, 24... ## $ employed &lt;int&gt; 23986, 85953, 8597, 8294, 22189, 3865,... ## $ private_work &lt;dbl&gt; 73.6, 81.5, 71.8, 76.8, 82.0, 79.5, 77... ## $ public_work &lt;dbl&gt; 20.9, 12.3, 20.8, 16.1, 13.5, 15.1, 16... ## $ self_employed &lt;dbl&gt; 5.5, 5.8, 7.3, 6.7, 4.2, 5.4, 6.2, 5.0... ## $ family_work &lt;dbl&gt; 0.0, 0.4, 0.1, 0.4, 0.4, 0.0, 0.2, 0.1... ## $ unemployment &lt;dbl&gt; 7.6, 7.5, 17.6, 8.3, 7.7, 18.0, 10.9, ... ## $ land_area &lt;dbl&gt; 594.44, 1589.78, 884.88, 622.58, 644.7... counties %&gt;% # Select state, county, population, and industry-related columns select(state, county, population, professional:production) %&gt;% # Arrange service in descending order arrange(desc(service)) Notice that when you select a group of related variables, its easy to find the insights youre looking for. 4.3.3 Select helpers In the video you learned about the select helper starts_with(). Another select helper is ends_with(), which finds the columns that end with a particular string. counties %&gt;% # Select the state, county, population, and those ending with &quot;work&quot; select(state, county, population, ends_with(&quot;work&quot;)) %&gt;% # Filter for counties that have at least 50% of people engaged in public work filter(public_work &gt;= 50) ## state county population private_work ## 1 Alaska Kusilvak Census Area 7914 45.4 ## 2 Alaska Lake and Peninsula Borough 1474 42.2 ## 3 Alaska Yukon-Koyukuk Census Area 5644 33.3 ## 4 California Lassen 32645 42.6 ## 5 Hawaii Kalawao 85 25.0 ## 6 North Dakota Sioux 4380 32.9 ## 7 South Dakota Oglala Lakota 14153 29.5 ## 8 South Dakota Todd 9942 34.4 ## 9 Wisconsin Menominee 4451 36.8 ## public_work family_work ## 1 53.8 0.3 ## 2 51.6 0.2 ## 3 61.7 0.0 ## 4 50.5 0.1 ## 5 64.1 0.0 ## 6 56.8 0.1 ## 7 66.2 0.0 ## 8 55.0 0.8 ## 9 59.1 0.4 It looks like only a few counties have more than half the population working for the government. 4.3.4 Video: The rename verb 4.3.5 Renaming a column after count The rename() verb is often useful for changing the name of a column that comes out of another verb, such ascount(). In this exercise, youll rename the n column from count() (which you learned about in Chapter 2) to something more descriptive. # Rename the n column to num_counties counties %&gt;% count(state) %&gt;% rename(num_counties = n) ## # A tibble: 50 x 2 ## state num_counties ## &lt;fct&gt; &lt;int&gt; ## 1 Alabama 67 ## 2 Alaska 29 ## 3 Arizona 15 ## 4 Arkansas 75 ## 5 California 58 ## 6 Colorado 64 ## 7 Connecticut 8 ## 8 Delaware 3 ## 9 Florida 67 ## 10 Georgia 159 ## # ... with 40 more rows Notice the difference between column names in the output from the first step to the second step. Dont forget, using rename() isnt the only way to choose a new name for a column! 4.3.6 Renaming a column as part of a select rename() isnt the only way you can choose a new name for a column: you can also choose a name as part of a select(). # Select state, county, and poverty as poverty_rate counties %&gt;% select(state, county, poverty_rate = poverty) As you can see, we were able to select the four columns of interest from our dataset, and rename one of those columns, using only the select() verb! 4.3.7 Video: The transmute verb 4.3.8 Question: Choosing among verbs Source: DataCamp Recall, you can think of transmute() as a combination of select() and mutate(), since you are getting back a subset of columns, but you are transforming and changing them at the same time. 4.3.9 Using transmute As you learned in the video, the transmute verb allows you to control which variables you keep, which variables you calculate, and which variables you drop. counties %&gt;% # Keep the state, county, and populations columns, and add a density column transmute(state, county, population, density = population / land_area) %&gt;% # Filter for counties with a population greater than one million filter(population &gt; 1000000) %&gt;% # Sort density in ascending order arrange(density) ## state county population density ## 1 California San Bernardino 2094769 104.4411 ## 2 Nevada Clark 2035572 257.9472 ## 3 California Riverside 2298032 318.8841 ## 4 Arizona Maricopa 4018143 436.7480 ## 5 Florida Palm Beach 1378806 699.9868 ## 6 California San Diego 3223096 766.1943 ## 7 Washington King 2045756 966.9999 ## 8 Texas Travis 1121645 1132.7459 ## 9 Florida Hillsborough 1302884 1277.0743 ## 10 Florida Orange 1229039 1360.4142 ## 11 Florida Miami-Dade 2639042 1390.6382 ## 12 Michigan Oakland 1229503 1417.0332 ## 13 California Santa Clara 1868149 1448.0653 ## 14 Utah Salt Lake 1078958 1453.5728 ## 15 Texas Bexar 1825502 1472.3928 ## 16 California Sacramento 1465832 1519.5638 ## 17 Florida Broward 1843152 1523.5305 ## 18 California Contra Costa 1096068 1530.9495 ## 19 New York Suffolk 1501373 1646.1521 ## 20 Pennsylvania Allegheny 1231145 1686.3152 ## 21 Massachusetts Middlesex 1556116 1902.7610 ## 22 Missouri St. Louis 1001327 1971.8925 ## 23 Maryland Montgomery 1017859 2071.9776 ## 24 California Alameda 1584983 2144.7092 ## 25 Minnesota Hennepin 1197776 2163.6518 ## 26 Texas Tarrant 1914526 2216.8873 ## 27 Ohio Franklin 1215761 2284.4492 ## 28 California Los Angeles 10038388 2473.8011 ## 29 Texas Harris 4356362 2557.3309 ## 30 Ohio Cuyahoga 1263189 2762.9410 ## 31 Texas Dallas 2485003 2852.1291 ## 32 Virginia Fairfax 1128722 2886.9785 ## 33 Michigan Wayne 1778969 2906.4322 ## 34 California Orange 3116069 3941.5472 ## 35 New York Nassau 1354612 4757.6988 ## 36 Illinois Cook 5236393 5539.2223 ## 37 Pennsylvania Philadelphia 1555072 11596.3609 ## 38 New York Queens 2301139 21202.7919 ## 39 New York Bronx 1428357 33927.7197 ## 40 New York Kings 2595259 36645.8486 ## 41 New York New York 1629507 71375.6899 Looks like San Bernadino is the lowest density county with a population about one million. counties %&gt;% # Keep the state, county, and populations columns, and add a density column transmute(state, county, population, density = population / land_area) %&gt;% # Filter for counties with a population greater than one million filter(population &gt; 1000000) %&gt;% # Sort density in ascending order arrange(density) ## state county population density ## 1 California San Bernardino 2094769 104.4411 ## 2 Nevada Clark 2035572 257.9472 ## 3 California Riverside 2298032 318.8841 ## 4 Arizona Maricopa 4018143 436.7480 ## 5 Florida Palm Beach 1378806 699.9868 ## 6 California San Diego 3223096 766.1943 ## 7 Washington King 2045756 966.9999 ## 8 Texas Travis 1121645 1132.7459 ## 9 Florida Hillsborough 1302884 1277.0743 ## 10 Florida Orange 1229039 1360.4142 ## 11 Florida Miami-Dade 2639042 1390.6382 ## 12 Michigan Oakland 1229503 1417.0332 ## 13 California Santa Clara 1868149 1448.0653 ## 14 Utah Salt Lake 1078958 1453.5728 ## 15 Texas Bexar 1825502 1472.3928 ## 16 California Sacramento 1465832 1519.5638 ## 17 Florida Broward 1843152 1523.5305 ## 18 California Contra Costa 1096068 1530.9495 ## 19 New York Suffolk 1501373 1646.1521 ## 20 Pennsylvania Allegheny 1231145 1686.3152 ## 21 Massachusetts Middlesex 1556116 1902.7610 ## 22 Missouri St. Louis 1001327 1971.8925 ## 23 Maryland Montgomery 1017859 2071.9776 ## 24 California Alameda 1584983 2144.7092 ## 25 Minnesota Hennepin 1197776 2163.6518 ## 26 Texas Tarrant 1914526 2216.8873 ## 27 Ohio Franklin 1215761 2284.4492 ## 28 California Los Angeles 10038388 2473.8011 ## 29 Texas Harris 4356362 2557.3309 ## 30 Ohio Cuyahoga 1263189 2762.9410 ## 31 Texas Dallas 2485003 2852.1291 ## 32 Virginia Fairfax 1128722 2886.9785 ## 33 Michigan Wayne 1778969 2906.4322 ## 34 California Orange 3116069 3941.5472 ## 35 New York Nassau 1354612 4757.6988 ## 36 Illinois Cook 5236393 5539.2223 ## 37 Pennsylvania Philadelphia 1555072 11596.3609 ## 38 New York Queens 2301139 21202.7919 ## 39 New York Bronx 1428357 33927.7197 ## 40 New York Kings 2595259 36645.8486 ## 41 New York New York 1629507 71375.6899 4.3.10 Question: Matching verbs to their definitions Weve learned a number of new verbs in this chapter that you can use to modify and change the variables you have. rename: Leaves the column you dont mention alone; doesnt allow you to calculate or change values. transmute: Must mention all the columns you keep; allows you to calculate or change values. mutate: Leaves the columns you dont mention alone; allows you to calculate or change values. Lets continue practising using the verbs to gain a better understanding of the differences between them. 4.3.11 Choosing among the four verbs In this chapter youve learned about the four verbs: select, mutate, transmute, and rename. Here, youll choose the appropriate verb for each situation. You wont need to change anything inside the parentheses. # Change the name of the unemployment column counties %&gt;% rename(unemployment_rate = unemployment) # Keep the state and county columns, and the columns containing poverty counties %&gt;% select(state, county, contains(&quot;poverty&quot;)) # Calculate the fraction_women column without dropping the other columns counties %&gt;% mutate(fraction_women = women / population) # Keep only the state, county, and employment_rate columns counties %&gt;% transmute(state, county, employment_rate = employed / population) Now you know which variable to choose depending on whether you want to keep, drop, rename, or change a variable in the dataset. 4.4 Case Study: The babynames Dataset Work with a new dataset that represents the names of babies born in the United States each year. Learn how to use grouped mutates and window functions to ask and answer more complex questions about your data. And use a combination of dplyr and ggplot2 to make interesting graphs to further explore your data. 4.4.1 Video: The babynames data library(babynames) library(dplyr) library(tidyr) babynames &lt;- babynames %&gt;% select(year, name, number) %&gt;% group_by(year, name) %&gt;% summarise(number = sum(number)) %&gt;% # ungroup() %&gt;% arrange(year, name) babynames ## # A tibble: 1,756,284 x 3 ## # Groups: year [138] ## year name number ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1880 Aaron 102 ## 2 1880 Ab 5 ## 3 1880 Abbie 71 ## 4 1880 Abbott 5 ## 5 1880 Abby 6 ## 6 1880 Abe 50 ## 7 1880 Abel 9 ## 8 1880 Abigail 12 ## 9 1880 Abner 27 ## 10 1880 Abraham 81 ## # ... with 1,756,274 more rows 4.4.2 Filtering and arranging for one year The dplyr verbs youve learned are useful for exploring data. For instance, you could find out the most common names in a particular year. babynames %&gt;% # Filter for the year 1990 filter(year == 1990) %&gt;% # Sort the number column in descending order arrange(desc(number)) ## # A tibble: 22,678 x 3 ## # Groups: year [1] ## year name number ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1990 Michael 65560 ## 2 1990 Christopher 52520 ## 3 1990 Jessica 46615 ## 4 1990 Ashley 45797 ## 5 1990 Matthew 44925 ## 6 1990 Joshua 43382 ## 7 1990 Brittany 36650 ## 8 1990 Amanda 34504 ## 9 1990 Daniel 33963 ## 10 1990 David 33862 ## # ... with 22,668 more rows It looks like the most common names for babies born in the US in 1990 were Michael, Christopher, and Jessica. 4.4.3 Using top_n with babynames You saw that you could use filter() and arrange() to find the most common names in one year. However, you could also use group_by and top_n to find the most common name in every year. # Find the most common name in each year babynames %&gt;% group_by(year) %&gt;% top_n(1, number) ## # A tibble: 138 x 3 ## # Groups: year [138] ## year name number ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1880 John 9701 ## 2 1881 John 8795 ## 3 1882 John 9597 ## 4 1883 John 8934 ## 5 1884 John 9428 ## 6 1885 Mary 9166 ## 7 1886 Mary 9921 ## 8 1887 Mary 9935 ## 9 1888 Mary 11804 ## 10 1889 Mary 11689 ## # ... with 128 more rows It looks like John was the most common name in 1880, and Mary was the most common name for a while after that. 4.4.4 Visualizing names with ggplot2 The dplyr package is very useful for exploring data, but its especially useful when combined with other tidyverse packages like ggplot2. (As of tidyverse 1.3.0, the following packages are included in the core tidyverse: dplyr, ggplot2, tidyr, readr, purrr, tibble, stringr, forcats. To make sure you are able to access the package, install all the packages in the tidyverse by running install.packages(\"tidyverse\"), then run library(tidyverse) to load the core tidyverse and make it available in your current R session.) # Filter for the names Steven, Thomas, and Matthew selected_names &lt;- babynames %&gt;% filter(name %in% c(&quot;Steven&quot;, &quot;Thomas&quot;, &quot;Matthew&quot;)) # Plot the names using a different color for each name ggplot(selected_names, aes(x = year, y = number, color = name)) + geom_line() It looks like names like Steven and Thomas were common in the 1950s, but Matthew became common more recently. 4.4.5 Video: Grouped mutates 4.4.6 Finding the year each name is most common In an earlier video, you learned how to filter for a particular name to determine the frequency of that name over time. Now, youre going to explore which year each name was the most common. To do this, youll be combining the grouped mutate approach with a top_n. # Calculate the fraction of people born each year with the same name babynames %&gt;% group_by(year) %&gt;% mutate(year_total = sum(number)) %&gt;% ungroup() %&gt;% mutate(fraction = number / year_total) %&gt;% # Find the year each name is most common group_by(name) %&gt;% top_n(1, fraction) ## # A tibble: 97,310 x 5 ## # Groups: name [97,310] ## year name number year_total fraction ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 Abbott 5 201484 0.0000248 ## 2 1880 Abe 50 201484 0.000248 ## 3 1880 Adelbert 28 201484 0.000139 ## 4 1880 Adella 26 201484 0.000129 ## 5 1880 Agustus 5 201484 0.0000248 ## 6 1880 Albert 1493 201484 0.00741 ## 7 1880 Albertus 5 201484 0.0000248 ## 8 1880 Alcide 7 201484 0.0000347 ## 9 1880 Alonzo 122 201484 0.000606 ## 10 1880 Amos 128 201484 0.000635 ## # ... with 97,300 more rows Notice that the results are grouped by year, then name, so the first few entries are names that were most popular in the 1880s that start with the letter A. 4.4.7 Adding the total and maximum for each name In the video, you learned how you could group by the year and use mutate() to add a total for that year. In these exercises, youll learn to normalize by a different, but also interesting metric: youll divide each name by the maximum for that name. This means that every name will peak at 1. Once you add new columns, the result will still be grouped by name. This splits it into 48,000 groups, which actually makes later steps like mutate slower. babynames %&gt;% group_by(name) %&gt;% mutate(name_total = sum(number), name_max = max(number)) %&gt;% # Ungroup the table ungroup() %&gt;% # Add the fraction_max column containing the number by the name maximum mutate(fraction_max = number / name_max) ## # A tibble: 1,756,284 x 6 ## year name number name_total name_max fraction_max ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 Aaron 102 579589 15411 0.00662 ## 2 1880 Ab 5 362 41 0.122 ## 3 1880 Abbie 71 21716 536 0.132 ## 4 1880 Abbott 5 1020 59 0.0847 ## 5 1880 Abby 6 57756 2048 0.00293 ## 6 1880 Abe 50 9158 280 0.179 ## 7 1880 Abel 9 50236 3245 0.00277 ## 8 1880 Abigail 12 357031 15948 0.000752 ## 9 1880 Abner 27 7641 202 0.134 ## 10 1880 Abraham 81 88852 2575 0.0315 ## # ... with 1,756,274 more rows This tells you, for example, that the name Abe was at 18.5% of its peak in the year 1880. 4.4.8 Visualizing the normalized change in popularity You picked a few names and calculated each of them as a fraction of their peak. This is a type of normalizing a name, where youre focused on the relative change within each name rather than the overall popularity of the name. In this exercise, youll visualize the normalized popularity of each name. Your work from the previous exercise, names_normalized, has been provided for you. names_normalized &lt;- babynames %&gt;% group_by(name) %&gt;% mutate(name_total = sum(number), name_max = max(number)) %&gt;% ungroup() %&gt;% mutate(fraction_max = number / name_max) # Filter for the names Steven, Thomas, and Matthew names_filtered &lt;- names_normalized %&gt;% filter(name %in% c(&quot;Steven&quot;, &quot;Thomas&quot;, &quot;Matthew&quot;)) # Visualize these names over time ggplot(names_filtered, aes(x = year, y = fraction_max, color = name)) + geom_line() As you can see, the line for each name hits a peak at 1, although the peak year differs for each name. 4.4.9 Video: Window functions The code shown at the end of the video (finding the variable difference for all names) contains a mistake. Can you spot it? Below is the corrected code. # As we did before, but now naming it babynames_fraction babynames_fraction &lt;- babynames %&gt;% group_by(year) %&gt;% mutate(year_total = sum(number)) %&gt;% ungroup() %&gt;% mutate(fraction = number / year_total) # Just for Matthew babynames_fraction %&gt;% filter(name == &quot;Matthew&quot;) %&gt;% arrange(year) %&gt;% # Display change in prevalence from year to year mutate(difference = fraction - lag(fraction)) %&gt;% # Arange in descending order arrange(desc(difference)) ## # A tibble: 138 x 6 ## year name number year_total fraction difference ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981 Matthew 43531 3459182 0.0126 0.00154 ## 2 1983 Matthew 50531 3462826 0.0146 0.00138 ## 3 1971 Matthew 22653 3432585 0.00660 0.000982 ## 4 1967 Matthew 13629 3395130 0.00401 0.000894 ## 5 1973 Matthew 24658 3017412 0.00817 0.000835 ## 6 1974 Matthew 27332 3040409 0.00899 0.000818 ## 7 1978 Matthew 34468 3174268 0.0109 0.000741 ## 8 1972 Matthew 23066 3143627 0.00734 0.000738 ## 9 1968 Matthew 15915 3378876 0.00471 0.000696 ## 10 1982 Matthew 46333 3507664 0.0132 0.000625 ## # ... with 128 more rows # For all names babynames_fraction %&gt;% group_by(name) %&gt;% # Display change in prevalence from year to year mutate(difference = fraction - lag(fraction)) %&gt;% # Arange in descending order arrange(name, year) ## # A tibble: 1,756,284 x 6 ## # Groups: name [97,310] ## year name number year_total fraction difference ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2007 Aaban 5 3994007 0.00000125 NA ## 2 2009 Aaban 6 3815638 0.00000157 3.21e-7 ## 3 2010 Aaban 9 3690700 0.00000244 8.66e-7 ## 4 2011 Aaban 11 3651914 0.00000301 5.74e-7 ## 5 2012 Aaban 11 3650462 0.00000301 1.20e-9 ## 6 2013 Aaban 14 3637310 0.00000385 8.36e-7 ## 7 2014 Aaban 16 3696311 0.00000433 4.80e-7 ## 8 2015 Aaban 15 3688687 0.00000407 -2.62e-7 ## 9 2016 Aaban 9 3652968 0.00000246 -1.60e-6 ## 10 2017 Aaban 11 3546301 0.00000310 6.38e-7 ## # ... with 1,756,274 more rows 4.4.10 Using ratios to describe the frequency of a name In the video, you learned how to find the difference in the frequency of a baby name between consecutive years. What if instead of finding the difference, you wanted to find the ratio? Youll start with the babynames_fraction data already, so that you can consider the popularity of each name within each year. babynames_ratio &lt;- babynames_fraction %&gt;% # Arrange the data in order of name, then year arrange(name, year) %&gt;% # Group the data by name group_by(name) %&gt;% # Add a ratio column that contains the ratio between each year mutate(ratio = fraction / lag(fraction)) Notice that the first observation for each name is missing a ratio, since there is no previous year. 4.4.11 Biggest jumps in a name Previously, you added a ratio column to describe the ratio of the frequency of a baby name between consecutive years to describe the changes in the popularity of a name. Now, youll look at a subset of that data, called babynames_ratios_filtered, to look further into the names that experienced the biggest jumps in popularity in consecutive years. babynames_ratios_filtered &lt;- babynames_ratio %&gt;% filter(fraction &gt;= 0.00001) babynames_ratios_filtered %&gt;% # Extract the largest ratio from each name top_n(1, ratio) %&gt;% # Sort the ratio column in descending order arrange(desc(ratio)) %&gt;% # Filter for fractions greater than or equal to 0.001 filter(fraction &gt;= 0.001) ## # A tibble: 155 x 6 ## # Groups: name [155] ## year name number year_total fraction ratio ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1957 Tammy 4398 4200007 0.00105 16.3 ## 2 1912 Woodrow 1854 988064 0.00188 9.99 ## 3 1931 Marlene 2599 2104071 0.00124 8.97 ## 4 1898 Dewey 1219 381458 0.00320 6.48 ## 5 2010 Bentley 4001 3690700 0.00108 6.21 ## 6 1884 Grover 809 243462 0.00332 5.68 ## 7 1984 Jenna 5898 3487820 0.00169 5.01 ## 8 1991 Mariah 5200 3894329 0.00134 4.78 ## 9 1943 Cheryl 2894 2822127 0.00103 4.75 ## 10 1989 Ethan 4067 3843559 0.00106 4.37 ## # ... with 145 more rows Some of these can be interpreted: for example, Grover Cleveland was a president elected in 1884. 4.4.12 Video: Contratulations! Youll find all of these skills valuable is these other DataCamp courses: Exploratory Data Analysis in R: Case Study Working with Data in the Tidyverse Machine Learning in the Tidyverse Categorical Data in the Tidyverse 4.5 Challenge Hey 4.6 Solutions sw &lt;- dplyr::starwars # Create sw dataframe head(sw) ## # A tibble: 6 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke~ 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl~ red 33 &lt;NA&gt; ## 4 Dart~ 202 136 none white yellow 41.9 male ## 5 Leia~ 150 49 brown light brown 19 female ## 6 Owen~ 178 120 brown, gr~ light blue 52 male ## # ... with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; 4.6.1 Question 1 dim(sw) # 1.a. ## [1] 87 13 dim.sw &lt;- nrow(sw)*ncol(sw) # 1.b. 4.6.2 Question 2 na.cells &lt;- sum(is.na(sw)) # 2.a. round(100*na.cells/dim.sw,1) # 2.b. ## [1] 8.9 colnames(sw[,colSums(is.na(sw)) == max(colSums(is.na(sw)))]) # 2.c. ## [1] &quot;birth_year&quot; filter(sw, is.na(homeworld)==TRUE &amp; (is.na(birth_year)==FALSE | is.na(mass)==FALSE)) # 2.d. ## # A tibble: 3 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Yoda 66 17 white green brown 896 male ## 2 IG-88 200 140 none metal red 15 none ## 3 Qui-~ 193 89 brown fair blue 92 male ## # ... with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; 4.6.3 Question 3 length(unique(sw$species)) # 3. ## [1] 38 4.6.4 Question 4 length(which(sw$species == &quot;Human&quot;)) # 4.a.i ## [1] 35 sw %&gt;% # 4.a.ii. filter(species==&quot;Human&quot;) %&gt;% group_by(gender) %&gt;% count() ## # A tibble: 2 x 2 ## # Groups: gender [2] ## gender n ## &lt;chr&gt; &lt;int&gt; ## 1 female 9 ## 2 male 26 filter(sw, is.na(gender) | (gender != &quot;male&quot; &amp; gender != &quot;female&quot;)) %&gt;% # 4.b. count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 6 sw %&gt;% # 4.c. group_by(species, gender) %&gt;% # Creates piles of similarly sexed members of the same species count() %&gt;% # Produces a table of tallies of the above groups group_by(species) %&gt;% # Groups within this table of tallies count() %&gt;% # Produces a tally of the numbers of members of the groups above filter(n &gt; 1) ## # A tibble: 5 x 2 ## # Groups: species [5] ## species n ## &lt;chr&gt; &lt;int&gt; ## 1 Droid 2 ## 2 Human 2 ## 3 Kaminoan 2 ## 4 Twi&#39;lek 2 ## 5 &lt;NA&gt; 2 4.6.5 Question 5 tb &lt;- sw %&gt;% # 5.a. group_by(gender) %&gt;% count() my_cols &lt;- unikn::usecol(c(Pinky, Seegruen, Seeblau, Grau)) ggplot(sw, aes(x = gender)) + # 5.b. geom_bar(aes(fill = gender)) + labs(title = &quot;Sex distribtion of the sw universe&quot;, tag = &quot;5.b.&quot;, x = &quot;Gender&quot;, y = &quot;Frequency&quot;, caption = &quot;Using raw data sw.&quot;) + scale_fill_manual(name = &quot;Gender:&quot;, values = my_cols, na.value = &quot;black&quot;) + ds4psy::theme_ds4psy() ggplot(tb, aes(x = gender, y = n)) + # 5.c., adding y=n to the aes() geom_bar(aes(fill = gender), stat = &quot;identity&quot;) + # Adding stat=&quot;identity&quot; to the aes() labs(title = &quot;Sex distribtion of the sw universe&quot;, tag = &quot;5.c.&quot;, x = &quot;Gender&quot;, y = &quot;Frequency&quot;, caption = &quot;Using raw data sw.&quot;) + scale_fill_manual(name = &quot;Gender:&quot;, values = my_cols, na.value = &quot;black&quot;) + ds4psy::theme_ds4psy() 4.6.6 Question 6 sw %&gt;% # 6.a. group_by(homeworld) %&gt;% count() %&gt;% arrange(desc(n)) ## # A tibble: 49 x 2 ## # Groups: homeworld [49] ## homeworld n ## &lt;chr&gt; &lt;int&gt; ## 1 Naboo 11 ## 2 Tatooine 10 ## 3 &lt;NA&gt; 10 ## 4 Alderaan 3 ## 5 Coruscant 3 ## 6 Kamino 3 ## 7 Corellia 2 ## 8 Kashyyyk 2 ## 9 Mirial 2 ## 10 Ryloth 2 ## # ... with 39 more rows sw %&gt;% # 6.b. filter(homeworld == &quot;Naboo&quot;, eye_color == &quot;orange&quot;) %&gt;% summarise(n = n(), mn_height =mean(height)) ## # A tibble: 1 x 2 ## n mn_height ## &lt;int&gt; &lt;dbl&gt; ## 1 3 209. 4.6.7 Question 7 sw %&gt;% # 7. filter(species == &quot;Droid&quot;) %&gt;% summarise(n = n(), not_NA_h = sum(!is.na(height)), md_height = median(height, na.rm=TRUE), mn_height = mean(height, na.rm=TRUE), sd_height = sd(height, na.rm=TRUE)) ## # A tibble: 1 x 5 ## n not_NA_h md_height mn_height sd_height ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 4 132 140 52.0 4.6.8 Question 8 h_m &lt;- sw %&gt;% # 8.a. group_by(species) %&gt;% summarise(mn_height = mean(height, na.rm=TRUE), mn_mass = mean(mass, na.rm=TRUE)) h_m %&gt;% # 8.b. arrange(mn_height) %&gt;% slice(1:3) ## # A tibble: 3 x 3 ## species mn_height mn_mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Yoda&#39;s species 66 17 ## 2 Aleena 79 15 ## 3 Ewok 88 20 h_m %&gt;% # 8.c. arrange(desc(mn_mass)) %&gt;% slice(1:3) ## # A tibble: 3 x 3 ## species mn_height mn_mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hutt 175 1358 ## 2 Kaleesh 216 159 ## 3 Wookiee 231 124 4.6.9 Question 9 sw %&gt;% # 9.a. filter(!is.na(species)) %&gt;% group_by(species) %&gt;% summarise(n = n()) %&gt;% arrange(desc(n)) %&gt;% slice(1:3) ## # A tibble: 3 x 2 ## species n ## &lt;chr&gt; &lt;int&gt; ## 1 Human 35 ## 2 Droid 5 ## 3 Gungan 3 sw %&gt;% # 9.b. select(name, homeworld, mass) %&gt;% group_by(homeworld) %&gt;% mutate(mn_mass = mean(mass, na.rm = TRUE), lighter = mass &lt; (mn_mass*0.8) ) %&gt;% filter(lighter == TRUE) ## # A tibble: 5 x 5 ## # Groups: homeworld [4] ## name homeworld mass mn_mass lighter ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 R2-D2 Naboo 32 64.2 TRUE ## 2 Leia Organa Alderaan 49 64 TRUE ## 3 R5-D4 Tatooine 32 85.4 TRUE ## 4 Yoda &lt;NA&gt; 17 82 TRUE ## 5 Padm Amidala Naboo 45 64.2 TRUE "],["cleaning-data-in-r.html", "Module 5 Cleaning Data in R 5.1 1: Common Data Problems 5.2 2: Categorical and Text Data 5.3 3: Advanced Data Problems 5.4 4: Record Linkage", " Module 5 Cleaning Data in R Its commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions. In this course, youll learn how to clean dirty data. Using R, youll learn how to identify values that dont look right and fix dirty data by converting data types, filling in missing values, and using fuzzy string matching. As you learn, youll brush up on your skills by working with real-world datasets, including bike-share trips, customer asset portfolios, and restaurant reviewsdeveloping the skills you need to go from raw data to awesome insights as quickly and accurately as possible! 5.1 1: Common Data Problems In this chapter, youll learn how to overcome some of the most common dirty data problems. Youll convert data types, apply range constraints to remove future data points, and remove duplicated data points to avoid double-counting. 5.1.1 Video: Data type constraints Errors can be introduced by typos and misspellings. Dirty data can the data science workflow before we even access the data, and if we dont address these errors early on, they can follow us all the way through the workflow. Errors appearing throughout the data science workflow. Source: DataCamp A quick reminder of data type constraints: Data types vs. data types in R. Source: DataCamp 5.1.2 Question: Common data types Solution. Source: DataCamp Correctly identifying what type your data is is one of the easiest ways to avoid hampering your analysis due to data type constraints in the long run. 5.1.3 Converting data types Throughout this chapter, youll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information. Before beginning to analyze any dataset, its important to take a look at the different types of columns youll be working with, which you can do using glimpse(). In this exercise, youll take a look at the data types contained in bike_share_rides and see how an incorrect data type can flaw your analysis. dplyr was previously loaded, so we just load assertive. bike_share_rides is not available so code requiring it will not be loaded. # Load assertive package library(assertive) # Glimpse at bike_share_rides glimpse(bike_share_rides) # Summary of user_birth_year summary(bike_share_rides$user_birth_year) Question The summary statistics of user_birth_year dont seem to offer much useful information about the different birth years in our dataset. Why do you think that is? The user_birth_year column is not of the correct type and should be converted to a character. The user_birth_year column has an infinite set of possible values and should be converted to a factor. The user_birth_year column represents groupings of data and should be converted to a factor. [Correct] # Convert user_birth_year to factor: user_birth_year_fct bike_share_rides &lt;- bike_share_rides %&gt;% mutate(user_birth_year_fct = as.factor(user_birth_year)) # Assert user_birth_year_fct is a factor assert_is_factor(bike_share_rides$user_birth_year_fct) # Summary of user_birth_year_fct summary(bike_share_rides$user_birth_year_fct) Looking at the new summary statistics, more riders were born in 1988 than any other year. 5.1.4 Trimming strings In the previous exercise, you were able to identify the correct data type and convert user_birth_year to the correct type, allowing you to extract counts that gave you a bit more insight into the dataset. Another common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as characters. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from character to numeric. In this exercise, youll need to convert the duration column from character to numeric, but before this can happen, the word \"minutes\" needs to be removed from each value. # Load stringr package library(stringr) ## Warning: package &#39;stringr&#39; was built under R version 3.5.3 bike_share_rides &lt;- bike_share_rides %&gt;% # Remove &#39;minutes&#39; from duration: duration_trimmed mutate(duration_trimmed = str_remove(duration, &quot;minutes&quot;), # Convert duration_trimmed to numeric: duration_mins duration_mins = as.numeric(duration_trimmed)) # Glimpse at bike_share_rides glimpse(bike_share_rides) # Assert duration_mins is numeric assert_is_numeric(bike_share_rides$duration_mins) # Calculate mean duration mean(bike_share_rides$duration_mins) By removing characters and converting to a numeric type, you were able to figure out that the average ride duration is about 13 minutes - not bad for a city like San Francisco! 5.1.5 Video: Range constraints What to do about about values outside of a variables allowed range? We could remove these rows from the data frame, but this should only be done when a small proportion of the values are out of range; otherwise we would significantly increase the amount of bias in our dataset. Treat these values as missing (NA). Replace them with the range limit, e.g. replace a rating of 6 on a 1 to 5 rating scale with 5. Replace with other value based on domain knowledge and/or knowledge of dataset, e.g. replace with average rating. 5.1.6 Ride duration constraints Values that are out of range can throw off an analysis, so its important to catch them early on. In this exercise, youll be examining the duration_min column more closely. Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned. In this exercise, youll replace erroneous data with the range limit (1440 minutes), however, you could just as easily replace these values with NAs. # Create breaks breaks &lt;- c(min(bike_share_rides$duration_min), 0, 1440, max(bike_share_rides$duration_min)) # Create a histogram of duration_min ggplot(bike_share_rides, aes(duration_min)) + geom_histogram(breaks = breaks) # duration_min_const: replace vals of duration_min &gt; 1440 with 1440 bike_share_rides &lt;- bike_share_rides %&gt;% mutate(duration_min_const = replace(duration_min, duration_min &gt; 1440, 1440)) # Make sure all values of duration_min_const are between 0 and 1440 assert_all_are_in_closed_range(bike_share_rides$duration_min_const, lower = 0, upper = 1440) The method of replacing erroneous data with the range limit works well, but you could just as easily replace these values with NAs or something else instead. 5.1.7 Back to the future Something has gone wrong and it looks like you have data with dates from the future, which is way outside of the date range you expected to be working with. To fix this, youll need to remove any rides from the dataset that have a date in the future. Before you can do this, the date column needs to be converted from a character to a Date. Having these as Date objects will make it much easier to figure out which rides are from the future, since R makes it easy to check if one Date object is before (&lt;) or after (&gt;) another. # Convert date to Date type bike_share_rides &lt;- bike_share_rides %&gt;% mutate(date = as.Date(date)) # Make sure all dates are in the past assert_all_are_in_past(bike_share_rides$date) # Filter for rides that occurred before or on today&#39;s date bike_share_rides_past &lt;- bike_share_rides %&gt;% filter(date &lt;= today()) # Make sure all dates from bike_share_rides_past are in the past assert_all_are_in_past(bike_share_rides_past$date) Handling data from the future like this is much easier than trying to verify the datas correctness by time traveling. 5.1.8 Video: Uniqueness constraints Duplicate entries. Source: DataCamp Source of duplicates. Source: DataCamp Full duplicates: Two rows have the exact same entries in every column. Partial duplicates: Two rows have the exact same entries in some columns. 5.1.9 Full duplicates Youve been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, youll need to ensure that any duplicates in the dataset are removed first. When multiple rows of a data frame share the same values for all columns, theyre full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique. # Count the number of full duplicates sum(duplicated(bike_share_rides)) # Remove duplicates bike_share_rides_unique &lt;- distinct(bike_share_rides) # Count the full duplicates in bike_share_rides_unique sum(duplicated(bike_share_rides_unique)) Removing full duplicates will ensure that summary statistics arent altered by repeated data points. 5.1.10 Removing partial duplicates Now that youve identified and removed the full duplicates, its time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, youll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first. # Find duplicated ride_ids bike_share_rides %&gt;% # Count the number of occurrences of each ride_id count(ride_id) %&gt;% # Filter for rows with a count &gt; 1 filter(n &gt; 1) # Remove full and partial duplicates bike_share_rides_unique &lt;- bike_share_rides %&gt;% # Only based on ride_id instead of all cols distinct(ride_id, .keep_all = TRUE) # Find duplicated ride_ids in bike_share_rides_unique bike_share_rides_unique %&gt;% # Count the number of occurrences of each ride_id count(ride_id) %&gt;% # Filter for rows with a count &gt; 1 filter(n &gt; 1) Its important to consider the data youre working with before removing partial duplicates, since sometimes its expected that there will be partial duplicates in a dataset, such as if the same customer makes multiple purchases. 5.1.11 Aggregating partial duplicates Another way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when youre not sure how your data was collected and want an average, or if based on domain knowledge, youd rather have too high of an estimate than too low of an estimate (or vice versa). bike_share_rides %&gt;% # Group by ride_id and date group_by(ride_id, date) %&gt;% # Add duration_min_avg column mutate(duration_min_avg = mean(duration_min) ) %&gt;% # Remove duplicates based on ride_id and date, keep all cols distinct(ride_id, date, .keep_all = TRUE) %&gt;% # Remove duration_min column select(-duration_min) Aggregation of partial duplicates allows you to keep some information about all data points instead of keeping information about just one data point. 5.2 2: Categorical and Text Data Categorical and text data can often be some of the messiest parts of a dataset due to their unstructured nature. In this chapter, youll learn how to fix whitespace and capitalization inconsistencies in category labels, collapse multiple categories into one, and reformat strings for consistency. 5.2.1 Video: Checking membership In R, categories are stored as factors in R. They are stored as numbers, and each number has a corresponding label. Factors. Source: DataCamp How many levels do the categorical variables above have? Values that dont belong. Source: DataCamp How do we end up with values outside those allowed by our factors? Source: DataCamp Filtering joins are a type of join thaat keeps or removes observations from the first table, but doesnt add any new columns. Semi-joins vs. anti-joins. Source: DataCamp Values that arent factors. Source: DataCamp 5.2.2 Question: Members only So far in the course, youve learned about a number of different problems you can run into when you have dirty data, including data type constraints, range constraints, uniqueness constraints, and membership constraints. Its important to be able to correctly identify the type of problem youre dealing with so that you can treat it correctly. In this exercise, youll practice identifying these problems by mapping dirty data scenarios to their constraint type. Solution. Source: DataCamp Being able to identify what kinds of errors are in your data is important so that you know how to go about fixing them. 5.2.3 Not a member Now that youve practiced identifying membership constraint problems, its time to fix these problems in a new dataset. Throughout this chapter, youll be working with a dataset called sfo_survey, containing survey responses from passengers taking flights from San Francisco International Airport (SFO). Participants were asked questions about the airports cleanliness, wait times, safety, and their overall satisfaction. There were a few issues during data collection that resulted in some inconsistencies in the dataset. In this exercise, youll be working with the dest_size column, which categorizes the size of the destination airport that the passengers were flying to. A data frame called dest_sizes is available that contains all the possible destination sizes. Your mission is to find rows with invalid dest_sizes and remove them from the data frame. # Count the number of occurrences of dest_size sfo_survey %&gt;% count(dest_size) # Find bad dest_size rows sfo_survey %&gt;% # Join with dest_sizes data frame to get bad dest_size rows anti_join(dest_sizes, by = &quot;dest_size&quot;) %&gt;% # Select id, airline, destination, and dest_size cols select(id, airline, destination, dest_size) # Remove bad dest_size rows sfo_survey %&gt;% # Join with dest_sizes semi_join(dest_sizes, by = &quot;dest_size&quot;) %&gt;% # Count the number of each dest_size count(dest_size) Anti-joins can help you identify the rows that are causing issues, and semi-joins can remove the issue-causing rows. In the next lesson, youll learn about other ways to deal with bad values so that you dont have to lose rows of data. 5.2.4 Video: Categorical data problems Categorical data problems Source: DataCamp 5.2.5 Identifying inconsistency In the video exercise, you learned about different kinds of inconsistencies that can occur within categories, making it look like a variable has more categories than it should. In this exercise, youll continue working with the sfo_survey dataset. Youll examine the dest_size column again as well as the cleanliness column and determine what kind of issues, if any, these two categorical variables face. # Count dest_size sfo_survey %&gt;% count(dest_size) # Count cleanliness sfo_survey %&gt;% count(cleanliness) In the next exercise, youll fix these inconsistencies to get more accurate counts. 5.2.6 Correcting inconsistency Now that youve identified that dest_size has whitespace inconsistencies and cleanliness has capitalization inconsistencies, youll use the new tools at your disposal to fix the inconsistent values in sfo_survey instead of removing the data points entirely, which could add bias to your dataset if more than 5% of the data points need to be dropped. # Add new columns to sfo_survey sfo_survey &lt;- sfo_survey %&gt;% # dest_size_trimmed: dest_size without whitespace mutate(dest_size_trimmed = str_trim(dest_size), # cleanliness_lower: cleanliness converted to lowercase cleanliness_lower = str_to_lower(cleanliness)) # Count values of dest_size_trimmed sfo_survey %&gt;% count(dest_size_trimmed) # Count values of cleanliness_lower sfo_survey %&gt;% count(cleanliness_lower) You were able to convert seven-category data into four-category data, which will help your analysis go more smoothly. 5.2.7 Collapsing categories One of the tablets that participants filled out the sfo_survey on was not properly configured, allowing the response for dest_region to be free text instead of a dropdown menu. This resulted in some inconsistencies in the dest_region variable that youll need to correct in this exercise to ensure that the numbers you report to your boss are as accurate as possible. # Count categories of dest_region sfo_survey %&gt;% count(dest_region) # Categories to map to Europe europe_categories &lt;- c(&quot;EU&quot;, &quot;eur&quot;, &quot;Europ&quot;) # Add a new col dest_region_collapsed sfo_survey %&gt;% # Map all categories in europe_categories to Europe mutate(dest_region_collapsed = fct_collapse(dest_region, Europe = europe_categories)) %&gt;% # Count categories of dest_region_collapsed count(dest_region_collapsed) Youve reduced the number of categories from 12 to 9, and you can now be confident that 401 of the survey participants were heading to Europe. 5.2.8 Video: Cleaning text data Text data. Source: DataCamp Unstructured data problems. Source: DataCamp More complex text problems. Source: DataCamp 5.2.9 Detecting inconsistent text data Youve recently received some news that the customer support team wants to ask the SFO survey participants some follow-up questions. However, the auto-dialer that the call center uses isnt able to parse all of the phone numbers since theyre all in different formats. After some investigation, you found that some phone numbers are written with hyphens (-) and some are written with parentheses ((,)). In this exercise, youll figure out which phone numbers have these issues so that you know which ones need fixing. # Filter for rows with &quot;-&quot; in the phone column sfo_survey %&gt;% filter(str_detect(phone, &quot;-&quot;)) # Filter for rows with &quot;(&quot; or &quot;)&quot; in the phone column sfo_survey %&gt;% filter(str_detect(phone, fixed(&quot;(&quot;)) | str_detect(phone, fixed(&quot;)&quot;))) Now that youve identified the inconsistencies in the phone column, its time to remove unnecessary characters to make the follow-up survey go as smoothly as possible. 5.2.10 Replacing and removing In the last exercise, you saw that the phone column of sfo_data is plagued with unnecessary parentheses and hyphens. The customer support team has requested that all phone numbers be in the format 123 456 7890. In this exercise, youll use your new stringr skills to fulfill this request. # Remove parentheses from phone column phone_no_parens &lt;- sfo_survey$phone %&gt;% # Remove &quot;(&quot;s str_remove_all(fixed(&quot;(&quot;)) %&gt;% # Remove &quot;)&quot;s str_remove_all(fixed(&quot;)&quot;)) # Add phone_no_parens as column sfo_survey %&gt;% mutate(sfo_survey, phone_no_parens) # Add phone_no_parens as column sfo_survey %&gt;% mutate(phone_no_parens = phone_no_parens, # Replace all hyphens in phone_no_parens with spaces phone_clean = str_replace_all(phone_no_parens, &quot;-&quot;, &quot; &quot;)) Now that your phone numbers are all in a single format, the machines in the call center will be able to auto-dial the numbers, making it easier to ask participants follow-up questions. 5.2.11 Invalid phone numbers The customer support team is grateful for your work so far, but during their first day of calling participants, they ran into some phone numbers that were invalid. In this exercise, youll remove any rows with invalid phone numbers so that these faulty numbers dont keep slowing the team down. # Check out the invalid numbers sfo_survey %&gt;% filter(str_length(phone) != 12) # Remove rows with invalid numbers sfo_survey %&gt;% filter(str_length(phone) == 12) Thanks to your savvy string skills, the follow-up survey will be done in no time! 5.3 3: Advanced Data Problems In this chapter, youll dive into more advanced data cleaning problems, such as ensuring that weights are all written in kilograms instead of pounds. Youll also gain invaluable skills that will help you verify that values have been added correctly and that missing values dont negatively impact your analyses. Uniformity. Source: DataCamp Sources of uniformity issues. Source: DataCamp What to do about a lack of uniformity? Source: DataCamp Date uniformity. Source: DataCamp Ambiguous dates. Source: DataCamp 5.3.1 Date uniformity In this chapter, you work at an asset management company and youll be working with the accounts dataset, which contains information about each customer, the amount in their account, and the date their account was opened. Your boss has asked you to calculate some summary statistics about the average value of each account and whether the age of the account is associated with a higher or lower account value. Before you can do this, you need to make sure that the accounts dataset youve been given doesnt contain any uniformity problems. In this exercise, youll investigate the date_opened column and clean it up so that all the dates are in the same format. # Check out the accounts data frame head(accounts) # Define the date formats formats &lt;- c(&quot;%Y-%m-%d&quot;, &quot;%B %d, %Y&quot;) # Convert dates to the same format accounts %&gt;% mutate(date_opened_clean = parse_date_time(date_opened, orders = formats)) Now that the date_opened dates are in the same format, youll be able to use them for some plotting in the next exercise. 5.3.2 Currency uniformity Now that your dates are in order, youll need to correct any unit differences. When you first plot the data, youll notice that theres a group of very high values, and a group of relatively lower values. The bank has two different offices - one in New York, and one in Tokyo, so you suspect that the accounts managed by the Tokyo office are in Japanese yen instead of U.S. dollars. Luckily, you have a data frame called account_offices that indicates which office manages each customers account, so you can use this information to figure out which totals need to be converted from yen to dollars. The formula to convert yen to dollars is USD = JPY / 104. # Scatter plot of opening date and total amount accounts %&gt;% ggplot(aes(x = date_opened, y = total)) + geom_point() # Left join accounts and account_offices by id accounts %&gt;% left_join(account_offices, by=&quot;id&quot;) %&gt;% # Convert totals from the Tokyo office to USD mutate(total_usd = ifelse(office == &quot;Tokyo&quot;, total / 104, total)) %&gt;% # Scatter plot of opening date vs total_usd ggplot(aes(x = date_opened, y = total_usd)) + geom_point() The points in your last scatter plot all fall within a much smaller range now and youll be able to accurately assess the differences between accounts from different countries. 5.3.3 Video: Cross field validation Cross field validation Does this value make sense based on other values? Link: https://www.buzzfeednews.com/article/katienotopoulos/graphs-that-lied-to-us What to do with nonsense data. Source: DataCamp (Note, impute means to assign (a value) to something by inference from the value of the products or processes to which it contributes.) 5.3.4 Validating totals In this lesson, youll continue to work with the accounts data frame, but this time, you have a bit more information about each account. There are three different funds that account holders can store their money in. In this exercise, youll validate whether the total amount in each account is equal to the sum of the amount in fund_A, fund_B, and fund_C. If there are any accounts that dont match up, you can look into them further to see what went wrong in the bookkeeping that led to inconsistencies. # Find invalid totals accounts %&gt;% # theoretical_total: sum of the three funds mutate(theoretical_total = fund_A + fund_B + fund_C) %&gt;% # Find accounts where total doesn&#39;t match theoretical_total filter(theoretical_total != total) By using cross field validation, youve been able to detect values that dont make sense. How you choose to handle these values will depend on the dataset. 5.3.5 Validating age Now that you found some inconsistencies in the total amounts, youre suspicious that there may also be inconsistencies in the acct_agecolumn, and you want to see if these inconsistencies are related. Using the skills you learned from the video exercise, youll need to validate the age of each account and see if rows with inconsistent acct_ages are the same ones that had inconsistent totals. # Find invalid acct_age accounts %&gt;% # theoretical_age: age of acct based on date_opened mutate(theoretical_age = floor(as.numeric(date_opened %--% today(), &quot;years&quot;))) %&gt;% # Filter for rows where acct_age is different from theoretical_age filter(theoretical_age != acct_age) There are three accounts that all have ages off by one year, but none of them are the same as the accounts that had total inconsistencies, so it looks like these two bookkeeping errors may not be related. 5.3.6 Video: Completeness What is missing data. Source: DataCamp Types of missingness. Source: DataCamp Dealing with missing data. Source: DataCamp 5.3.7 Question: Types of missingness You just learned about the three flavors of missing data: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). In this exercise, youll solidify your new knowledge by mapping examples to the types of missingness. Question types of missingness. Source: DataCamp 5.3.8 Visualizing missing data Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data. You just received a new version of the accounts data frame containing data on the amount held and amount invested for new and existing customers. However, there are rows with missing inv_amount values. You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. # Visualize the missing values by column vis_miss(accounts) accounts %&gt;% # missing_inv: Is inv_amount missing? mutate(missing_inv = is.na(inv_amount)) %&gt;% # Group by missing_inv group_by(missing_inv) %&gt;% # Calculate mean age for each missing_inv group summarize(avg_age = mean(age, na.rm=TRUE)) Since the average age for missing_inv = TRUE is 22 and the average age for missing_inv = FALSE is 44, it is likely that the inv_amount variable is missing mostly in young customers. # Sort by age and visualize missing vals accounts %&gt;% arrange(age) %&gt;% vis_miss() Investigating summary statistics based on missingness is a great way to determine if data is missing completely at random or missing at random. 5.3.9 Treating missing data In this exercise, youre working with another version of the accounts data that contains missing values for both the cust_id and acct_amount columns. You want to figure out how many unique customers the bank has, as well as the average amount held by customers. You know that rows with missing cust_id dont really help you, and that on average, the acct_amount is usually 5 times the amount of inv_amount. In this exercise, you will drop rows of accounts with missing cust_ids, and impute missing values of inv_amount with some domain knowledge. Well need to install and load the assertive package. # Create accounts_clean accounts_clean &lt;- accounts %&gt;% # Filter to remove rows with missing cust_id filter(!is.na(cust_id)) %&gt;% # Add new col acct_amount_filled with replaced NAs mutate(acct_amount_filled = ifelse(is.na(acct_amount), inv_amount * 5, acct_amount)) # Assert that cust_id has no missing vals assert_all_are_not_na(accounts_clean$cust_id) # or sum(is.na(accounts_clean$cust_id)) # Assert that acct_amount_filled has no missing vals assert_all_are_not_na(accounts_clean$acct_amount_filled) Since your assertions passed, theres no missing data left, and you can definitely bank on nailing your analysis! 5.4 4: Record Linkage Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, youll learn how to link records by calculating the similarity between stringsyoull then use your new skills to join two restaurant review datasets into one clean master dataset. Minimum edit distance. Source: DataCamp Edit distance = 1. Source: DataCamp Edit distance = 4. Source: DataCamp Types of edit distance. Source: DataCamp Comparing strings to clean data. Source: DataCamp 5.4.1 Calculating distance In the video exercise, you saw how to use Damerau-Levenshtein distance to identify how similar two strings are. As a reminder, Damerau-Levenshtein distance is the minimum number of steps needed to get from String A to String B, using these operations: Insertion of a new character. Deletion of an existing character. Substitution of an existing character. Transposition of two existing consecutive characters. Substituting and inserting is the best way to get from \"puffin\" to \"muffins\". In the next exercise, youll calculate string distances using R functions. 5.4.2 Small distance, small difference In the video exercise, you learned that there are multiple ways to calculate how similar or different two strings are. Now youll practice using the stringdist package to compute string distances using various methods. Its important to be familiar with different methods, as some methods work better on certain datasets, while others work better on other datasets. library(stringdist) ## ## Attaching package: &#39;stringdist&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract # Calculate Damerau-Levenshtein distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;dl&quot;) ## [1] 2 # Calculate LCS distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;lcs&quot;) ## [1] 4 The Jaccard method 1. Count the total number of distinct elements in the two strings. 2. Count the number of those elements which appear in both strings. 3. Divide the latter by the former and multiply by 100. # Calculate Jaccard distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;jaccard&quot;) ## [1] 0 As there are no elements in the first string that arent in the second string, and vice versa, the Jaccard method finds no difference between the two strings. In this way, the Jaccard method treats the strings more as sets of letters rather than sequences of letters. In the next exercise, youll use Damerau-Levenshtein distance to map typo-ridden cities to their true spellings. 5.4.3 Fixing typos with string distance In this chapter, one of the datasets youll be working with, zagat, is a set of restaurants in New York, Los Angeles, Atlanta, San Francisco, and Las Vegas. The data is from Zagat, a company that collects restaurant reviews, and includes the restaurant names, addresses, phone numbers, as well as other restaurant information. The city column contains the name of the city that the restaurant is located in. However, there are a number of typos throughout the column. Your task is to map each city to one of the five correctly-spelled cities contained in the cities data frame. Well need to install and load fuzzyjoin package. library(fuzzyjoin) # Count the number of each city variation zagat %&gt;% count(city) # Join zagat and cities and look at results zagat %&gt;% # Left join based on stringdist using city and city_actual cols stringdist_left_join(cities, by = c(&quot;city&quot; = &quot;city_actual&quot;)) %&gt;% # Select the name, city, and city_actual cols select(name, city, city_actual) Now that youve created consistent spelling for each city, it will be much easier to compute summary statistics by city. 5.4.4 Video: Generating and comparing pairs When joins wont work. Source: DataCamp What is record linkage? Source: DataCamp 5.4.5 Exercise: Link or join? Similar to joins, record linkage is the act of linking data from different sources regarding the same entity. But unlike joins, record linkage does not require exact matches between different pairs of data, and instead can find close matches using string similarity. This is why record linkage is effective when there are no common unique keys between the data sources you can rely upon when linking data sources such as a unique identifier. Link or join. Source: DataCamp Dont make things more complicated than they need to be: record linkage is a powerful tool, but its more complex than using a traditional join. 5.4.6 Pair blocking Zagat and Fodors are both companies that gather restaurant reviews. The zagat and fodors datasets both contain information about various restaurants, including addresses, phone numbers, and cuisine types. Some restaurants appear in both datasets, but dont necessarily have the same exact name or phone number written down. In this chapter, youll work towards figuring out which restaurants appear in both datasets. The first step towards this goal is to generate pairs of records so that you can compare them. In this exercise, youll first generate all possible pairs, and then use your newly-cleaned city column as a blocking variable. # Load reclin library(reclin) # Generate all possible pairs pair_blocking(zagat, fodors) # Generate all possible pairs pair_blocking(zagat, fodors, blocking_var = &quot;city&quot;) By using city as a blocking variable, you were able to reduce the number of pairs youll need to compare from 165,230 pairs to 40,532. 5.4.7 Comparing pairs Now that youve generated the pairs of restaurants, its time to compare them. You can easily customize how you perform your comparisons using the by and default_comparator arguments. Theres no right answer as to what each should be set to, so in this exercise, youll try a couple options out. # Generate pairs pair_blocking(zagat, fodors, blocking_var = &quot;city&quot;) %&gt;% # Compare pairs by name using lcs() compare_pairs(by =&quot;name&quot;, default_comparator = lcs()) # Generate pairs pair_blocking(zagat, fodors, blocking_var = &quot;city&quot;) %&gt;% # Compare pairs by name using lcs() compare_pairs(by = c(&quot;name&quot;, &quot;phone&quot;, &quot;addr&quot;), default_comparator = jaro_winkler()) Choosing a comparator and the columns to compare is highly dataset-dependent, so its best to try out different combinations to see which works best on the dataset youre working with. Next, youll build on your string comparison skills and learn about record linkage! 5.4.8 Video: Scoring and linking 5.4.9 Score then select or select then score? Record linkage requires a number of steps that can be difficult to keep straight. In this exercise, youll solidify your knowledge of the record linkage process so that its a breeze when you code it yourself! Question order for record linkage. Source: DataCamp 5.4.10 Putting it together During this chapter, youve cleaned up the city column of zagat using string similarity, as well as generated and compared pairs of restaurants from zagat and fodors. The end is near - all thats left to do is score and select pairs and link the data together, and youll be able to begin your analysis in no time! # Create pairs pair_blocking(zagat, fodors, blocking_var = &quot;city&quot;) %&gt;% # Compare pairs compare_pairs(by = &quot;name&quot;, default_comparator = jaro_winkler()) %&gt;% # Score pairs score_problink() %&gt;% # Select pairs select_n_to_m() %&gt;% # Link data link() Now that your two datasets are merged, you can use the data to figure out if there are certain characteristics that make a restaurant more likely to be reviewed by Zagat or Fodors. 5.4.11 Video: Congratulations! Module 5, chapter 1 summary. Source: DataCamp Module 5, chapter 2 summary. Source: DataCamp Module 5, chapter 3 summary. Source: DataCamp Module 5, chapter 4 summary. Source: DataCamp Module 5 - further courses. Source: DataCamp "],["introduction-to-data-visualization-with-ggplot2.html", "Module 6 Introduction to Data Visualization with ggplot2", " Module 6 Introduction to Data Visualization with ggplot2 "],["exploratory-data-analysis-in-r.html", "Module 7 Exploratory Data Analysis in R 7.1 Comics 7.2 Cars 7.3 Gapminder 7.4 Email 7.5 Challenge 7.6 Solutions 1: Data cleaning and summarizing with dplyr 7.7 Solutions 2: Visualization with ggplot2 7.8 Solutions 3: Tidy modeling with broom", " Module 7 Exploratory Data Analysis in R 7.1 Comics In this chapter, you will learn how to create graphical and numerical summaries of two categorical variables. url &lt;- &quot;https://assets.datacamp.com/production/course_1796/datasets/comics.csv&quot; filename &lt;- basename(url) if (!file.exists(filename)) download(url,destfile=filename) comics &lt;- read.csv(filename) # comics %&gt;% # rename( # sepal_length = Sepal.Length, # sepal_width = Sepal.Width # ) 7.1.1 Video: Exploring categorical data View slides. str(comics) ## &#39;data.frame&#39;: 23272 obs. of 11 variables: ## $ name : Factor w/ 23272 levels &quot;&#39;Spinner (Earth-616)&quot;,..: 19830 3335 22769 9647 20956 2220 17576 9347 18794 10957 ... ## $ id : Factor w/ 4 levels &quot;No Dual&quot;,&quot;Public&quot;,..: 3 2 2 2 1 2 2 2 2 2 ... ## $ align : Factor w/ 4 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Neutral&quot;,..: 2 2 3 2 2 2 2 2 3 2 ... ## $ eye : Factor w/ 26 levels &quot;Amber Eyes&quot;,&quot;Auburn Hair&quot;,..: 11 5 5 5 5 5 6 6 6 5 ... ## $ hair : Factor w/ 28 levels &quot;Auburn Hair&quot;,..: 7 27 3 3 4 14 7 7 7 4 ... ## $ gender : Factor w/ 3 levels &quot;Female&quot;,&quot;Male&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ gsm : Factor w/ 6 levels &quot;Bisexual Characters&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## $ alive : Factor w/ 2 levels &quot;Deceased Characters&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ appearances : int 4043 3360 3061 2961 2258 2255 2072 2017 1955 1934 ... ## $ first_appear: Factor w/ 1606 levels &quot;1935, October&quot;,..: 874 1278 1513 1296 1424 1432 1432 1364 1570 1432 ... ## $ publisher : Factor w/ 2 levels &quot;dc&quot;,&quot;marvel&quot;: 2 2 2 2 2 2 2 2 2 2 ... levels(comics$align) ## [1] &quot;Bad&quot; &quot;Good&quot; &quot;Neutral&quot; ## [4] &quot;Reformed Criminals&quot; levels(comics$id) ## [1] &quot;No Dual&quot; &quot;Public&quot; &quot;Secret&quot; &quot;Unknown&quot; table(comics$id, comics$align) ## ## Bad Good Neutral Reformed Criminals ## No Dual 474 647 390 0 ## Public 2172 2930 965 1 ## Secret 4493 2475 959 1 ## Unknown 7 0 2 0 ggplot(comics, aes(x=align, fill=id)) + geom_bar() ggplot(comics, aes(x=id, fill=align)) + geom_bar() 7.1.2 Question: Bar chart expectations Which one of the barcharts shows no relationship between age and flavor? In other words, which shows that pie preference is the same for both young and old? Source: DataCamp Its the first one. 7.1.3 Contingency table review In this chapter youll continue working with the comics dataset introduced in the video. This is a collection of characteristics on all of the superheroes created by Marvel and DC comics in the last 80 years. Lets start by creating a contingency table, which is a useful way to represent the total counts of observations that fall into each combination of the levels of categorical variables. # Print the first rows of the data head(comics) ## name id align eye ## 1 Spider-Man (Peter Parker) Secret Good Hazel Eyes ## 2 Captain America (Steven Rogers) Public Good Blue Eyes ## 3 Wolverine (James \\\\&quot;Logan\\\\&quot; Howlett) Public Neutral Blue Eyes ## 4 Iron Man (Anthony \\\\&quot;Tony\\\\&quot; Stark) Public Good Blue Eyes ## 5 Thor (Thor Odinson) No Dual Good Blue Eyes ## 6 Benjamin Grimm (Earth-616) Public Good Blue Eyes ## hair gender gsm alive appearances first_appear ## 1 Brown Hair Male &lt;NA&gt; Living Characters 4043 Aug-62 ## 2 White Hair Male &lt;NA&gt; Living Characters 3360 Mar-41 ## 3 Black Hair Male &lt;NA&gt; Living Characters 3061 Oct-74 ## 4 Black Hair Male &lt;NA&gt; Living Characters 2961 Mar-63 ## 5 Blond Hair Male &lt;NA&gt; Living Characters 2258 Nov-50 ## 6 No Hair Male &lt;NA&gt; Living Characters 2255 Nov-61 ## publisher ## 1 marvel ## 2 marvel ## 3 marvel ## 4 marvel ## 5 marvel ## 6 marvel # Check levels of align levels(comics$align) ## [1] &quot;Bad&quot; &quot;Good&quot; &quot;Neutral&quot; ## [4] &quot;Reformed Criminals&quot; # Check the levels of gender levels(comics$gender) ## [1] &quot;Female&quot; &quot;Male&quot; &quot;Other&quot; # Create a 2-way contingency table table(comics$align, comics$gender) ## ## Female Male Other ## Bad 1573 7561 32 ## Good 2490 4809 17 ## Neutral 836 1799 17 ## Reformed Criminals 1 2 0 7.1.4 Dropping levels The contingency table from the last exercise revealed that there are some levels that have very low counts. To simplify the analysis, it often helps to drop such levels. In R, this requires two steps: first filtering out any rows with the levels that have very low counts, then removing these levels from the factor variable with droplevels(). This is because the droplevels() function would keep levels that have just 1 or 2 counts; it only drops levels that dont exist in a dataset. # Assign contingency table to tab tab &lt;- table(comics$align, comics$gender) # Load dplyr library(dplyr) # Print tab tab ## ## Female Male Other ## Bad 1573 7561 32 ## Good 2490 4809 17 ## Neutral 836 1799 17 ## Reformed Criminals 1 2 0 # Remove align level comics_filtered &lt;- comics %&gt;% filter(align != &quot;Reformed Criminals&quot;) %&gt;% droplevels() # See the result head(comics_filtered) ## name id align eye ## 1 Spider-Man (Peter Parker) Secret Good Hazel Eyes ## 2 Captain America (Steven Rogers) Public Good Blue Eyes ## 3 Wolverine (James \\\\&quot;Logan\\\\&quot; Howlett) Public Neutral Blue Eyes ## 4 Iron Man (Anthony \\\\&quot;Tony\\\\&quot; Stark) Public Good Blue Eyes ## 5 Thor (Thor Odinson) No Dual Good Blue Eyes ## 6 Benjamin Grimm (Earth-616) Public Good Blue Eyes ## hair gender gsm alive appearances first_appear ## 1 Brown Hair Male &lt;NA&gt; Living Characters 4043 Aug-62 ## 2 White Hair Male &lt;NA&gt; Living Characters 3360 Mar-41 ## 3 Black Hair Male &lt;NA&gt; Living Characters 3061 Oct-74 ## 4 Black Hair Male &lt;NA&gt; Living Characters 2961 Mar-63 ## 5 Blond Hair Male &lt;NA&gt; Living Characters 2258 Nov-50 ## 6 No Hair Male &lt;NA&gt; Living Characters 2255 Nov-61 ## publisher ## 1 marvel ## 2 marvel ## 3 marvel ## 4 marvel ## 5 marvel ## 6 marvel # Check contingency table table(comics_filtered$align, comics_filtered$gender) ## ## Female Male Other ## Bad 1573 7561 32 ## Good 2490 4809 17 ## Neutral 836 1799 17 7.1.5 Side-by-side barcharts While a contingency table represents the counts numerically, its often more useful to represent them graphically. Here youll construct two side-by-side barcharts of the comics data. This shows that there can often be two or more options for presenting the same data. Passing the argument position = \"dodge\" to geom_bar() says that you want a side-by-side (i.e. not stacked) barchart. # Load ggplot2 library(ggplot2) # Create side-by-side barchart of gender by alignment ggplot(comics, aes(x = align, fill = gender)) + geom_bar(position = &quot;dodge&quot;) # Create side-by-side barchart of alignment by gender ggplot(comics, aes(x = gender, fill = align)) + geom_bar(position = &quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 90)) Take a moment to toggle between the resulting plots in the plotting window. 7.1.6 Question: Bar chart interpretation Which of the following interpretations of the bar charts to your right is not valid? Source: DataCamp Its Across all genders, Bad is the most common alignment. 7.1.7 Video: Counts vs. proportions View slides. 7.1.8 Question: Conditional proportions tab &lt;- table(comics$align, comics$gender) options(scipen = 999, digits = 3) # Print fewer digits prop.table(tab) # Joint proportions ## ## Female Male Other ## Bad 0.0821968 0.3950985 0.0016722 ## Good 0.1301144 0.2512933 0.0008883 ## Neutral 0.0436850 0.0940064 0.0008883 ## Reformed Criminals 0.0000523 0.0001045 0.0000000 prop.table(tab, 2) # Conditional on columns ## ## Female Male Other ## Bad 0.321020 0.533554 0.484848 ## Good 0.508163 0.339355 0.257576 ## Neutral 0.170612 0.126949 0.257576 ## Reformed Criminals 0.000204 0.000141 0.000000 Approximately what proportion of all female characters are good? Its 51%. To answer this question, you needed to look at how align was distributed within each gender. That is, you wanted to condition on the gender variable. 7.1.9 Counts vs. proportions (2) Bar charts can tell dramatically different stories depending on whether they represent counts or proportions and, if proportions, what the proportions are conditioned on. To demonstrate this difference, youll construct two barcharts in this exercise: one of counts and one of proportions. # Plot of gender by align ggplot(comics, aes(x = align, fill = gender)) + geom_bar() # Plot proportion of gender, conditional on align ggplot(comics, aes(x = align, fill = gender)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;proportion&quot;) By adding position = \"fill\" to geom_bar(), you are saying you want the bars to fill the entire height of the plotting window, thus displaying proportions and not raw counts. 7.1.10 Video: Distribution of one variable View slides. Take your original two-way table, then sum the cells across each level of one of the variables. Since weve summed over the margins of the other variable, this is sometimes called a marginal distribution. Faceting vs. stacking. Source: DataCamp Areas are easier to compare in bar charts. Source: DataCamp 7.1.11 Marginal barchart If you are interested in the distribution of alignment of all superheroes, it makes sense to construct a barchart for just that single variable. You can improve the interpretability of the plot, though, by implementing some sensible ordering. Superheroes that are \"Neutral\" show an alignment between \"Good\" and \"Bad\", so it makes sense to put that bar in the middle. # Change the order of the levels in align comics$align &lt;- factor(comics$align, levels = c(&quot;Bad&quot;, &quot;Neutral&quot;, &quot;Good&quot;)) # Create plot of align comics %&gt;% filter(!is.na(align)) %&gt;% ggplot(aes(x = align)) + geom_bar() 7.1.12 Conditional barchart Now, if you want to break down the distribution of alignment based on gender, youre looking for conditional distributions. You could make these by creating multiple filtered datasets (one for each gender) or by faceting the plot of alignment based on gender. As a point of comparison, weve provided your plot of the marginal distribution of alignment from the last exercise. # Plot of alignment broken down by gender ggplot(comics, aes(x = align)) + geom_bar() + facet_wrap(~ gender) 7.1.13 Improve piechart The piechart is a very common way to represent the distribution of a single categorical variable, but they can be more difficult to interpret than barcharts. This is a piechart of a dataset called pies that contains the favorite pie flavors of 98 people. Improve the representation of these data by constructing a barchart that is ordered in descending order of count. pies &lt;- data.frame(c(rep(&quot;apple&quot;, times = 17), rep(&quot;blueberry&quot;, times = 14), rep(&quot;boston creme&quot;, times =15), rep(&quot;cherry&quot;, times =13), rep(&quot;key lime&quot;, times =16), rep(&quot;pumpkin&quot;, times =12), rep(&quot;strawberry&quot;, times =11))) names(pies) &lt;- &quot;flavor&quot; # Create pie chart of flavor pie(table(pies$flavor)) ggplot(pies, aes(x = flavor)) + geom_bar() # Put levels of flavor in descending order lev &lt;- c(&quot;apple&quot;, &quot;key lime&quot;, &quot;boston creme&quot;, &quot;blueberry&quot;, &quot;cherry&quot;, &quot;pumpkin&quot;, &quot;strawberry&quot;) pies$flavor &lt;- factor(pies$flavor, levels = lev) # Create barchart of flavor ggplot(pies, aes(x = flavor)) + geom_bar(fill = &quot;chartreuse&quot;) + theme(axis.text.x = element_text(angle = 90)) # Alternative solution to finding levels # lev &lt;- unlist(select(arrange(cnt, desc(n)), flavor)) 7.2 Cars In this chapter, you will learn how to graphically summarize numerical data. library(readr) cars &lt;- read_csv(&#39;https://assets.datacamp.com/production/course_1796/datasets/cars04.csv&#39;) ## Parsed with column specification: ## cols( ## name = col_character(), ## sports_car = col_logical(), ## suv = col_logical(), ## wagon = col_logical(), ## minivan = col_logical(), ## pickup = col_logical(), ## all_wheel = col_logical(), ## rear_wheel = col_logical(), ## msrp = col_double(), ## dealer_cost = col_double(), ## eng_size = col_double(), ## ncyl = col_double(), ## horsepwr = col_double(), ## city_mpg = col_double(), ## hwy_mpg = col_double(), ## weight = col_double(), ## wheel_base = col_double(), ## length = col_double(), ## width = col_double() ## ) #cars &lt;- cars %&gt;% # mutate(msrp = as.integer(msrp)) cars[,c(9:10,12:19)] &lt;- sapply(cars[,c(9:10,12:19)],as.integer) 7.2.1 Video: Exploring numerical data View slides. str(cars) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 428 obs. of 19 variables: ## $ name : chr &quot;Chevrolet Aveo 4dr&quot; &quot;Chevrolet Aveo LS 4dr hatch&quot; &quot;Chevrolet Cavalier 2dr&quot; &quot;Chevrolet Cavalier 4dr&quot; ... ## $ sports_car : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ suv : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ wagon : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ minivan : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ pickup : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ all_wheel : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ rear_wheel : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ msrp : int 11690 12585 14610 14810 16385 13670 15040 13270 13730 15460 ... ## $ dealer_cost: int 10965 11802 13697 13884 15357 12849 14086 12482 12906 14496 ... ## $ eng_size : num 1.6 1.6 2.2 2.2 2.2 2 2 2 2 2 ... ## $ ncyl : int 4 4 4 4 4 4 4 4 4 4 ... ## $ horsepwr : int 103 103 140 140 140 132 132 130 110 130 ... ## $ city_mpg : int 28 28 26 26 26 29 29 26 27 26 ... ## $ hwy_mpg : int 34 34 37 37 37 36 36 33 36 33 ... ## $ weight : int 2370 2348 2617 2676 2617 2581 2626 2612 2606 2606 ... ## $ wheel_base : int 98 98 104 104 104 105 105 103 103 103 ... ## $ length : int 167 153 183 183 183 174 174 168 168 168 ... ## $ width : int 66 66 69 68 69 67 67 67 67 67 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. name = col_character(), ## .. sports_car = col_logical(), ## .. suv = col_logical(), ## .. wagon = col_logical(), ## .. minivan = col_logical(), ## .. pickup = col_logical(), ## .. all_wheel = col_logical(), ## .. rear_wheel = col_logical(), ## .. msrp = col_double(), ## .. dealer_cost = col_double(), ## .. eng_size = col_double(), ## .. ncyl = col_double(), ## .. horsepwr = col_double(), ## .. city_mpg = col_double(), ## .. hwy_mpg = col_double(), ## .. weight = col_double(), ## .. wheel_base = col_double(), ## .. length = col_double(), ## .. width = col_double() ## .. ) # The most direct way to represent numerical data is a dotplot ggplot(cars, aes(x = weight)) + geom_dotplot(dotsize = 0.4) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing non-finite values (stat_bindot). # There is zero data loss in a dotplot - you could recreate the data set perfectly from the display ggplot(cars, aes(x = weight)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing non-finite values (stat_bin). # Because of binning, it&#39;s no longer possible to recreate the original data set # A density plot avoids the unnatural step-wise nature of a histogram ggplot(cars, aes(x = weight)) + geom_density() ## Warning: Removed 2 rows containing non-finite values (stat_density). # Use only when you have a large number of cases # A more abstracted sense of the distribution ggplot(cars, aes(x = 1, y = weight)) + geom_boxplot() + coord_flip() ## Warning: Removed 2 rows containing non-finite values (stat_boxplot). ggplot(cars, aes(x = hwy_mpg)) + geom_histogram() + facet_wrap(~pickup) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). 7.2.2 Faceted histogram In this chapter, youll be working with the cars dataset, which records characteristics on all of the new models of cars for sale in the US in a certain year. You will investigate the distribution of mileage across a categorial variable, but before you get there, youll want to familiarize yourself with the dataset. # Load package library(ggplot2) # Learn data structure str(cars) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 428 obs. of 19 variables: ## $ name : chr &quot;Chevrolet Aveo 4dr&quot; &quot;Chevrolet Aveo LS 4dr hatch&quot; &quot;Chevrolet Cavalier 2dr&quot; &quot;Chevrolet Cavalier 4dr&quot; ... ## $ sports_car : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ suv : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ wagon : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ minivan : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ pickup : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ all_wheel : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ rear_wheel : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ msrp : int 11690 12585 14610 14810 16385 13670 15040 13270 13730 15460 ... ## $ dealer_cost: int 10965 11802 13697 13884 15357 12849 14086 12482 12906 14496 ... ## $ eng_size : num 1.6 1.6 2.2 2.2 2.2 2 2 2 2 2 ... ## $ ncyl : int 4 4 4 4 4 4 4 4 4 4 ... ## $ horsepwr : int 103 103 140 140 140 132 132 130 110 130 ... ## $ city_mpg : int 28 28 26 26 26 29 29 26 27 26 ... ## $ hwy_mpg : int 34 34 37 37 37 36 36 33 36 33 ... ## $ weight : int 2370 2348 2617 2676 2617 2581 2626 2612 2606 2606 ... ## $ wheel_base : int 98 98 104 104 104 105 105 103 103 103 ... ## $ length : int 167 153 183 183 183 174 174 168 168 168 ... ## $ width : int 66 66 69 68 69 67 67 67 67 67 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. name = col_character(), ## .. sports_car = col_logical(), ## .. suv = col_logical(), ## .. wagon = col_logical(), ## .. minivan = col_logical(), ## .. pickup = col_logical(), ## .. all_wheel = col_logical(), ## .. rear_wheel = col_logical(), ## .. msrp = col_double(), ## .. dealer_cost = col_double(), ## .. eng_size = col_double(), ## .. ncyl = col_double(), ## .. horsepwr = col_double(), ## .. city_mpg = col_double(), ## .. hwy_mpg = col_double(), ## .. weight = col_double(), ## .. wheel_base = col_double(), ## .. length = col_double(), ## .. width = col_double() ## .. ) # Create faceted histogram ggplot(cars, aes(x = city_mpg)) + geom_histogram() + facet_wrap(~ suv) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). In this exercise, you faceted by the suv variable, but its important to note that you can facet a plot by any categorical variable using facet_wrap(). 7.2.3 Boxplots and density plots The mileage of a car tends to be associated with the size of its engine (as measured by the number of cylinders). To explore the relationship between these two variables, you could stick to using histograms, but in this exercise youll try your hand at two alternatives: the box plot and the density plot. # Create box plots of city mpg by ncyl ggplot(cars, aes(x = as.factor(ncyl), y = city_mpg)) + geom_boxplot() ## Warning: Removed 14 rows containing non-finite values (stat_boxplot). # Check how many possible levels of ncyl there are unique(cars$ncyl) ## [1] 4 6 3 8 5 12 10 -1 # Which levels are the most common? table(cars$ncyl) ## ## -1 3 4 5 6 8 10 12 ## 2 1 136 7 190 87 2 3 # Filter cars with 4, 6, 8 cylinders common_cyl &lt;- filter(cars, ncyl %in% c(4, 6, 8)) # Create box plots of city mpg by ncyl ggplot(common_cyl, aes(x = as.factor(ncyl), y = city_mpg)) + geom_boxplot() ## Warning: Removed 11 rows containing non-finite values (stat_boxplot). # Create overlaid density plots for same data ggplot(common_cyl, aes(x = city_mpg, fill = as.factor(ncyl))) + geom_density(alpha = .3) ## Warning: Removed 11 rows containing non-finite values (stat_density). 7.2.4 Compare distribution via plots Which of the following interpretations of the plot is not valid? Its The variability in mileage of 8 cylinder cars is similar to the variability in mileage of 4 cylinder cars. The variability in mileage of 8 cylinder cars seem much smaller than that of 4 cylinder cars. 7.2.5 Video: Distribution of one variable View slides. Specifically a numerical variable. ggplot(cars, aes(x = hwy_mpg)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). # Adding a facet_wrap layer (faceting on a categorical value) ggplot(cars, aes(x = hwy_mpg)) + geom_histogram() + facet_wrap(~pickup) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). # Adding a filter (filtering on a numerical variable) cars2 &lt;- cars %&gt;% filter(eng_size &lt; 2.0) ggplot(cars2, aes(x = hwy_mpg)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Alternatively cars %&gt;% filter(eng_size &lt; 2.0) %&gt;% ggplot(aes(x = hwy_mpg)) + # Note that you don&#39;t need to specify the data frame, as we&#39;re piping in cars geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Alternatively cars %&gt;% filter(eng_size &lt; 2.0) %&gt;% ggplot(aes(x = hwy_mpg)) + # Note that you don&#39;t need to specify the data frame, as we&#39;re piping in cars geom_histogram(binwidth = 5) 7.2.6 Marginal and conditional histograms Now, turn your attention to a new variable: horsepwr. The goal is to get a sense of the marginal distribution of this variable and then compare it to the distribution of horsepower conditional on the price of the car being less than $25,000. Youll be making two plots using the data pipeline paradigm, where you start with the raw data and end with the plot. # Create hist of horsepwr cars %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram() + xlim(c(90, 550)) + ylim(c(0, 50)) + ggtitle(&quot;Distribution of horsepower for all cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). ## Warning: Removed 3 rows containing missing values (geom_bar). # Create hist of horsepwr for affordable cars cars %&gt;% filter(msrp &lt; 25000) %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram() + xlim(c(90, 550)) + ylim(c(0, 50)) + ggtitle(&quot;Distribution of horsepower for affordable cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). ## Warning: Removed 2 rows containing missing values (geom_bar). 7.2.7 Question: Marginal and conditional histograms interpretation Observe the two histograms in the plotting window and decide which of the following is a valid interpretation. Its The highest horsepower car in the less expensive range has just under 250 horsepower. 7.2.8 Three binwidths Before you take these plots for granted, its a good idea to see how things change when you alter the binwidth. The binwidth determines how smooth your distribution will appear: the smaller the binwidth, the more jagged your distribution becomes. Its good practice to consider several binwidths in order to detect different types of structure in your data. # Create hist of horsepwr with binwidth of 3 cars %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram(binwidth = 3) + ggtitle(&quot;binwidth = 3&quot;) # Create hist of horsepwr with binwidth of 30 cars %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram(binwidth = 30) + ggtitle(&quot;binwidth = 30&quot;) # Create hist of horsepwr with binwidth of 60 cars %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram(binwidth = 60) + ggtitle(&quot;binwidth = 60&quot;) Be sure to toggle back and forth in the plots pane to compare the histograms. 7.2.9 Question: Three binwidths interpretation What feature is present in Plot A thats not found in B or C? Its There is a tendency for cars to have horsepower right at 200 or 300 horsepower. Plot A is the only histogram that shows the count for cars with exactly 200 and 300 horsepower. 7.2.10 Video: Box plots View slides. 7.2.11 Box plots for outliers In addition to indicating the center and spread of a distribution, a box plot provides a graphical means to detect outliers. You can apply this method to the msrp column (manufacturers suggested retail price) to detect if there are unusually expensive or cheap cars. # Construct box plot of msrp cars %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() # Exclude outliers from data cars_no_out &lt;- cars %&gt;% filter(msrp &lt; 100000) # Construct box plot of msrp using the reduced dataset cars_no_out %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() Be sure to toggle back and forth in the plots pane to compare the box plots. 7.2.12 Plot selection Consider two other columns in the cars dataset: city_mpg and width. Which is the most appropriate plot for displaying the important features of their distributions? Remember, both density plots and box plots display the central tendency and spread of the data, but the box plot is more robust to outliers. # Create plot of city_mpg cars %&gt;% ggplot(aes(x = city_mpg)) + geom_density() ## Warning: Removed 14 rows containing non-finite values (stat_density). cars %&gt;% ggplot(aes(x = 1, y = city_mpg)) + geom_boxplot() ## Warning: Removed 14 rows containing non-finite values (stat_boxplot). # Create plot of width cars %&gt;% ggplot(aes(x = width)) + geom_density() ## Warning: Removed 28 rows containing non-finite values (stat_density). cars %&gt;% ggplot(aes(x = 1, y = width)) + geom_boxplot() ## Warning: Removed 28 rows containing non-finite values (stat_boxplot). Because the city_mpg variable has a much wider range with its outliers, its best to display its distribution as a box plot. 7.2.13 Video: Visualization in higher dimensions View slides. ggplot(cars, aes(x = msrp)) + geom_density() + facet_grid(pickup ~ rear_wheel) # Add labels to aid understanding ggplot(cars, aes(x = msrp)) + geom_density() + facet_grid(pickup ~ rear_wheel, labeller = label_both) # There&#39;s very few rear wheel pickups and front wheel pickups table(cars$rear_wheel, cars$pickup) # table(rows, columns) ## ## FALSE TRUE ## FALSE 306 12 ## TRUE 98 12 7.2.14 3 variable plot Faceting is a valuable technique for looking at several conditional distributions at the same time. If the faceted distributions are laid out in a grid, you can consider the association between a variable and two others, one on the rows of the grid and the other on the columns. # Facet hists using hwy mileage and ncyl common_cyl %&gt;% ggplot(aes(x = hwy_mpg)) + geom_histogram() + facet_grid(ncyl ~ suv, labeller = label_both) + ggtitle(&quot;Mileage by suv and ncyl&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 11 rows containing non-finite values (stat_bin). 7.2.15 Question: Interpret 3 var plot Which of the following interpretations of the plot is valid? Its Across both SUVs and non-SUVs, mileage tends to decrease as the number of cylinders increases. 7.3 Gapminder Now that weve looked at exploring categorical and numerical data, youll learn some useful statistics for describing distributions of data. 7.3.1 Video: Measures of center View slides. 7.3.2 Question: Choice of center measure The choice of measure for center can have a dramatic impact on what we consider to be a typical observation, so it is important that you consider the shape of the distribution before deciding on the measure. Which set of measures of central tendency would be worst for describing the two distributions shown here? Source: DataCamp Its A: mean, B: mode. 7.3.3 Calculate center measures Throughout this chapter, you will use data from gapminder, which tracks demographic data in countries of the world over time. To learn more about it, you can bring up the help file with ?gapminder. For this exercise, focus on how the life expectancy differs from continent to continent. This requires that you conduct your analysis not at the country level, but aggregated up to the continent level. This is made possible by the one-two punch of group_by() and summarize(), a very powerful syntax for carrying out the same analysis on different subsets of the full dataset. # Create dataset of 2007 data gap2007 &lt;- filter(gapminder, year == 2007) # Compute groupwise mean and median lifeExp gap2007 %&gt;% group_by(continent) %&gt;% summarize(mean(lifeExp), median(lifeExp)) ## # A tibble: 5 x 3 ## continent `mean(lifeExp)` `median(lifeExp)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 54.8 52.9 ## 2 Americas 73.6 72.9 ## 3 Asia 70.7 72.4 ## 4 Europe 77.6 78.6 ## 5 Oceania 80.7 80.7 # Generate box plots of lifeExp for each continent gap2007 %&gt;% ggplot(aes(x = continent, y = lifeExp)) + geom_boxplot() 7.3.4 Video: Measures of variability View slides. x &lt;- head(round(gap2007$lifeExp), 11) x - mean(x) ## [1] -23.91 8.09 4.09 -24.91 7.09 13.09 12.09 8.09 -3.91 ## [10] 11.09 -10.91 sum(x - mean(x)) # which is close to 0 ## [1] 0.0000000000000284 sum((x - mean(x))^2) # which will keep getting bigger the more data you add ## [1] 1965 n &lt;- 11 sum((x - mean(x))^2)/n ## [1] 179 sum((x - mean(x))^2)/(n-1) # The sample variance ## [1] 196 var(x) # R&#39;s built-in function ## [1] 196 sqrt(sum((x - mean(x))^2)/(n-1)) ## [1] 14 sd(x) ## [1] 14 # which is more commonly used than the... summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 43.0 60.5 75.0 67.9 77.5 81.0 IQR(x) ## [1] 17 # However SD is affected by extreme values, unlike IQR, which is better to use with large skew and extreme outliers Choice of spread measure The choice of measure for spread can dramatically impact how variable we consider our data to be, so it is important that you consider the shape of the distribution before deciding on the measure. Which set of measures of spread would be worst for describing the two distributions shown here? Source: DataCamp Its A: Variance, B: Range. Notice the high peak of A and the considerable width of it. What does that tell you about its variance? 7.3.5 Calculate spread measures Lets extend the powerful group_by() and summarize() syntax to measures of spread. If youre unsure whether youre working with symmetric or skewed distributions, its a good idea to consider a robust measure like IQR in addition to the usual measures of variance or standard deviation. # Compute groupwise measures of spread gap2007 %&gt;% group_by(continent) %&gt;% summarize(sd(lifeExp), IQR(lifeExp), n()) ## # A tibble: 5 x 4 ## continent `sd(lifeExp)` `IQR(lifeExp)` `n()` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 9.63 11.6 52 ## 2 Americas 4.44 4.63 25 ## 3 Asia 7.96 10.2 33 ## 4 Europe 2.98 4.78 30 ## 5 Oceania 0.729 0.516 2 # Generate overlaid density plots gap2007 %&gt;% ggplot(aes(x = lifeExp, fill = continent)) + geom_density(alpha = 0.3) 7.3.6 Choose measures for center and spread Consider the density plots shown here. What are the most appropriate measures to describe their centers and spreads? In this exercise, youll select the measures and then calculate them. Source: DataCamp # Compute stats for lifeExp in Americas gap2007 %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% summarize(mean(lifeExp), sd(lifeExp)) ## # A tibble: 1 x 2 ## `mean(lifeExp)` `sd(lifeExp)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 73.6 4.44 # Compute stats for population gap2007 %&gt;% summarize(median(pop), IQR(pop)) ## # A tibble: 1 x 2 ## `median(pop)` `IQR(pop)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10517531 26702008. Like mean and standard deviation, median and IQR measure the central tendency and spread, respectively, but are robust to outliers and non-normal data. 7.3.7 Video: Shape and transformations View slides. 7.3.8 Describe the shape To build some familiarity with distributions of different shapes, consider the four that are plotted here. Which of the following options does the best job of describing their shape in terms of modality and skew/symmetry? Source: DataCamp Its A: unimodal left-skewed; B: unimodal symmetric; C: unimodal right-skewed, D: bimodal symmetric. 7.3.9 Transformations Highly skewed distributions can make it very difficult to learn anything from a visualization. Transformations can be helpful in revealing the more subtle structure. Here youll focus on the population variable, which exhibits strong right skew, and transform it with the natural logarithm function (log() in R). # Create density plot of old variable gap2007 %&gt;% ggplot(aes(x = pop)) + geom_density() # Transform the skewed pop variable gap2007 &lt;- gap2007 %&gt;% mutate(log_pop = log(pop)) # Create density plot of new variable gap2007 %&gt;% ggplot(aes(x = log(pop))) + geom_density() 7.3.10 Video: Outliers View slides. 7.3.11 Identify outliers Consider the distribution, shown here, of the life expectancies of the countries in Asia. The box plot identifies one clear outlier: a country with a notably low life expectancy. Do you have a guess as to which country this might be? Test your guess in the console using either min() or filter(), then proceed to building a plot with that country removed. # Filter for Asia, add column indicating outliers gap_asia &lt;- gap2007 %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% mutate(is_outlier = lifeExp &lt; 50) # Remove outliers, create box plot of lifeExp gap_asia %&gt;% filter(!is_outlier) %&gt;% ggplot(aes(x = 1, y = lifeExp)) + geom_boxplot() 7.4 Email Apply what youve learned to explore and summarize a real world dataset in this case study of email spam. We will use the email data set in the openintro package. 7.4.1 Video: Introducing the data View slides. email &lt;- email %&gt;% mutate(spam = as.factor(spam)) levels(email$spam) &lt;- c(&quot;not-spam&quot;, &quot;spam&quot;) head(email) ## # A tibble: 6 x 21 ## spam to_multiple from cc sent_email time image ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 not-~ 0 1 0 0 2012-01-01 06:16:41 0 ## 2 not-~ 0 1 0 0 2012-01-01 07:03:59 0 ## 3 not-~ 0 1 0 0 2012-01-01 16:00:32 0 ## 4 not-~ 0 1 0 0 2012-01-01 09:09:49 0 ## 5 not-~ 0 1 0 0 2012-01-01 10:00:01 0 ## 6 not-~ 0 1 0 0 2012-01-01 10:04:46 0 ## # ... with 14 more variables: attach &lt;dbl&gt;, dollar &lt;dbl&gt;, ## # winner &lt;fct&gt;, inherit &lt;dbl&gt;, viagra &lt;dbl&gt;, password &lt;dbl&gt;, ## # num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, format &lt;dbl&gt;, re_subj &lt;dbl&gt;, ## # exclaim_subj &lt;dbl&gt;, urgent_subj &lt;dbl&gt;, exclaim_mess &lt;dbl&gt;, ## # number &lt;fct&gt; str(email) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3921 obs. of 21 variables: ## $ spam : Factor w/ 2 levels &quot;not-spam&quot;,&quot;spam&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ to_multiple : num 0 0 0 0 0 0 1 1 0 0 ... ## $ from : num 1 1 1 1 1 1 1 1 1 1 ... ## $ cc : int 0 0 0 0 0 0 0 1 0 0 ... ## $ sent_email : num 0 0 0 0 0 0 1 1 0 0 ... ## $ time : POSIXct, format: &quot;2012-01-01 06:16:41&quot; ... ## $ image : num 0 0 0 0 0 0 0 1 0 0 ... ## $ attach : num 0 0 0 0 0 0 0 1 0 0 ... ## $ dollar : num 0 0 4 0 0 0 0 0 0 0 ... ## $ winner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ inherit : num 0 0 1 0 0 0 0 0 0 0 ... ## $ viagra : num 0 0 0 0 0 0 0 0 0 0 ... ## $ password : num 0 0 0 0 2 2 0 0 0 0 ... ## $ num_char : num 11.37 10.5 7.77 13.26 1.23 ... ## $ line_breaks : int 202 202 192 255 29 25 193 237 69 68 ... ## $ format : num 1 1 1 1 0 0 1 1 0 1 ... ## $ re_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_subj: num 0 0 0 0 0 0 0 0 0 0 ... ## $ urgent_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_mess: num 0 1 6 48 1 1 1 18 1 0 ... ## $ number : Factor w/ 3 levels &quot;none&quot;,&quot;small&quot;,..: 3 2 2 2 1 1 3 2 2 2 ... 7.4.2 Spam and num_char Is there an association between spam and the length of an email? You could imagine a story either way: Spam is more likely to be a short message tempting me to click on a link, or My normal email is likely shorter since I exchange brief emails with my friends all the time. Here, youll use the email dataset to settle that question. Begin by bringing up the help file and learning about all the variables with ?email. As you explore the association between spam and the length of an email, use this opportunity to try out linking a dplyr chain with the layers in a ggplot2 object. # Compute summary statistics email %&gt;% group_by(spam) %&gt;% summarize(median(num_char), IQR(num_char)) ## # A tibble: 2 x 3 ## spam `median(num_char)` `IQR(num_char)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 not-spam 6.83 13.6 ## 2 spam 1.05 2.82 # Create plot email %&gt;% mutate(log_num_char = log(num_char)) %&gt;% ggplot(aes(x = spam, y = log_num_char)) + geom_boxplot() Youll interpret this plot in the next exercise. 7.4.3 Spam and num_char interpretation Which of the following interpretations of the plot is valid? Its The median length of not-spam emails is greater than that of spam emails. 7.4.4 Spam and !!! Lets look at a more obvious indicator of spam: exclamation marks. exclaim_mess contains the number of exclamation marks in each message. Using summary statistics and visualization, see if there is a relationship between this variable and whether or not a message is spam. Experiment with different types of plots until you find one that is the most informative. Recall that youve seen: Side-by-side box plots Faceted histograms Overlaid density plots # Compute center and spread for exclaim_mess by spam email %&gt;% group_by(spam) %&gt;% summarize(median(exclaim_mess), IQR(exclaim_mess)) ## # A tibble: 2 x 3 ## spam `median(exclaim_mess)` `IQR(exclaim_mess)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 not-spam 1 5 ## 2 spam 0 1 # Create plot for spam and exclaim_mess email %&gt;% mutate(log_exclaim_mess = log(exclaim_mess + 0.01)) %&gt;% ggplot(aes(x = 1, y = log_exclaim_mess)) + geom_boxplot() + facet_wrap(~ spam) # Alternative plot: side-by-side box plots email %&gt;% mutate(log_exclaim_mess = log(exclaim_mess + 0.01)) %&gt;% ggplot(aes(x = log_exclaim_mess)) + geom_histogram() + facet_wrap(~ spam) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Alternative plot: Overlaid density plots email %&gt;% mutate(log_exclaim_mess = log(exclaim_mess + .01)) %&gt;% ggplot(aes(x = log_exclaim_mess, fill = spam)) + geom_density(alpha = 0.3) 7.4.5 Spam and !!! interpretation Which interpretation of these faceted histograms is not correct? Its There are more cases of spam in this dataset than not-spam. 7.4.6 Video: Check-in 1 View slides. 7.4.7 Collapsing levels If it was difficult to work with the heavy skew of exclaim_mess, the number of images attached to each email (image) poses even more of a challenge. Run the following code at the console to get a sense of its distribution: table(email$image) ## ## 0 1 2 3 4 5 9 20 ## 3811 76 17 11 2 2 1 1 Recall that this tabulates the number of cases in each category (so there were 3811 emails with 0 images, for example). Given the very low counts at the higher number of images, lets collapse image into a categorical variable that indicates whether or not the email had at least one image. In this exercise, youll create this new variable and explore its association with spam. # Create plot of proportion of spam by image email %&gt;% mutate(has_image = image &gt; 0) %&gt;% ggplot(aes(x = has_image, fill = spam)) + geom_bar(position = &quot;fill&quot;) Lets interpret this plot in the next exercise. 7.4.8 Question: Image and spam interpretation Which of the following interpretations of the plot is valid? Its An email without an image is more likely to be not-spam than spam. 7.4.9 Data Integrity In the process of exploring a dataset, youll sometimes come across something that will lead you to question how the data were compiled. For example, the variable num_char contains the number of characters in the email, in thousands, so it could take decimal values, but it certainly shouldnt take negative values. You can formulate a test to ensure this variable is behaving as we expect: email$num_char &lt; 0 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [11] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [21] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [31] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [41] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [51] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [71] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [81] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [91] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [101] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [111] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [131] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [141] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [151] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [161] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [171] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [191] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [201] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [211] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [221] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [231] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [251] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [261] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [271] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [281] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [291] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [311] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [321] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [331] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [341] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [351] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [371] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [381] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [391] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [401] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [411] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [431] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [441] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [451] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [461] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [471] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [491] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [501] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [511] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [521] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [531] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [551] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [561] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [571] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [581] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [591] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [601] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [611] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [621] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [631] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [641] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [651] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [671] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [681] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [691] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [701] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [711] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [731] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [741] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [751] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [761] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [771] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [791] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [801] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [811] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [821] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [831] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [851] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [861] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [871] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [881] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [891] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [911] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [921] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [931] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [941] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [951] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [961] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [971] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [981] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [991] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [ reached getOption(&quot;max.print&quot;) -- omitted 2921 entries ] If you run this code at the console, youll get a long vector of logical values indicating for each case in the dataset whether that condition is TRUE. Here, the first 1000 values all appear to be FALSE. To verify that all of the cases indeed have non-negative values for num_char, we can take the sum of this vector: sum(email$num_char &lt; 0) ## [1] 0 This is a handy shortcut. When you do arithmetic on logical values, R treats TRUE as 1 and FALSE as 0. Since the sum over the whole vector is zero, you learn that every case in the dataset took a value of FALSE in the test. That is, the num_char column is behaving as we expect and taking only non-negative values. # Test if images count as attachments sum(email$image &gt; email$attach) ## [1] 0 Since image is never greater than attach, we can infer that images are counted as attachments. 7.4.10 Answering questions with chains When you have a specific question about a dataset, you can find your way to an answer by carefully constructing the appropriate chain of R code. For example, consider the following question: Within non-spam emails, is the typical length of emails shorter for those that were sent to multiple people? This can be answered with the following chain: email %&gt;% filter(spam == &quot;not-spam&quot;) %&gt;% group_by(as.factor(to_multiple)) %&gt;% summarize(median(num_char)) ## # A tibble: 2 x 2 ## `as.factor(to_multiple)` `median(num_char)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 0 7.20 ## 2 1 5.36 email %&gt;% filter(spam == &quot;not-spam&quot;) %&gt;% group_by(to_multiple) %&gt;% summarise(median(num_char)) ## # A tibble: 2 x 2 ## to_multiple `median(num_char)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 7.20 ## 2 1 5.36 str(email) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3921 obs. of 21 variables: ## $ spam : Factor w/ 2 levels &quot;not-spam&quot;,&quot;spam&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ to_multiple : num 0 0 0 0 0 0 1 1 0 0 ... ## $ from : num 1 1 1 1 1 1 1 1 1 1 ... ## $ cc : int 0 0 0 0 0 0 0 1 0 0 ... ## $ sent_email : num 0 0 0 0 0 0 1 1 0 0 ... ## $ time : POSIXct, format: &quot;2012-01-01 06:16:41&quot; ... ## $ image : num 0 0 0 0 0 0 0 1 0 0 ... ## $ attach : num 0 0 0 0 0 0 0 1 0 0 ... ## $ dollar : num 0 0 4 0 0 0 0 0 0 0 ... ## $ winner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ inherit : num 0 0 1 0 0 0 0 0 0 0 ... ## $ viagra : num 0 0 0 0 0 0 0 0 0 0 ... ## $ password : num 0 0 0 0 2 2 0 0 0 0 ... ## $ num_char : num 11.37 10.5 7.77 13.26 1.23 ... ## $ line_breaks : int 202 202 192 255 29 25 193 237 69 68 ... ## $ format : num 1 1 1 1 0 0 1 1 0 1 ... ## $ re_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_subj: num 0 0 0 0 0 0 0 0 0 0 ... ## $ urgent_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_mess: num 0 1 6 48 1 1 1 18 1 0 ... ## $ number : Factor w/ 3 levels &quot;none&quot;,&quot;small&quot;,..: 3 2 2 2 1 1 3 2 2 2 ... The code makes it clear that you are using num_char to measure the length of an email and median() as the measure of what is typical. If you run this code, youll learn that the answer to the question is yes: the typical length of non-spam sent to multiple people is a bit lower than those sent to only one person. This chain concluded with summary statistics, but others might end in a plot; it all depends on the question that youre trying to answer. # Question 1 email %&gt;% filter(dollar &gt; 0) %&gt;% group_by(spam) %&gt;% summarize(mean(dollar)) ## # A tibble: 2 x 2 ## spam `mean(dollar)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 not-spam 8.21 ## 2 spam 3.44 # Question 2 email %&gt;% filter(dollar &gt; 10) %&gt;% ggplot(aes(x = spam)) + geom_bar() 7.4.11 Video: Check-in 2 View slides. Were now moving from Exploratory graphics (aiming to find out whats there) to Expository graphics (aiming to you seek to explain, describe or communicate a particular idea). 7.4.12 Whats in a number? Turn your attention to the variable called number. Read more about it by pulling up the help file with ?email. To explore the association between this variable and spam, select and construct an informative plot. For illustrating relationships between categorical variables, youve seen Faceted barcharts Side-by-side barcharts Stacked and normalized barcharts. Lets practice constructing a faceted barchart. # Reorder levels email$number_reordered &lt;- factor(email$number, levels = c(&quot;none&quot; , &quot;small&quot;, &quot;big&quot;)) # Construct plot of number_reordered ggplot(email, aes(x = number_reordered)) + geom_bar() + facet_wrap(~spam) 7.4.13 Whats in a number interpretation Which of the following interpretations of the plot is not valid? Its Given that an email contains no number, it is more likely to be spam. 7.4.14 Video: Conclusion View slides. 7.5 Challenge Via DataCamp Once youve started learning tools for data manipulation and visualization like dplyr and ggplot2, this course gives you a chance to use them in action on a real dataset. Youll explore the historical voting of the United Nations General Assembly, including analyzing differences in voting between countries, across time, and among international issues. In the process youll gain more practice with the dplyr and ggplot2 packages, learn about the broom package for tidying model output, and experience the kind of start-to-finish exploratory analysis common in data science. 7.6 Solutions 1: Data cleaning and summarizing with dplyr The best way to learn data wrangling skills is to apply them to a specific case study. Here youll learn how to clean and filter the United Nations voting dataset using the dplyr package, and how to summarize it into smaller, interpretable units. url &lt;- &quot;https://github.com/datasciencelabs/data/raw/master/rawvotingdata13.tab&quot; filename &lt;- basename(url) if (!file.exists(filename)) download(url,destfile=filename) votes &lt;-read.delim(&quot;rawvotingdata13.tab&quot;, header = TRUE, sep = &quot;\\t&quot;, quote = &quot;&quot;) votes &lt;- votes %&gt;% filter(session != 19) 7.6.1 Video: The United Nations Voting Dataset View slides. str(votes) ## &#39;data.frame&#39;: 1051555 obs. of 4 variables: ## $ rcid : num 3 3 3 3 3 3 3 3 3 3 ... ## $ session: num 1 1 1 1 1 1 1 1 1 1 ... ## $ vote : num 1 3 9 1 1 1 9 9 9 9 ... ## $ ccode : num 2 20 31 40 41 42 51 52 53 54 ... head(votes) ## rcid session vote ccode ## 1 3 1 1 2 ## 2 3 1 3 20 ## 3 3 1 9 31 ## 4 3 1 1 40 ## 5 3 1 1 41 ## 6 3 1 1 42 votes &lt;- votes %&gt;% mutate(year = session + 1945) 7.6.2 Filtering rows The vote column in the dataset has a number that represents that countrys vote: 1 = Yes 2 = Abstain 3 = No 8 = Not present 9 = Not a member One step of data cleaning is removing observations (rows) that youre not interested in. In this case, you want to remove Not present and Not a member. unique(votes$vote) ## [1] 1 3 9 2 8 # Filter for votes that are &quot;yes&quot;, &quot;abstain&quot;, or &quot;no&quot; votes %&gt;% filter(vote &lt;= 3) 7.6.3 Adding a year column The next step of data cleaning is manipulating your variables (columns) to make them more informative. In this case, you have a session column that is hard to interpret intuitively. But since the UN started voting in 1946, and holds one session per year, you can get the year of a UN resolution by adding 1945 to the session number. # Add another %&gt;% step to add a year column votes %&gt;% filter(vote &lt;= 3) %&gt;% mutate(year = session + 1945) 7.6.4 Adding a country column The country codes in the ccode column are whats called Correlates of War codes. This isnt ideal for an analysis, since youd like to work with recognizable country names. You can use the countrycode package to translate. For example: library(countrycode) # Translate the country code 2 countrycode(2, &quot;cown&quot;, &quot;country.name&quot;) ## [1] &quot;United States&quot; # Translate the country code 703 countrycode(703, &quot;cown&quot;, &quot;country.name&quot;) ## [1] &quot;Kyrgyzstan&quot; # Translate multiple country codes countrycode(c(2, 20, 40), &quot;cown&quot;, &quot;country.name&quot;) ## [1] &quot;United States&quot; &quot;Canada&quot; &quot;Cuba&quot; # Convert country code 100 countrycode(100, &quot;cown&quot;, &quot;country.name&quot;) # Add a country column within the mutate: votes_processed votes_processed &lt;- votes %&gt;% filter(vote &lt;= 3) %&gt;% mutate(year = session + 1945, country = countrycode(ccode, &quot;cown&quot;, &quot;country.name&quot;)) ## Warning in countrycode(ccode, &quot;cown&quot;, &quot;country.name&quot;): Some values were not matched unambiguously: 260 votes_processed$country[votes_processed$ccode == &quot;260&quot;] &lt;- &quot;German Federal Republic&quot; 7.6.5 Video: Grouping and summarizing View slides. 7.6.6 Summarizing the full dataset In this analysis, youre going to focus on % of votes that are yes as a metric for the agreeableness of countries. Youll start by finding this summary for the entire dataset: the fraction of all votes in their history that were yes. Note that within your call to summarize(), you can use n() to find the total number of votes and mean(vote == 1) to find the fraction of yes votes. head(votes_processed) ## rcid session vote ccode year country ## 1 3 1 1 2 1946 United States ## 2 3 1 3 20 1946 Canada ## 3 3 1 1 40 1946 Cuba ## 4 3 1 1 41 1946 Haiti ## 5 3 1 1 42 1946 Dominican Republic ## 6 3 1 1 70 1946 Mexico votes_processed %&gt;% summarize(total = n(), percent_yes = mean(vote == 1)) ## total percent_yes ## 1 724922 0.797 7.6.7 Summarizing by year The summarize() function is especially useful because it can be used within groups. For example, you might like to know how much the average agreeableness of countries changed from year to year. To examine this, you can use group_by() to perform your summary not for the entire dataset, but within each year. by_year &lt;- votes_processed %&gt;% group_by(year) %&gt;% summarize(total = n(), percent_yes = mean(vote == 1)) The group_by() function must go before your call to summarize() when youre trying to perform your summary within groups. 7.6.8 Summarizing by country In the last exercise, you performed a summary of the votes within each year. You could instead summarize() within each country, which would let you compare voting patterns between countries. # Summarize by country: by_country by_country &lt;- votes_processed %&gt;% group_by(country) %&gt;% summarize(total = n(), percent_yes = mean(vote == 1)) 7.6.9 Video: Sorting and filtering summarized data View slides. Transforming tidy data. Source: DataCamp 7.6.10 Sorting by percentage of yes votes Now that youve summarized the dataset by country, you can start examining it and answering interesting questions. For example, you might be especially interested in the countries that voted yes least often, or the ones that voted yes most often. # Print first few entries of the by_country dataset head(by_country) ## # A tibble: 6 x 3 ## country total percent_yes ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 4899 0.840 ## 2 Albania 3439 0.719 ## 3 Algeria 4450 0.898 ## 4 Andorra 1487 0.649 ## 5 Angola 3025 0.923 ## 6 Antigua &amp; Barbuda 2595 0.918 # Sort in ascending order of percent_yes by_country %&gt;% arrange(percent_yes) ## # A tibble: 200 x 3 ## country total percent_yes ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Zanzibar 2 0 ## 2 United States 5312 0.285 ## 3 Palau 841 0.313 ## 4 Israel 4866 0.349 ## 5 German Federal Republic 2151 0.398 ## 6 Micronesia (Federated States of) 1405 0.414 ## 7 United Kingdom 5294 0.428 ## 8 France 5247 0.433 ## 9 Marshall Islands 1534 0.485 ## 10 Belgium 5313 0.495 ## # ... with 190 more rows # Now sort in descending order by_country %&gt;% arrange(desc(percent_yes)) ## # A tibble: 200 x 3 ## country total percent_yes ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Seychelles 1771 0.978 ## 2 Timor-Leste 769 0.969 ## 3 So Tom &amp; Prncipe 2388 0.967 ## 4 Djibouti 3269 0.956 ## 5 Guinea-Bissau 3001 0.955 ## 6 Cape Verde 3223 0.946 ## 7 Comoros 2468 0.945 ## 8 Mozambique 3382 0.943 ## 9 United Arab Emirates 3954 0.942 ## 10 Suriname 3354 0.941 ## # ... with 190 more rows 7.6.11 Filtering summarized output In the last exercise, you may have noticed that the country that voted least frequently, Zanzibar, had only 2 votes in the entire dataset. You certainly cant make any substantial conclusions based on that data! Typically in a progressive analysis, when you find that a few of your observations have very little data while others have plenty, you set some threshold to filter them out. # Filter out countries with fewer than 100 votes by_country %&gt;% filter(total &gt;= 100) %&gt;% arrange(percent_yes) ## # A tibble: 199 x 3 ## country total percent_yes ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 United States 5312 0.285 ## 2 Palau 841 0.313 ## 3 Israel 4866 0.349 ## 4 German Federal Republic 2151 0.398 ## 5 Micronesia (Federated States of) 1405 0.414 ## 6 United Kingdom 5294 0.428 ## 7 France 5247 0.433 ## 8 Marshall Islands 1534 0.485 ## 9 Belgium 5313 0.495 ## 10 Luxembourg 5245 0.513 ## # ... with 189 more rows 7.7 Solutions 2: Visualization with ggplot2 Once youve cleaned and summarized data, youll want to visualize them to understand trends and extract insights. Here youll use the ggplot2 package to explore trends in United Nations voting within each country over time. 7.7.1 Video: Visualization with ggplot2 View slides. 7.7.2 Choosing an aesthetic Youre going to create a line graph to show the trend over time of how many votes are yes. Which of the following aesthetics should you map the year variable to? ItsX-axis. To plot a line graph to show the trend over time, the year variable should be on the x-axis. # Create line plot ggplot(by_year, aes(x = year, y = percent_yes)) + geom_line() 7.7.3 Other ggplot2 layers A line plot is one way to display this data. You could also choose to display it as a scatter plot, with each year represented as a single point. This requires changing the layer (i.e. geom_line() to geom_point()). You can also add additional layers to your graph, such as a smoothing curve with geom_smooth(). # Change to scatter plot and add smoothing curve ggplot(by_year, aes(year, percent_yes)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 7.7.4 Video: Visualizing by country View slides. by_year_country &lt;- votes_processed %&gt;% group_by(year, country) %&gt;% summarize(total = n(), percent_yes = mean(vote == 1)) us_france &lt;- by_year_country %&gt;% filter(country %in% c(&quot;United States&quot;, &quot;France&quot;)) us_france ## # A tibble: 136 x 4 ## # Groups: year [68] ## year country total percent_yes ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1946 France 43 0.558 ## 2 1946 United States 43 0.605 ## 3 1947 France 38 0.737 ## 4 1947 United States 38 0.711 ## 5 1948 France 104 0.452 ## 6 1948 United States 104 0.452 ## 7 1949 France 64 0.312 ## 8 1949 United States 64 0.281 ## 9 1950 France 53 0.321 ## 10 1950 United States 53 0.491 ## # ... with 126 more rows ggplot(us_france, aes(x = year, y = percent_yes, color = country)) + geom_line() + ylim(0,1) 7.7.5 Summarizing by year and country Youre more interested in trends of voting within specific countries than you are in the overall trend. So instead of summarizing just by year, summarize by both year and country, constructing a dataset that shows what fraction of the time each country votes yes in each year. by_year_country &lt;- votes_processed %&gt;% group_by(year, country) %&gt;% summarize(total = n(), percent_yes = mean(vote == 1)) Lets make some plots using this new dataset in the next exercise. 7.7.6 Plotting just the UK over time Now that you have the percentage of time that each country voted yes within each year, you can plot the trend for a particular country. In this case, youll look at the trend for just the United Kingdom. This will involve using filter() on your data before giving it to ggplot2. # Create a filtered version: UK_by_year UK_by_year &lt;- by_year_country %&gt;% filter(country %in% c(&quot;United Kingdom&quot;)) # Line plot of percent_yes over time for UK only ggplot(UK_by_year, aes(x = year, y = percent_yes)) + geom_line() # Create a filtered version: UK_by_year Kg_by_year &lt;- by_year_country %&gt;% filter(country %in% c(&quot;Kyrgyzstan&quot;)) # Line plot of percent_yes over time for UK only ggplot(Kg_by_year, aes(x = year, y = percent_yes)) + geom_line() + ylim(0,1) 7.7.7 Plotting multiple countries Plotting just one country at a time is interesting, but you really want to compare trends between countries. For example, suppose you want to compare voting trends for the United States, the UK, France, and India. Youll have to filter to include all four of these countries and use another aesthetic (not just x- and y-axes) to distinguish the countries on the resulting visualization. Instead, youll use the color aesthetic to represent different countries. # Vector of four countries to examine countries &lt;- c(&quot;United States&quot;, &quot;United Kingdom&quot;, &quot;France&quot;, &quot;India&quot;) # Filter by_year_country: filtered_4_countries filtered_4_countries &lt;- by_year_country %&gt;% filter(country %in% countries) # Line plot of % yes in four countries ggplot(filtered_4_countries, aes(x = year, y = percent_yes, color = country)) + geom_line() 7.7.8 Video: Faceting by country View slides. 7.7.9 Faceting the time series Now youll take a look at six countries. While in the previous exercise you used color to represent distinct countries, this gets a little too crowded with six. Instead, you will facet, giving each country its own sub-plot. To do so, you add a facet_wrap() step after all of your layers. # Vector of six countries to examine countries &lt;- c(&quot;United States&quot;, &quot;United Kingdom&quot;, &quot;France&quot;, &quot;Japan&quot;, &quot;Brazil&quot;, &quot;India&quot;) # Filtered by_year_country: filtered_6_countries filtered_6_countries &lt;- by_year_country %&gt;% filter(country %in% countries) # Line plot of % yes over time faceted by country ggplot(filtered_6_countries, aes(x = year, y = percent_yes)) + geom_line() + facet_wrap(~country) 7.7.10 Faceting with free y-axis In the previous plot, all six graphs had the same axis limits. This made the changes over time hard to examine for plots with relatively little change. Instead, you may want to let the plot choose a different y-axis for each facet. # Line plot of % yes over time faceted by country ggplot(filtered_6_countries, aes(year, percent_yes)) + geom_line() + facet_wrap(~ country, scales=&quot;free_y&quot;) 7.7.11 Choose your own countries The purpose of an exploratory data analysis is to ask questions and answer them with data. Now its your turn to ask the questions. Youll choose some countries whose history you are interested in and add them to the graph. If you want to look up the full list of countries, enter by_country$country in the console. # Add three more countries to this list countries &lt;- c(&quot;United States&quot;, &quot;United Kingdom&quot;, &quot;France&quot;, &quot;Japan&quot;, &quot;Brazil&quot;, &quot;India&quot;, &quot;Kyrgyzstan&quot;, &quot;Georgia&quot;, &quot;Germany&quot;) # Filtered by_year_country: filtered_countries filtered_countries &lt;- by_year_country %&gt;% filter(country %in% countries) # Line plot of % yes over time faceted by country ggplot(filtered_countries, aes(year, percent_yes)) + geom_line() + facet_wrap(~ country, scales = &quot;free_y&quot;) 7.8 Solutions 3: Tidy modeling with broom While visualization helps you understand one country at a time, statistical modeling lets you quantify trends across many countries and interpret them together. Here youll learn to use the tidyr, purrr, and broom packages to fit linear models to each country, and understand and compare their outputs. 7.8.1 Video: Linear regression View slides. "],["correlation-and-regression-in-r.html", "Module 8 Correlation and Regression in R", " Module 8 Correlation and Regression in R "],["intermediate-data-visualization-with-ggplot2.html", "Module 9 Intermediate Data Visualization with ggplot2 9.1 Statistics", " Module 9 Intermediate Data Visualization with ggplot2 This ggplot2 course builds on your knowledge from the introductory course to produce meaningful explanatory plots. Statistics will be calculated on the fly and youll see how Coordinates and Facets aid in communication. Youll also explore details of data visualization best practices with ggplot2 to help make sure you have a sound understanding of what works and why. By the end of the course, youll have all the tools needed to make a custom plotting function to explore a large data set, combining statistics and excellent visuals. 9.1 Statistics A picture paints a thousand words, which is why R ggplot2 is such a powerful tool for graphical data analysis. In this chapter, youll progress from simply plotting data to applying a variety of statistical methods. These include a variety of linear models, descriptive and inferential statistics (mean, standard deviation and confidence intervals) and custom functions. 9.1.1 Video: Stats with geoms View slides. 9.1.2 Smoothing "],["tree-based-models-in-r.html", "Module 10 Tree-Based Models in R", " Module 10 Tree-Based Models in R "],["end-of-semester-project.html", "Module 11 End-of-semester project", " Module 11 End-of-semester project "]]
